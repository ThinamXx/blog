[
  {
    "objectID": "posts/WordVectors/WordVectors.html#word-vectors",
    "href": "posts/WordVectors/WordVectors.html#word-vectors",
    "title": "Word Vectors",
    "section": "Word Vectors",
    "text": "Word Vectors\nWord vectors are also called word embeddings or neural word representations because these whole bunch of words are represented in a high dimensional vector space and they are embedded into that space. They are also called as a distibuted representation.\nWord vectors means having a vector for each word type i.e both for context and outside, which are initialized randomly and those vectors are progressively updated by using iterative algorithms so that they can do better job at predicting which words appear in the context of other words.\n\nDistributional Semantics\nIt states that a word’s meaning is given by the words that frequently appear close-by. When a word w appears in a text, it’s context is the set of words that appear nearby within a fixed-size window."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#word2vec",
    "href": "posts/WordVectors/WordVectors.html#word2vec",
    "title": "Word Vectors",
    "section": "Word2Vec",
    "text": "Word2Vec\nWord2Vec is a framework for learning word vectors developed by Mikolov et al. 2013. The idea behind Word2Vec is that we have a large corpus or body of text and every word in a fixed-vocabulary is represented by a vector. We have to go through each position t in the text, which has a center word c and context words o and we have to use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa. We have to keep adjusting the word vectors to maximize this probability. Word2Vec model maximizes the objective function by putting similar words nearby in high dimensional vector space.\nTwo model variants: - Skip Grams: Predict context words given center word. - Continuous Bag of Words: Predict center word from context words.\nMain Idea of Word2Vec - Start with random word vectors. - Iterate through each word in the whole corpus. - Try to predict the surrounding words using word vectors. Try and predict what words surrounds the center word by using the probability distribution that is defined in terms of the dot product between the word vectors for the center word and the context words. - Updating the vectors so that they can predict the actual surrounding words better and better.\nKey Points: - Word2Vec model actually ignores the position of words. - Taking a log likelihood turns all of the products into sums which decreases the computational complexity. - A dot product is a natural measure for similarity between words. If two words have a larger dot product, that means they are more similar. - The simple way to avoid negative probabilities is to apply exponential function.\nTraining Methods: - To train a model, we gradually adjust parameters to minimize a loss. - Theta represents all the model parameters, in one long vector. We optimize these parameters by walking down the gradient.\nBag of Words Model: - Bag of words models are the models that don’t pay attention to words order or position, it doesn’t matter if the word is near to the center word or a bit further away on the left or right. The probability estimation will be the same at each position.\nIn bag of words, we have outside word vector and center word vector, which undergoes dot product followed by softmax activation function.\n\nOptimization: Gradient Descent\n\nTo learn good word vectors, we have a cost function J(\\(\\theta\\)).\nGradient descent is an algorithm to minimize the J(\\(\\theta\\)) by changing \\(\\theta\\).\nGradient Descent is an iterative learning algorithm that learns to maximize the J(\\(\\theta\\)) by changing the \\(\\theta\\).\nFrom the current value of \\(\\theta\\), calculate the gradient of J(\\(\\theta\\)), then take small step in the direction of negative gradient to gradually move down towards the minimum.\n\nProblems of Gradient Descent - J(\\(\\theta\\)) is a function of all windows in the corpus which is often billions. So, actually working out J(\\(\\theta\\)) or the gradient of J(\\(\\theta\\)) would be extremely expensive because we have to iterate over the entire corpus. - We would wait a very long before making a single update.\n\n\nStochastic Gradient Descent\nStochastic gradient descent is a very simple modification of the gradient descent algorithm. Rather than working out an estimate of the gradient based on the entire corpus, we simply take one center word or a small batch like 32 center words, and we work out the estimate of the gradient based on them.\nStochastic gradient descent is kind of noisy and bounces around as it makes progress, it actually means that in complex networks it learns better solution. So, it can do much more quickly and much better.\n\n\nSoftmax Function\nThe softmax function will take any R in vector and turns it into the range 0 and 1. The name of the softmax function comes from the fact that it’s sort of like a max because the exponential function gives more emphasis to the big contents in different dimensions of calculating the similarity. The softmax function takes some numbers and returns the whole probability distribution.\n\n\nCo-occurence Vector\n\nVectors increase in size with the vocabulary.\nVery high dimensional and requires a lot of storage though sparse.\nSubsequent classification models have sparsity issues which makes models less robust."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#glove",
    "href": "posts/WordVectors/WordVectors.html#glove",
    "title": "Word Vectors",
    "section": "GloVe",
    "text": "GloVe\n\nFast training.\nScalable to huge corpus.\nGood performance even with small corpus and small vectors.\n\nGloVe model unify the thinking between the co-occurence matrix models and the neural models by being someway similar to the neural models but actually calculated on top of a co-occurence matrix count."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html",
    "href": "posts/PPO/ppo_loss.html",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "",
    "text": "In my previous blog post, we discussed the Policy Gradient Optimization where we derived the expression for the gradient of the objective function w.r.t the policy parameters: \\(\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i | s_t^i) R(\\tau^i)\\). Similarly, the expression for the objective function which we want to maximize is: \\(J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\). However, PGO works well in simple environments, but it can be difficult to scale to more complex environments or problems like LLMs.\n\nHigh Variance: As seen in the above expression of computing the gradient of the objective function, we are approximating the expectation using samples (computing mean) which means that we are calculating the expression only on a subset of the trajectories which can lead to high variance in the gradient estimates. Therefore, the gradient approximation is unbiased which means that the average of the gradients over many samples or trajectories will eventually converge to the true gradient. In large language models, we can’t easily increase the number of sample trajectories to reduce the variance because it is computationally expensive.\nSampling: In PGO, we sample the trajectories from the policy (or LLMs) and compute the reward associated with the trajectory, compute the log probability of the trajectory and then combine them to compute the gradient of the objective function. This entire process is repeated multiple times for each gradient ascent step to update the policy parameters. However, this sampling process can be computationally expensive and inefficient in large language models."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#limitations-of-policy-gradient-optimization",
    "href": "posts/PPO/ppo_loss.html#limitations-of-policy-gradient-optimization",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "",
    "text": "In my previous blog post, we discussed the Policy Gradient Optimization where we derived the expression for the gradient of the objective function w.r.t the policy parameters: \\(\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i | s_t^i) R(\\tau^i)\\). Similarly, the expression for the objective function which we want to maximize is: \\(J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\). However, PGO works well in simple environments, but it can be difficult to scale to more complex environments or problems like LLMs.\n\nHigh Variance: As seen in the above expression of computing the gradient of the objective function, we are approximating the expectation using samples (computing mean) which means that we are calculating the expression only on a subset of the trajectories which can lead to high variance in the gradient estimates. Therefore, the gradient approximation is unbiased which means that the average of the gradients over many samples or trajectories will eventually converge to the true gradient. In large language models, we can’t easily increase the number of sample trajectories to reduce the variance because it is computationally expensive.\nSampling: In PGO, we sample the trajectories from the policy (or LLMs) and compute the reward associated with the trajectory, compute the log probability of the trajectory and then combine them to compute the gradient of the objective function. This entire process is repeated multiple times for each gradient ascent step to update the policy parameters. However, this sampling process can be computationally expensive and inefficient in large language models."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#off-policy-learning",
    "href": "posts/PPO/ppo_loss.html#off-policy-learning",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "Off-Policy Learning",
    "text": "Off-Policy Learning\nTherefore, to address the limitations of PGO, we can use the Off-Policy Learning which uses the technique of Importance Sampling (i.e. it allows us to evaluate the expectation over a distribution \\(X\\) using samples taken from a different distribution \\(Y\\)).\nLet’s consider a language model with parameters \\(\\theta_{offline}\\) which is basically an Offline Policy (model). Then we sample the trajectories (i.e. sequence of state, action, reward) and compute the log probability and reward associated with the trajectories. We then take the mini-batch of trajectories from the above sampled trajectories and optimize the parameters of the new language model with parameters \\(\\theta_{online}\\) which is an Online Policy (model). After optimizing the parameters of the online policy model, we then update the parameters of the offline policy model with the parameters of the online policy model i.e. \\(\\theta_{offline} = \\theta_{online}\\).\n\n\n\nFig a. Off-Policy Learning diagram prepared by Umar Jamil.\n\n\nTherefore, using the Off-Policy Learning technique, we don’t have to sample the trajectories from the policy model for each gradient ascent step. Instead, we can sample the trajectories and keep them in the memory buffer and use them to optimize the policy model parameters for multiple gradient ascent steps. Then we can sample the trajectories again and repeat the process. This way, we can reduce the computational cost and improve the efficiency of the policy optimization process."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#reward-model",
    "href": "posts/PPO/ppo_loss.html#reward-model",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "Reward Model",
    "text": "Reward Model\nReward model is a type of language models that are trained in such a way that they give high rewards to the choosen answers and low rewards to the not choosen answers when given a concatenated question and answer pair to the model.\n\n\n\nFig b. Reward Model prepared by Umar Jamil.\n\n\nAs shown in the above figure, we feed a series of input tokens (i.e. tokens of the concatenated question and answer pair) to a pre-trained language model (i.e. a transformer model), it generates the series of the output embeddings which are called the hidden states. In the context of language models, we usually consider the last hidden state to generate the new output tokens. Similarly, we will choose the last hidden state or output embedding of the language model to generate the reward for the answer fed to the model along with a particular question. We will use a Linear layer that takes the last hidden state of the language model and outputs one feature representation which is the reward for the answer.\n\nDataset for Reward Model\nAccording to the paper, Learning to summarize from human feedback, training reward models often involves utilizing a dataset comprised of paired comparisons between two answers generated for the same question input. The dataset is prepared by:\n- Let’s consider we have a pretrained language model and we feed a question to the model and generate multiple answers for the question. - We then ask the human experts to compare the answers and choose the best answer among them. - As mentioned above in the Reward Model section, we then train the reward model, when given a concatenated question and answer pair, it should generate a high numerical reward for the choosen answer and a low numerical reward for the not choosen answer (i.e. the answer which is not choosen by the human expert).\n\n\n\nFig c. Dataset for Reward Model prepared by Umar Jamil.\n\n\n\n\nLoss Function for Reward Model\nSince, we now have the dataset and the procedure to train the reward model, we can define the loss function for the reward model as follows:\n\\(L(\\theta) = -log(\\sigma(r(x, y_{win}) - r(x, y_{lose})))\\) where \\(\\sigma\\) is the sigmoid function, \\(r(x, y_{win})\\) is the reward generated by the reward model for the choosen answer and \\(r(x, y_{lose})\\) is the reward generated by the reward model for the not choosen answer.\nThe above loss function will have two cases: - When \\(r(x, y_{win}) &gt; r(x, y_{lose})\\), it will generate a positive value which means that the sigmoid function will generate a value between 0.5 and 1.0. It will then be fed to the log function which will generate a negative value between 0 and -1.0 which will be multiplied by -1. Therefore, the loss will be a small positive value around 0 and 1.0. - When \\(r(x, y_{win}) &lt; r(x, y_{lose})\\), it will generate a negative value which means that the sigmoid function will generate a value between 0 and 0.5. It will then be fed to the log function which will generate a negative value between -1.0 and \\(-\\inf\\) which will be multiplied by -1. Therefore, the loss will be a large positive value.\n\n\n\nFig d. Sigmoid and Log function used in the above explanation.\n\n\nTherefore, the above loss function will penalize the reward model when it generates a high reward for the not choosen answer and a low reward for the choosen answer and vice versa as we know that the reward model will learn by minimizing the loss function."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#proximal-policy-optimization-ppo",
    "href": "posts/PPO/ppo_loss.html#proximal-policy-optimization-ppo",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\nPPO is a type of policy optimization technique in RL, aimed at effectively training a policy without jeopardizing the stability of the training process. The underlying idea behind PPO is a small, stable steps, a philosophy of gently nudging the policy towards optimization, rather than forcing aggressive updates that could potentially destabilize the overall learning process.\nIn traditional RL, the principle of policy gradient mandates that the policy we want to optimize (online policy) and the policy we use to sample the trajectories (offline policy) should remain close in the parameter space. However, this proximity constraint in parameter space doesn’t necessarily equate to the similar performance, and a slight variance in the parameters can drastically impact the effectiveness of the policy which can lead to a scenario often described as falling off the cliff.This is where PPO comes in, by ensuring that the policy updates are small and stable, it helps to prevent the policy from falling off the cliff and ensures that the policy remains stable throughout the training process.\n\nClipped Surrogate Objective Function\nPPO-Clip attempts to keep the (online) policy close to the (offline) policy by introducing a clipped surrogate objective function. The clipped surrogate objective function is defined as follows:\n\\(L^{CLIP}(\\theta) = \\mathbb{E}_{t} [min(\\frac{\\pi_{\\theta_{online}}(a_t | s_t)}{\\pi_{\\theta_{offline}}(a_t | s_t)} \\hat{A}_t, clip(\\frac{\\pi_{\\theta_{online}}(a_t | s_t)}{\\pi_{\\theta_{offline}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)]\\)\nwhere \\(\\frac{\\pi_{\\theta_{online}}(a_t | s_t)}{\\pi_{\\theta_{offline}}(a_t | s_t)}\\) is the ratio of the online policy probability to the offline policy probability, \\(\\hat{A}_t\\) is the advantage function (i.e. it tells us how much better a particular action is compared to the average action in a particular state), and \\(\\epsilon\\) is the hyperparameter that determines how much the online policy can deviate from the offline policy. The clip function limits the ratio of the online policy probability to the offline policy probability to the range of \\([1 - \\epsilon, 1 + \\epsilon]\\) which ensures that the policy updates are small and stable.\n\n\nValue Function Estimation\nIn PPO, the value function is used to estimate the expected reward we can receive by starting from a particular state and following a particular policy. The learning objective for the value function estimator (or model) is to minimize the discrepancy between the predicted value and the actual reward value. The value function estimator is trained using the following loss function:\n\\(L^{VF}(\\theta) = \\mathbb{E}_{t} [(V_{\\theta}{(s_t)} - \\hat{R}_t)^2]\\)\nwhere \\(V_{\\theta}{s_t}\\) is the model’s predicted value for the state \\(s_t\\) with parameters \\(\\theta\\) and \\(\\hat{R}_t\\) is the actual reward value for the state \\(s_t\\) and can be estimated as \\(\\hat{R}_t = \\sum_{i=0}^{T} \\gamma^i r_{t+i}\\) where \\(\\gamma\\) is the discount factor and \\(r_{t+i}\\) is the reward received at time step \\(t+i\\) from the sampled trajectory.\n\n\nPPO Loss Function\nThe overall loss function for the PPO is a combination of the clipped surrogate objective function, the value loss function and entropy loss function. The entropy loss function is used to encourage the policy to explore more options and prevent it from getting stuck in a local optima. If we don’t include the entropy loss function, the policy or (model) will always choose the same action in a particular state that has the highest advantage value. The entropy loss function is defined as: \\(L^{ENT}(\\theta) = -\\sum_{a} \\pi_{\\theta}(a | s) \\log \\pi_{\\theta}(a | s)\\) where \\(\\pi_{\\theta}(a | s)\\) is the probability of taking action \\(a\\) in state \\(s\\).\nTherefore, the overall loss function for the PPO is defined as:\n\\(L(\\theta) = L^{CLIP}(\\theta) - c_1 L^{VF}(\\theta) + c_2 L^{ENT}(\\theta)\\)\nwhere \\(c_1\\) and \\(c_2\\) are the hyperparameters that determine the weight of the value loss function and entropy loss function in the overall loss function."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#kl-divergence",
    "href": "posts/PPO/ppo_loss.html#kl-divergence",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "KL Divergence",
    "text": "KL Divergence\nKL Divergence is an alternative to the clipped surrogate objective function, which introduces a constraint on the policy updates by regularizing and ensuring that the KL divergence remains within a certain threshold. The expression for the KL divergence is defined as:\n\\(L^{KL}(\\theta) = \\mathbb{E}_{t} [\\frac{\\pi_{\\theta_{online}}(a_t | s_t)}{\\pi_{\\theta_{offline}}(a_t | s_t)} \\hat{A}_t - \\beta KL(\\pi_{\\theta_{offline}}(a_t | s_t) || \\pi_{\\theta_{online}}(a_t | s_t))]\\)\nwhere \\(\\beta\\) is the hyperparameter that determines the weight of the KL divergence in the overall loss function. The KL divergence ensures that the policy updates are small and stable by regularizing the policy updates and preventing the policy from deviating too far from the offline policy."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#conclusion",
    "href": "posts/PPO/ppo_loss.html#conclusion",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we discussed the limitations of the Policy Gradient Optimization (PGO) and how we can address these limitations using the Off-Policy Learning technique. We also discussed the Reward Model, the dataset for training the reward model, the loss function for the reward model, and the importance of the reward model in training large language models. We then discussed the Proximal Policy Optimization (PPO) technique, the clipped surrogate objective function, the value function estimation, the overall loss function for the PPO, and the KL divergence. We also discussed the importance of the entropy loss function in the overall PPO loss and how it encourages the policy to explore more options and prevent it from getting stuck in a local optima."
  },
  {
    "objectID": "posts/PPO/ppo_loss.html#references",
    "href": "posts/PPO/ppo_loss.html#references",
    "title": "Proximal Policy Optimization (PPO)",
    "section": "References",
    "text": "References\n\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., & Christiano, P. (2020). Learning to summarize from human feedback. ArXiv. /abs/2009.01325\nZheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., Xiong, L., Chen, L., Xi, Z., Xu, N., Lai, W., Zhu, M., Chang, C., Yin, Z., Weng, R., . . . Huang, X. (2023). Secrets of RLHF in Large Language Models Part I: PPO. ArXiv. /abs/2307.04964\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. ArXiv. /abs/1312.5602\nLi, Y. (2018). Deep Reinforcement Learning. ArXiv. /abs/1810.06339\nReinforcement Learning from Human Feedback explained with math derivations and the PyTorch code.\nIntro to Policy Optimization\nDeep Reinforcement Learning: Pong from Pixels"
  },
  {
    "objectID": "posts/Pattern/Pattern.html#introduction",
    "href": "posts/Pattern/Pattern.html#introduction",
    "title": "Pattern Recognition & ML",
    "section": "Introduction",
    "text": "Introduction\nThe field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Generalization, the ability to categorize correctly new examples that differ from those used for training is a central goal in pattern recognition.\nApplications in which the training data comprises examples of the input vectors with their corresponding target vectors are known as supervised learning problems. The cases such as the digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification. If the desired output consists of one or more continuous variables, then the task is called regression.\nThe pattern recognition problems in which the training data consists of a set of input vectors x without any corresponding target values are called unsupervised learning problems. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.\nThe technique of reinforcement learning is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#probability-theory",
    "href": "posts/Pattern/Pattern.html#probability-theory",
    "title": "Pattern Recognition & ML",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory provides a consistent framework for the quantification and manipulation of uncertainty. When combined with decision theory, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#the-rules-of-probability",
    "href": "posts/Pattern/Pattern.html#the-rules-of-probability",
    "title": "Pattern Recognition & ML",
    "section": "The Rules of Probability",
    "text": "The Rules of Probability\n\nSum Rule: p(X) = \\(\\\\sum\\_{Y}^{}{p(X,\\\\ Y)}\\)\nProduct Rule: p(X, Y) = p(Y / X)p(X)\n\nHere, p(X, Y) is a joint probability and is verbalized as “the probability of X and Y”. Similarly, the quantity p(Y/X) is a conditional probability and is verbalized as “the probability of Y given X”, where the quantity p(X) is a marginal probability and is verbalized as “the probability of X”."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#bayes-theorem",
    "href": "posts/Pattern/Pattern.html#bayes-theorem",
    "title": "Pattern Recognition & ML",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nFrom the product rule, together with the symmetry property p(X, Y) = p(Y, X), we obtain the following relationship between conditional probabilities which is called Bayes’ theorem which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator as being the normalization constant required ensuring that the sum of the conditional probability on the left-hand side over all values of Y equals one.\n\np(Y / X) = \\(\\\\frac{p\\\\left( \\\\frac{X}{Y} \\\\right)p(X)}{p(X)}\\)\np(X) = \\(\\\\sum\\_{Y}^{}{p\\\\left( \\\\frac{X}{Y} \\\\right)p(Y)}\\)"
  },
  {
    "objectID": "posts/Pattern/Pattern.html#model-selection",
    "href": "posts/Pattern/Pattern.html#model-selection",
    "title": "Pattern Recognition & ML",
    "section": "Model Selection",
    "text": "Model Selection\nIn the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, the one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. However, in many applications, the data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. If the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this problem is to use cross-validation. This allows a proportion (S – 1) / S of the available data to be used for training while making use of all of the data to assess performance."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#decision-theory",
    "href": "posts/Pattern/Pattern.html#decision-theory",
    "title": "Pattern Recognition & ML",
    "section": "Decision Theory",
    "text": "Decision Theory\nIf we have an input vector x together with a corresponding vector t of target variables, our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classification problems, t will represent class labels."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#minimizing-loss-function",
    "href": "posts/Pattern/Pattern.html#minimizing-loss-function",
    "title": "Pattern Recognition & ML",
    "section": "Minimizing Loss Function",
    "text": "Minimizing Loss Function\nLoss function is also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred."
  },
  {
    "objectID": "posts/NLP/NLP.html#books",
    "href": "posts/NLP/NLP.html#books",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Books:",
    "text": "Books:\n\nNatural Language Processing with Python\nNatural Language Processing in Action\nNatural Language Processing with PyTorch\nNatural Language Processing Specialization"
  },
  {
    "objectID": "posts/NLP/NLP.html#research-papers",
    "href": "posts/NLP/NLP.html#research-papers",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Research Papers:",
    "text": "Research Papers:\n\nTeacher Forcing in Recurrent Neural Networks\nExploring the Limits of Transfer Learning with a Unified Text to Text Transformer\nDeep Learning Based Text Classification: A Comprehensive Review\nThe Illustrated Transformer\nNLP’s ImageNet Moment\nThe Illustrated BERT, ELMO and Co.\nGeneralized Language Models\nEfficient Estimation of Word Representations in Vector Space\nDistributed Representations of Words and Phrases and their Compositionality\nGloVe : Global Vectors for Word Representation\nA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\nNeural Machine Translation By Jointly Learning To Align and Translate\nDynamic Memory Networks for Natural Language Processing\nDynamic Memory Networks for Natural Language Processing\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\nStanza: A Python Natural Language Processing Toolkit for Many Human Languages\nLearning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models\nImproved Semantic Representations From Tree Structured Long Short Term Memory Networks\nThe Stanford CoreNLP Natural Language Processing Toolkit\nZero Shot Learning Through Cross Modal Transfer\nAssessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies\nFinding Universal Grammatical Relations in Multilingual BERT"
  },
  {
    "objectID": "posts/NLP/NLP.html#notebooks",
    "href": "posts/NLP/NLP.html#notebooks",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Notebooks:",
    "text": "Notebooks:\n\nTopic Modeling with Singular Value Decomposition and Non negative Matrix Formation\nSentiment Classification of Internet Movie Reviews Database Reviews\nSemantic Analysis with LDA, LSA and LDIA\nSentiment Analysis of Large Movie Dataset using RNN, CNN and LSTM\nText Generation using Long Short Term Memory or LSTM\nChatbot with Sequence to Sequence Networks\nYELP Reviews Sentiment Analysis\nAmazon Reviews Analysis\nSurname Classification with Demographics\nDuplicate Questions Recognition using Trax\n\nDay1 of 66DaysOfData! - Natural Language Processing: Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today I am learning Natural Language Processing from very begining. I have read and Implemented the Fundamentals of Natural Language Processing. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also presented the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited about the days ahead !! - Topics: - Fundamentals of Natural Language Processing\n\nDay2 of 66DaysOfData! - String Tokenization: In Natural Language Processing, String Tokenization is a process where the string is splitted into Individual words or Individual parts without blanks and tabs. In the same step, the words in the String is converted into lower case. The Tokenize Module from NLTK or Naural Language Toolkit makes very easy to carry out this process. In my Journey of Natural Language Processing, Today I have learned about String Tokenization, Stop word and Punctuation in Natural Language Processing. I have implemented TweetTokenizer and presented the process to remove Stopwords and Punctuation from the Tokenized Tweets here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited about the days ahead!! - Topics: - Fundamentals of Natural Language Processing\n\nDay3 of 66DaysOfData! - Stemming in Natural Language Processing: Stemming is a process of converting a word to its most General form or Stem. It’s basically the process of removing the suffix from a word and reduce it to it’s root word. It helps in reducing the size of Vocabulary. In my Journey of Natural Language Processing, Today I learned about Stemming in Natural Language Processing which is one of the most important steps while working with Text. I have presented the Implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited for coming days!! - Porter Stemmer: It is one of the most common and gentle stemmer which is very fast but not very precise. - Snowball Stemmer: It’s actual name is English Stemmer is more precise over large Dataset. - Lancaster Stemmer: It is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons. - Topics: - Fundamentals of Natural Language Processing\n\nDay4 of 66DaysOfData! - Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word’s Lemma or a Dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than Simple Stemming. Lemmatization looks at the surrounding text to determine a given words’s part of speech where it doesn’t categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it’s simple implementation using Spacy as well NLTK. I have covered the fundamentals of Natural Language Processing such as Tokenization, Stemming and Lemmatization. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited for the days ahead!! - Topics: - Fundamentals of Natural Language Processing\n\nDay5 of 66DaysOfData! - Natural Language Processing: Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. As a part of Natural Langauge Processing Journey, I have started reading and Implementing from the Book Natural Language Processing with Python. It’s really amazing and I have encountered many basic functions which are unknown to me such as Concordance Function, Similar Function, Common Context Function and a basic Dispersion plot as well. I will be using this Book in my journey. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited for the days ahead!! - Book: - Natural Language Processing with Python\nDay6 of 66DaysOfData! - In my Journey of Natural Language Processing, Today I have explored about Gutenberg Corpus using NLTK and Python. I have also learned about various Interesting Challenges with proper explanation of each Topics under the hood of Natural Language Processing from the Book Natural Language Processing with Python. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. - Word Sense Disambiguation: In Natural Language Processing, Word Sense Disambiguation is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other Computer related writing such as discourse, improving relevance of Search Engines, Anaphora Resolution, Coherence, and Inference. - Pronounce Resolution - Generating Language Output - Machine Translation: Machine Translation is a sub field of Computational Linguistics that investigates the use of software to translate text or speech from one language to another. - Spoken Dialog System - Textual Entailment - Book: - Natural Language Processing with Python\n\nDay7 of 66DaysOfData! - On my Journey of Natural Language Processing, Today I have learned about different Text Corpora and Basic Corpus Functionality defined in NLTK from the Book Natural Language Processing with Python . I have also learned about Loading own Corpus, Plotting and Tabulating Distributions and Generating Random Text with Bigrams here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. The Topics I have covered are summarized below: - Web and Chat Text - Brown Corpus - Reuters Corpus - Inaugural Address Corpus - Annotated Text Corpora - Corpora in Other Languages - Conditional Frequency Distributions - Book: - Natural Language Processing with Python\n\nDay8 of 66DaysOfData! - On my Journey of Natural Language Processing, Today I have learned about Processing Raw Text in Natural Language Processing. Basically, I have completed Processing the Text from Electronic Books and from HTML documents from the Book Natural Language Processing with Python. Apart from that, I have learned about WordNet. The topics I have covered in WordNet are: The WordNet Hierarchy and Semantic Similarity: Semantic Similarity is a metric defined over a set of Documents or Terms where the idea of distance between items is based on the likeness of their meaning or Semantic content as opposed to Lexicographical Similarity. Example:Road and Driving. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay9 of 66DaysOfData! - On my journey of Natural Language Processing, Today I have learned about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper explanations. Within a Program we can manipulate Unicode just like normal strings. Unicode are encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8. I have also Implemented the Regular Expressions for Detecting the Word Patterns using basic meta characters such as: - Dollar sign ( $ ) matches the characters of the end of word. - Dot symbol ( . ) matches any single character. - Caret symbol ( ^ ) matches the characters of the start of word. - Question mark ( ? ) specifies the previous character is optional. - Plus sign ( + ) means one or more instances of the preceding item. - Sign ( * ) means zero or more instances of the preceding item. - Backslash (  ) means following character is deprived. - Pipe symbol ( | ) specifies the choice between left and right. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay10 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about the various useful applications of Regular Expressions such as Finding Word Stems, Regular Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have also read about the issues of Tokenization: Tokenization turns out to be far more difficult task than one might have expected. No single solution works well accross the board. Another issue of Tokenization is the presence of contractions as well such as in didn’t. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay11 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have completed all the preliminaries process or techniques required in Natural Language Processing included in the Book Natural Language Processing with Python. I have completed the topics such as Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions from this Book. Apart from that, I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters Corpus and so on. And I have completed first 110 pages of the Book Natural Language Processing with Python. I have plotted a simple bar plot using various categories of Brown Corpus presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n \nDay12 of 66DaysOfData! - Part of Speech Tagging: The process of classifying words into their Parts of Speech and Labeling them accordingly is known as Part of Speech Tagging which is also known as POS tagging or simply Tagging. The collections of tags used for a particular task is known as Tagset. In my Journey of Natural Language Processing, Today I have learned about Automatic Tagging such as Default Tagger, Regular Expression Tagger and Lookup Tagger along with N-Gram Tagging such as Unigram Tagger, Bigram Tagger and so on. I have also learned about Combining Taggers using backoff parameter. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay13 of 66DaysOfData! - Supervised Classification: Classification is the process of choosing the correct class label for a given input. The Classification is said to be Supervised Classification if it is built based on training corpora containing the correct label for each input. One common example of Classification is deciding whether an Email is spam or not. In my Journey of Natural Language Processing, Today I have learned about Supervised Classification. I have covered the topics such as Choosing Right Features, Document Classification, Gender Identification, Part of Speech Tagging using Naive Bayes Classifier and Decision Trees Classifier under the hood of Supervised Classification of Natural Language Processing. I have presented the basic Implementation of Naive Bayes Classifier in Document Classification. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead!! - Book: - Natural Language Processing with Python\n\nDay14 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Sequence Classification, Sentence Segmentation and various Evaluation methods under the hood of Natural Language Processing. I have covered the Fundamental Topics such as Test Data, Accuracy measure, Precision and Recall, Confusion Matrices, Cross validation, Decision Trees and Naive Bayes with proper implementations which are so helpful for my understanding. I have been in this Journey for 2 weeks and I have covered all the fundamentals which are so relevant in Natural Language Processing. I have been following the Natural Language Processing with Python Book and it really helps me a lot. Now, I will be focusing more on Implementations so that I will be following the course of Fastai on Natural Language Processing. I have implemented the Naive Bayes Classifier in Text Corpus and I hope you can gain insight about the Implementation of Naive Bayes. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead!! - Book: - Natural Language Processing with Python\n\nDay15 of 66DaysOfData! - Singular Value Decomposition or SVD: The words that appear most frequently in one topic would appear less frequently in the other, otherwise that word wouldn’t make a good choice to separate out the two topics. Therefore, the Topics are Orthogonal. The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows along with diagonal matrix which contains the relative importance of each factor. - NonNegative Matrix Factorization or NMF: Non Negative Matrix Factorization (NMF) is a factorization or constrain of non negative dataset. NMF is non exact factorization that factors into one short positive matrix. - Topic Frequency Inverse Document Frequency or TFIDF: TFIDF is a way to normalize the term counts by taking into account how often they appear in a document and how long the document is and how common or rare the document is. - In my journey of Natural Language Processing, Today I have learned and Implemented about SVD, NMF and TFIDF in Topic Modeling Project. I have captured just the overview of the implementations here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai\n\nTopic Modeling with SVD and NMF\n\n \nDay16 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Implementations of Natural Language Processing from Fastai course which has been published recently. As the Fastai course primarily focuses on the Coding part and follows the top down aspect of Learning and Teaching. It’s bit complicated to learn than other courses. Fastai’s API is really amazing and powerful as well. I learned the basic steps of Natural Language Processing with Fastai such as Word Tokenization, Subword Tokenization, Numericalization, and Preparing TextBlock and DataBlock. I am currently working on Sentiment Analysis of IMDB reviews using Fastai. I have shared the Implementations of the Word Tokenization, Subword Tokenization, Numericalization and process to prepare the TextBlock and DataBlock with Fastai here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai - Sentiment Classification of Internet Movie Database reviews\n\nDay17 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about the Implementation of Fastai in preparing a Sentiment Classifier Model. I have prepared a Model using Fastai API which can classify Sentiment of Internet Movie Database reviews i.e. classifying the Positive or Negative Sentiment. Fastai’s API is really powerful and effective so that the Model can classify the Sentiment of Internet Movie Database reviews with above 90% accuracy in just few lines of code. I have learned about Word Tokenization, Subword Tokenization, Numericalization, TextBlock and DataBlock API and Training the Classifier Model using Fastai.I have presented the snapshot of the Implementation of Fastai API in preparing the Language Model and Training the Model. I have also presented the Implementation of Fastai API in preparing the Classifier Model using the Language Model and also the concept of unfreezing the Model. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai - Sentiment Classification of Internet Movie Database reviews\n\nDay18 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Topic of Natural Language Processing in the Book Dive into Deep Learning by Aston Zhang. Here, I have covered the Topics such as Text Processing, Machine Translation, Natural Language Processing Pretraining and Word Embedding. The information and explanations were great and the code implementation is in MXNET. I am not quite familiar with MXNET framework so I have just presented a small Snapshot here. Apart from that, Today I have read a very small part of the book Natural Language Processing in Action by Hobson Lane. I am so much thankful to Anthony Camarillo for sharing this book with me. And I will give continuation with this Book Natural Language Processing in Action. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Dive into Deep Learning - Natural Language Processing in Action\n\nDay19 of 66DaysOfCode! - In my journey of Natural Language Processing, Today I have read and implemented the First chapter of the Book Natural Language Processing in Action. In this chapter, I have covered the Topics such as Natural Language Processing and the Magic, Regular Expressions, Finite State Machine or FSM concept, Word order and Grammar, simple NLP Chatbot Pipeline and Natural Language IQ. I have presented the simple Chatbot using Regular Expressions and Finite State Machine or FSM concept. Basically, I will be working on much advanced Chatbots using Neural Networks in coming days. I hope you will also google out the FSM concept in NLP and also the Implementation of Regular Expressions in FSM from here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Natural Language Processing in Action\n\nDay20 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a couple of topics under the hood of Natural Language Processing such as Tokenized Phrase, Dot Product in Natural Language Processing, Bag of Words and Overlapping, Token improvement with Regex which is apart from Regular Expressions, Contractions, Extending Vocabulary with NGrams. I have also read and implemented the Regex Tokenizer, Tree Bank Tokenizer and Casual Tokenizer. Actually I am continuing my learning Journey with a Book Natural Language Processing in Action and lots of the preprocessing concepts which I have already read are coming along my way. I prefer to go through the concepts again because I don’t want to skip any topics from this book. Although the concepts might match along the way I won’t repeat the same implementation in any of my Snapshots. I have presented the Implementation of Regex Tokenizer, Tree Bank Tokenizer, Casual Tokenizer and NGrams here in the Snapshots. These Tokenization steps are more better than Traditional Tokenization steps using Regular Expressions. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Natural Language Processing in Action\n\nDay21 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and implemented about Stopwords, Stemming, Lemmatization and Sentiment Analysis using VADER Approach which is the Algorithm composed by human and also the Machine Learning Approach with the help of Naive Bayes Classifier. I have completed the first 2 chapters of the book, Natural Language Processing in Action and it is helping me a lot along my Journey. I have presented the Implementation of VADER Approach in Sentiment Analysis along with Naive Bayes Classifier and I have also included the Implementation of Casual Tokenizer in the Movies Dataset. I hope you will gain insights about the Implementation of VADER Approach and Naive Bayes in Sentiment Analysis. Actually, VADER Approach is not as efficient as Machine Learning Approach such as Naive Bayes Classifier. I hope you will spend some time in learning about Naive Bayes in Sentiment Analysis along with VADER Approach. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay22 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Bag of Words, Vectorizing concept, Vector Spaces, Cosine Similarity, Zipf’s Law and Inverse Frequency concept, Text Modeling, TFIDF, Relevance Ranking and Okapi BM25 concept. I have completed the first three chapters of the book Natural Language Processing in Action and this chapter primarily focuses on the concept of Vectorizing the Tokens which are obtained after Tokenization using TFIDF Vectorizer. Text Vectorization is the process of converting Text into Numerical representation. I have also read and Implemented the concept of Cosine Similarity under the hood of Natural Language Processing. I have presented the Implementation of TFIDF Vectorizer and also the process of Tokenizing the Text Documents and removing the Stopwords. I have also Implemented the Cosine Similarity using Numpy and pure Python as well in this Snapshot. I hope you will gain insights about Text Vectorization and Tokenization from here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay23 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have started learning about Semantic Analysis. I have read and implemented about Topic Vectors, Algorithms for Scoring Topics and Semantic Analysis such as Latent Semantic Analysis LSA, Linear Discriminant Analysis LDA and Latent Dirichlet Allocation LDIA. Today, I primarily focused on reading and Implementing about Linear Discriminant Analysis LDA. LDA is one of the most straight forward and fast Dimension Reduction and Classification Models. In Natural Language Processing, Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of LDA’s working Principal which states that Computing the centroid of TFIDF vectors for each side of the binary Class here in the Snapshot. I hope you will gain insights about Implementation of LDA Classifier and creating NLP Pipeline of Tokenizer and Vectorizer from here. And I hope you will spend some time in learning about Semantic Analysis as well. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead !! - Books: - Natural Language Processing in Action\n\nDay24 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented Latent Semantic Analysis “LSA”, Singular Value Decomposition “SVD”, Principal Component Analysis “PCA”, Truncated SVD and Latent Dirichlet Allocation “LDIA”. I have primarily focused on reading and Implementing LSA and LDIA for Semantic Analysis. LSA works well with Normalized TFIDF Vectors whereas LDIA works well with raw Bag of Words “BOW” Count Vectors. Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of Linear Discriminant Analysis “LDA” while working with TFIDF Vectors and BOW Count Vectors here in the Snapshot. I hope you will gain insights about the Implementation of LDA Classifier along with LDIA Topic Vectors and BOW count Vectors. Incase you want to see my Notebook, I have presented the overall Implementation of Semantic Analysis with LSA, LDA and LDIA with proper Documentation here: Excited about the days to come!! - Books: - Natural Language Processing in Action - Semantic Analysis\n\nDay25 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have started learning and Implementing Neural Networks and Deep Learning for Natural Language Processing. I have completed the Implementation of LSA and LDIA in Semantic Analysis along with LDA. I have read the topics such as Neural Networks and Perceptrons, Gradients, Local and Global Minimum and Backpropagation under the hood of Neural Networks and Deep Learning. Actually, I have primarily focused on reading the topics needed to understand the Neural Networks and Deep Learning rather than Implementing the concepts. I have presented the Simple workflow of Neural Networks using Keras API. I will be Implementing the Keras API in Natural Language Processing from today. I hope you will also spend some time to learn the basic topics which I have mentioned above to understand the Neural Networks and Deep Learning. Excited to learn and Implement Neural Networks for NLP in coming days!! - Book: - Natural Language Processing in Action\n\nDay26 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Word Vectors, Softmax Function, Negative Sampling, Document Similarity with Doc2Vec and Google’s Word2vec, GloVe and Facebook’s FastText Models which were pretrained on Billions of Text Data. I have primarily focused on learning and Implementing the Word2vec pretrained Model today. I am continuing my learning journey along with the book, Natural Language Processing in Action. Word2vec is a Model for Natural Language Processing. The Word2vec Algorithm uses a Neural Network Model to learn Word associations from a large corpus of text. Once Word2vec Model is trained, It can detect Synonymous words or suggest additional words for a partial sentence. Word2vec is a group of related Models that are used to produce Word Embeddings. I have presented the Implementation to access the Google’s Word2vec pretrained Model and it’s basic Functions and the process to create own Domain Specific Word2vec Model. I have also presented the Implementation of Doc2vec Model here in the Snapshot. I hope you will also spend so time to learn about Word2vec pretrained Model. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay27 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as CNN building blocks, Step size or Stride, Filter Composition, Padding, Convolutional Pipeline, Learning, Narrow Windows and Implementing the Keras API under the hood of Convolutional Neural Network for NLP. I have started working on the Sentiment Analysis of Large Movie Review Dataset which was compiled for the 2011 paper “Learning Word Vectors for Sentiment Analysis”. Since, It is a very large Dataset, I have used just the subset of the Dataset. I will be Implementing CNN for this Project. I have presented the basic Implementations of approaching the Dataset such as Importing the Dependencies, Processing the Dataset with Tokenization and Google News pretrained Model Vectorization and Splitting the Dataset into Training set and Test set. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come !! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n \nDay28 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as Convolutional Neural Network Architecture, Pooling, Dropout, CNN parameters, Optimization and Training CNN Model under the hood of Convolutional Neural Network for Natural Language Processing. I have presented the overall Implementation of Convolutional Neural Network here in the Snapshot. I have also presented the short info of all the parameters mentioned in the CNN Model and the process of Compiling and Training the Model as well. I hope you will gain insights about the Implementation of CNN Model in Sentiment Analysis. Actually, It is the continuation of yesterday’s Snapshots. I hope you will also spend some time working on it. I have completed working on the Sentiment Analysis of Large Movie Review Dataset. I have prepared a Model using Convolutional Neural Network which can classify the Sentiment of Text Data. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay29 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Simple Recurrent Neural Network as well as Bidirectional Recurrent Neural Network. Here, I have covered the topics such as Backpropagation through Time, Hyperparameters, Statefulness, Bidirectional Networks and various other topics similar to Convolutional Neural Network mentioned in the previous Snapshot. I have Implemented the Recurrent Neural Network in the same Large Movie Review Dataset to predict the Sentiment of Text Data. I have presented the overall Implementation of RNN Model here in the Snapshot. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of RNN Model in Sentiment Analysis. Although the Implementation of RNN Model is not so appreciable, I hope you will spend some time understanding the working principle of RNN and working on the same. I have completed working on the Sentiment Analysis of Large Movie Review Dataset using Simple RNN as well as Bidirectional RNN. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay30 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Long Short Term Memory or LSTM. I have covered the topics such as LSTM working principle, Backpropagation through time, Keras API and various other topics similar to CNN and RNN as mentioned in the previous Snapshots under the hood of NLP. Actually, I have primarily focused on Implementing the LSTM Model in the same Large Movie Review Dataset to compare the effectiveness of CNN, RNN and LSTM on Sentiment Analysis of Text Data. Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. I have presented the Simple Implementation of Long Short Term Memory or LSTM Model in the same Large Movie Review Dataset. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of LSTM Model in Sentiment Analysis. I have completed working on the same with LSTM Model. Excited about the days ahead!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay31 of 66DaysOfData! - Long Short Term Memory or LSTM: Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. LSTM is applicable for Text Generation as well. Today I have read and Implemented about Generating the Text using Long Short Term Memory or LSTM. I have prepared a Model using LSTM which generates the Text similar to William Shakespeare’s writing. I have used the Gutenberg Corpus which contains the 3 plays of Shakespeare to train the Neural Network. I have presented the Implementation of LSTM Model as well as the Implementation of Keras API in Text Generation here in the Snapshot. I have also presented the Snapshot of Generated Text with the help of LSTM Model. I have completed working on Generating Text with the help of LSTM Model. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n \nDay32 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented the topics such as Encoder and Decoder Architecture in Sequence to sequence Models, Thought Vector, Decoding Thought using LSTM, Sequence Encoder and Keras Functional API in assembling Sequence to Sequence Pipeline under the hood of Natural Language Processing. I have started working on building a Chatbot using Sequence to sequence Neural Networks and Keras Functional API. I have presented the simple Implementation of processing the Text Corpus and few steps to make the Text Data ready to train the Sequence to sequence Chatbot here in the Snapshot. I will be using the Cornell Dialog Dataset for training the Chatbot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Chatbot with Sequence to sequence Networks\n\nDay33 of 66DaysOfData! - Sequence to Sequence Model: Sequence to Sequence Neural Networks can be built with a modular and reusable Encoder and Decoder Architecture. The Encoder Model generates a Thought Vector which is a Dense and fixed Dimension Vector representation of the Data. The Decoder Model use Thought Vectors to generate Output Sequences. In my journey of Natural Language Processing, Today I have read and Implemented various topics under Sequence to Sequence Networks. I have continued working on the Chatbot using Sequence to Sequence Learning. I have used the Keras Functional API and Cornell Dialog Dataset for Training the Model. I have presented the Implementation of Thought Encoder and Thought Decoder using Keras Functional API here in the Snapshot. I have also presented the techniques for Training the Model and Generating the Response Sequences here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Chatbot with Sequence to sequence Networks\n\nDay34 of 66DaysOfData! - In my Journey of Natural Language Processing, Today I have read about Sentence Segmentation, Named Entity Recognition, Understanding Chatbot Approaches and few NLP Pipelines under the hood of Natural Language Processing. I have also started reading the book Natural Language Processing with PyTorch. Actually, I had never worked with PyTorch and never found any particular reason to start with PyTorch. But, Today I got motivated to start Natural Language Processing with PyTorch. I will be reading and Implementing Natural Language Processing with PyTorch from today. I have presented the simple Implementation of AIML Bot and the Vectorization concept here in the Snapshot. I am fond of revisiting the small concepts again and again so that I won’t get stuck while Implementing in real problems. I am so excited to start Natural Language Processing with PyTorch. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch\n \nDay35 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented The Supervised Learning Paradigm, Computational Graphs, PyTorch Basics and various NLP fundamentals such as Tokenization, Lemmatization, Tagging and Semantics. I have also read and Implemented about Activation Functions such as Sigmoid, Tanh, ReLU, Softmax and Loss Functions such as Mean Squared Error and Cross Entropy under the hood of Natural Language Processing with PyTorch. I have presented the Implementation of PyTorch in various Activation Functions and Loss Functions along with few preliminaries for Understanding and working with PyTorch here in the Snapshots. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch\n \nDay36 of 66DaysOfData! - Regularization: In Machine Learning, Regularization is the process of adding Information in order to solve a well posed problems or to prevent Overfitting. In my journey of Natural Language Processing, Today I have read and Implemented about Model Selection approaches, Choosing Loss Functions, Choosing Optimizers, Gradient Based Supervised Learning, Evaluation Metrics, Regularization and Early Stopping under the hood of Natural Language Processing with PyTorch. I have started working on the YELP Reviews Dataset and the Neural Networks will be Implemented using PyTorch. I have presented some Data Preprocessing Techniques which I have Implemented while working with YELP Reviews Dataset here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis\n\nDay37 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about PyTorch’s Data Representation, The Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of Natural Language Processing with PyTorch. I am continuing my journey along with the book Natural Language Processing with PyTorch. - The Vocabulary Class: The Vocabulary Class not only manages the Bijection i.e Allowing user to add new Tokens and have the Index auto increment but also handles the special token called UNK which stands for Unknown. By using the UNK Token, It will be easy to handle Tokens at Test time that were never seen in Training Instance. - The Vectorizer Class: The second stage of going from a Text Dataset to a vectorized minibatch is to iterate through the Tokens of an Input Data Point and convert each Token to its Integer form. The result of this iteration should be a Vector. Because this Vector will be combined with Vectors from other Data points, there is Constraint that the Vectors produced by the Vectorizer should always have the same length. - The DataLoader Class: The Final step of Text to Vectorized minibatch pipeline is to actually group the Vectorized Datapoints. Because grouping into mini batches is a viatal part of Training the Neural Networks, PyTorch provides a built in class called DataLoader for coordinating the Process. - I have presented the Implementation of Dataset Class using PyTorch here in the Snapshot. I have been working on the Implementation of Dataset Class, Vectorizer Class and DataLoader Class and I feel quite overwhelmed with the complexity of PyTorch because I am not familiar with PyTorch Framework. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis.\n \nDay38 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Perceptron Classifier, The Training Routine Class using PyTorch’s Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using PyTorch under the hood of “Natural Language Processing with PyTorch”. I am continuing my journey along with the book, Natural Language Processing with PyTorch. Today, I have continued working with YELP Review Dataset for Sentiment Analysis using PyTorch. I have presented the simple Implementation of PyTorch in Training the Classifier Model along with the process of Instantiating the Dataset, Model, Loss, Optimizer and Training State here in the Snapshots. Actually, It is the continuation of yesterday’s Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis.\n \nDay39 of 66DaysOfData! - **Long Short Term Memory or LSTM Model: Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. In my journey of Natural Language Processing, Today I have started working on new Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. I will be working on the same until I finish it completely. Apart from that, Today I have watched some videos on YouTube and read some Notebooks in Kaggle under the hood of Natural Language Processing. I have presented the overall Implementation of TensorFlow and TensorBoard in Processing the Data such as Tokenization and Encoding and the techniques for preparing LSTM Model here in the Snapshots. I hope you will gain some insights and work on the same. Excited about the days to come!! - Amazon Reviews Analysis\n \nDay40 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have continued working on yesterday’s Notebook which was of Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. Actually, I spent most of my time in Training the Model and I tried to optimize the Model to increase the speed of Training but still the Training procedure took most part of my time using GPU as well. Apart from that, Today I have spent some time working on Greedy Programming Algorithms. Actually I got a chance to interview from one of the Tech Giants as well. I had prepared a Model using LSTM which was trained on Amazon Reviews Dataset. This Snapshot is the continuation of yesterday’s Snapshots. So, I have presented some basic workflow of Model Evaluation and deploying the Trained Model on unseen Text Data for Analysis. I have also presented the simple technique of Data Visualization for evaluating the Model here in the Snapshot. I hope you will gain some insights and work on the same. Excited about the days to come!! - Amazon Reviews Analysis\n\nDay41 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Feed Forward Networks for Natural Language Processing using PyTorch. I have covered the topics such as The Multi Layer Perceptron or MLP, Simple XOR Functions with MLP using PyTorch and Softmax Activation Function under the hood of Natural Language Processing with PyTorch. I have started working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. I will be using PyTorch for building the Classifier Model. I have presented the Implementation of PyTorch in building the simple MLP Class here in the Snapshots. I have also presented the techniques for processing the raw Dataset for Surname Classification Project using PyTorch. I hope you will gain some insights and you will also spend some time learning about MLP and working on the same and get ready for building the Model. Excited about the days ahead !! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n \nDay42 of 66DaysOfData! - The Vectorizer Class: The Vocabulary converts individual tokens into Integers and The Surname Vectorizer is responsible for applying the Vocabulary and converting surname into Vectors. Surnames are sequence of characters and each character is an individual token in the Vocabulary. In my journey of Natural Language Processing, Today I have read and Implemented about PyTorch’s Data Representation, The Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of Natural Language Processing with PyTorch. I have continued working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. Today, I have worked out the Implementation of Dataset Class, Vectorizer Class, Vocabulary Class and Multi Layer Perceptron or MLP Classifier Model. I have presented the Implementation of Vectorizer Class using PyTorch here in the Snapshot. I hope you will gain some insights from here and you will also spend some time working on the same. Excited about the days to come!! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n\nDay43 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about MLP Classifier, The Training Routine Class using PyTorch’s Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using PyTorch under the hood of Natural Language Processing with PyTorch. I have continued working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. Today, I have worked out the Implementation of MLP Classifier, The Training Routine Class, Inference and Inspection of the Model prepared using PyTorch. I have presented the simple Implementation of MLP Classifier Model along with the Snapshot of the Inference and Inspection of the Model Evaluation using PyTorch. Actually, It is the continuation of yesterday’s Snapshot. I hope you will gain some insights and you will also spend some time working on the same. Excited about the days to come !! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n \nDay44 of 66DaysOfData! - Logistic Regression: Logistic Regression is a Statistical Model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In Regression Analysis, Logistic Regression is estimating the parameters of a Logistic Model in a form of Binary Regression. In my journey of Natural Language Processing, Today I have started learning from Natural Language Processing Specialization on Coursera. I have just started learning the first course and I will spend couple of weeks in this Specialization. I have covered the topics such as Logistic Regression and Naive Bayes along with various Fundamental Preprocessing steps under the hood of Natural Language Processing. This Specialization is managed and organized by the team of Andrew Ng i.e deeplearning.ai so that I will gain more exposure to Mathematics behind every topics which has very high Importance. I have presented the Implementation of Naive Bayes Classifier along with Testing procedure here in the Snapshot. I hope you will also spend some time going through this Specialization and I am so much excited about the days to come !! - Course: - Natural Language Processing Specialization: Coursera\n\nDay45 of 66DaysOfData! - KNearest Neighbors: The KNN Algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. KNN works by finding the distances between a Query and all the examples in the Data, selecting the specified number of examples or K closest to the Query, then votes for the most Frequent Label or Averages the Labels. In my journey of Natural Language Processing, Today I have completed the first course i.e Natural Language Processing with Classification and Vector Spaces from Natural Language Processing Specialization on Coursera. I have covered the topics such as Vector Space Models, Euclidean Distance, Cosine Similarity, PCA, KNN, Logistic Regression, Naive Bayes and many more in this course. I have presented the simple Implementation of KNN along with techniques for creating Hash table here in the Snapshots. Actually, I have presented this Implementation on the basis of this Course for self understanding so I have not included all the dependencies here in the Snapshot. I hope you will also spend some time going through this Specialization. Excited about the days ahead !! - Course: - Natural Language Processing Specialization: Coursera - Natural Language Processing with Classification and Vector Spaces\n\nDay46 of 66DaysOfData! - Minimum Edit Distance: The Minimum Edit Distance between two strings is defined as the minimum number of Editing Operations like Insertion, Deletion and Substitution needed to transform one string into another. It’s working principle is applicable in building the Auto Correction Model. In my journey of Natural Language Processing, Today I have started learning and Implementing NLP from the second course of Natural Language Processing Specialization on Coursera i.e Natural Language Processing with Probabilistic Models. I have covered the topics such as Autocorrect Models using Minimum Edit Distance Algorithm, POS Tagging, Markov’s Models and The Viterbi Algorithm under the hood of Natural Language Processing. I will spend couple of weeks in this Specialization. I have presented the simple Implementation of Minimum Edit Distance Algorithm whose working principle is applicable in Auto Correction Model here in the Snapshot. I hope you will also spend some time learning about the topics mentioned above. I hope you will also spend some time in this Specialization. I am excited about the days to come!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay47 of 66DaysOfData! - Continuous Bag of Words: In the Continuous Bag of Words Model, The distributed representations of context or surrounding words are combined to predict the word in the middle. The Model predicts the current word from a window of surrounding context words. In my journey of Natural Language Processing, Today I have learned about Ngrams and Probabilities, Sequence Probabilities, Smoothing, Word Embeddings, CBOW, Cleaning and Tokenization from the course Natural Language Processing with Probabilistic Models which is the second course in Natural Language Processing Specialization on Coursera. I have completed the two Courses in this Specialization. I have presented the simple Implementation of Initializing the Model, Softmax Activation Function, Forward Propagation, Cost Function and Back Propagation for training the CBOW Model here in the Snapshot. It presents the Mathematics behind simple Neural Network which is crucial for understanding the Implementation of Neural Networks and Deep Learning. I hope you will gain some insights and you will also spend some time learning about the topics mentioned above. Excited about days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay48 of 66DaysOfData! - Gated Recurrent Unit: A Gated Recurrent Unit or GRU is a gating mechanism in RNN similar to a Long Short Term Memory or LSTM unit but without an output gate. GRU solves the vanishing Gradient problem that can come with standard RNN by using an Update Gate and a Reset Gate. In my journey of Natural Language Processing, Today I have started learning and Implementing NLP from the third course of Natural Language Processing Specialization on Coursera i.e Natural Language Processing with Sequence Models. I have covered the topics such as Trax and Neural Networks, Dense and ReLU Layers, Serial Layers, RNN and GRU using Trax, Deep and Bidirectional RNN under the hood of Natural Language Processing. Neural Networks and Trax: Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks. I have presented the simple Implementation of Training GRU Model using Trax here in the Snapshot. I hope you will gain some insights from here and you will also spend some time learning about Trax Frameworks in Natural Language Processing and Deep Learning. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay49 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about LSTMs and Named Entity Recognition, RNNs and Vanishing Gradients, Siamese Neural Networks, Triplets and One Shot Learning from the course Natural Language Processing with Sequence Models which is the part of Natural Language Processing Specialization on Coursera. I have completed three Courses in this Specialization. I have started working with Quora Questions Answers Dataset to build a LSTM Model that can Identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I will build this Model using Trax Framework which is maintained by Google Brain Team and good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. I have presented the simple Implementation of Data Preparation for training the LSTM Model using Quora Dataset here in the Snapshot. I hope you will gain some insights and you will also start working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n\nDay50 of 66DaysOfData! - Siamese Neural Network: Siamese Neural Network is an Artificial Neural Network which uses the same weight while working in tandem on two different input vectors to compute comparable output vectors. In my journey of Natural Language Processing, Today I have learned and Implemented about Data Generators or Iterators, Siamese Neural Networks, LSTMs and Triplet Loss under the hood of Natural Language Processing. I have continued working with Quora Questions Answers Dataset to build a Model using LSTM that can identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I have presented the Implementation of Siamese Neural Network using LSTM Model along with the Implementation of Triplet Loss here in the Snapshots. I have also presented the Implementation of Training the Model using Data Generators and other dependencies. I have presented all the Implementations using Trax Framework here. I hope you will gain some insights from here and you will also continue working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n \nDay51 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned and Implemented about Siamese Neural Networks and LSTMs, Implementation of Trax in Siamese Neural Networks under the hood of Natural Language Processing. I have completed working with Quora Questions Answers Dataset to build a LSTM Model that can identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I have build a Model using Trax which can identify the Duplicate Questions. Neural Networks and Trax: Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks. I have presented the Implementation of Siamese Neural Network using LSTM Model that can identify the Similar or Duplicate Questions here in the Snapshots. I have also presented the output of the Model here which fascinates me a lot. I hope you will gain some insights from here and you will also spend some time working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n \nDay52 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Sequence to Sequence Models, Alignment, Attention Models, BLEU or Bilingual Evaluation Understudy, ROUGE or Recall Oriented Understudy, Greedy Decoding, Random Sampling, Temperature Parameter, Beam Search Decoding, Minimum Bayes Risk or MBR and Teacher Forcing Algorithm in LSTM from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization. Teacher Forcing: Teacher Forcing is the technique where the target word is passed as the next input to the Decoder. Training with Teacher Forcing converges faster. I have presented the simple Implementation of Neural Machine Translation Model which uses Attention along with the Implementation of Preparing Attention Input using Encoder and Decoder Activations here in the Snapshots. I have presented this Implementation on the basis of this course for self understanding so I have not included all the dependencies here in the Snapshot. I hope you will also spend some time going through the topics mentioned above. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Teacher Forcing in RNNs\n\nDay53 of 66DaysOfData! - Transformer and Natural Language Processing: The Transformer is a Deep Learning Model which are designed to handle Sequential Data such as Translation and Summarization and allows much more parallelization than RNNs such as LSTM. In my journey of Natural Language Processing, Today I have learned about Transformers and it’s Applications, Dot Product, Multi Head and Causal Attentions, The Transformer Decoder and Summarizer, State of Art Transformers such as GPT2 or Generative Pretraining for Transformer, BERT or Bidirectional Encoder Representations from Transformer, T5 or Text to Text Transfer Transformer and Multitask Training Strategy and GLUE Benchmark from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization on Coursera. I have presented the simple Implementation of Transformer Language Model using Trax here in the Snapshot. I have also presented the Implementation of PositionalEncoder, FeedForward and DecoderBlock which returns the list of Layers required for the Transformer Model here. I hope you will gain some insights and you will also spend some time learning about the Transformer Model. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay54 of 66DaysOfData! - Reformer: The Efficient Transformer or The Reformer is a Transformer Model which combines two crucial techniques to solve the problems of Attention and Memory allocation that limit Transformer Application to Long context windows. Reformer uses LSH to reduce the complexity of attending over Long sequences and Reversible residual layers to more efficiently use the memory available. In my journey of Natural Language Processing, Today I have learned about Long Sequential Data, Transformer Complexity, Locality Sensitive Hashing or LSH Attention, Reversible Residual Layers and Reformer from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization. I have presented the simple Implementation of Reformer Language Model along with the Implementation of Training the Reformer Model using Trax here in the Snapshots. I have also presented the Implementation of Forward Reversible Layer and Reverse Reversible Layer here. I hope you will gain some insights and you will also spend some time learning about the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Natural Language Processing Specialization Completion\n\nDay55 of 66DaysOfData! - Transfer Learning: Transfer Learning is a process where a Model is first pretrained on a Data rich task before being fine tuned on a Downstream Task and Transfer Learning has emerged as a powerful technique in Natural Language Processing or NLP. The effectiveness of Transfer Learning has given rise to a Diversity of approaches, methodology, and practice. In my journey of Natural Language Processing, Today I have started reading the Papers of Transformers and Natural Language Processing which is named as Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer. I have presented the Implementation of Encoder and Decoder Class which are the key components in any Dominant Sequence Models. I hope you will also spend some time reading about the Transformer from the Paper mentioned above. Excited about the days ahead!! - Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer\n\nDay56 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Paper about Deep Learning and Text Classification which is named as Deep Learning Based Text Classification: A Comprehensive Review. I have read about various Text Classification Tasks such as Sentiment Analysis, News Categorization, Topic Analysis and Question Answering. I have also read about various Deep Learning Models for Text Classification such as Feed Forward Neural Networks, RNN Based Models, CNN Based Models, Capsule Neural Networks, Models with Attention, Transformers, Graph Neural Networks, Siamese Neural Networks and Hybrid Models from this Paper. I have presented the simple Implementation of Layer Normalization, Sublayer Connection, Encoder Layer and Feed Forward Layer using PyTorch here in the Snapshots. I hope you will also spend time reading about Deep Learning and Text Classification from the Paper mentioned above. I am excited about the days ahead!! - Deep Learning Based Text Classification: A Comprehensive Review\n\nDay57 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Paper about Deep Learning and Text Classification which is named as Deep Learning Based Text Classification: A Comprehensive Review. I have completed reading this Paper. I have read about Unsupervised Learning using Autoencoders, Adversarial Training, Reinforcement Learning and few Popular Datasets such as YELP and IMDB. I have also read about Popular Metrics for Text Classification such as Accuracy and Error Rate, Precision, Recall and F1 Score from this Paper. I have also read one of the most popular Articles named The Illustrated Transformer by Jay Alammar. It teaches about the Transformer from Scratch. I have read the topics such as Encoder, Decoder, Self Attention, Feed Forward and Multi Head Attention in this Article. I have presented the Implementation of Decoder Class, DecoderLayer with FeedForward and Subsequent Mask using PyTorch here in the Snapshots. I hope you will spend some time reading the Paper and Article mentioned above. Excited about the days ahead!! - Deep Learning Based Text Classification: A Comprehensive Review - The Illustrated Transformer\n\nDay58 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read some Articles about Deep Learning and Natural Language Processing. I have read an Article named NLP’s ImageNet Moment which is written by Sebastian Ruder. Here, I have read about the recent advances in NLP such as ULMFiT, ELMO, OpenAI Transformer and the ImageNet for Language. Similarly, I have read another Article named as The Illustrated BERT, ELMO and Co. which is written by Jay Alammar. Here, I have read about BERT Model Architecture, Word Embeddings, Transfer Learning and the topics mentioned above. I have also read another Article named Generalized Language Models which is written by Lilian Weng. Here, I am reading about Language Models and I am still reading this Article. I have presented the Implementation of Attention and Multi Head Attention Class using PyTorch here in the Snapshot. Actually, It is the continuation of yesterday’s Snapshot. I hope you will also spend some time reading the Articles mentioned above. Excited about the days ahead!! - NLP’s ImageNet Moment - The Illustrated BERT, ELMO and Co. - Generalized Language Models\n\nDay59 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read Papers about Skip Gram and NLP. I have read a Paper which is named as Efficient Estimation of Word Representations in Vector Space. Here, I have read about Model Architectures, Feed Forward Neural Net Language Model, Recurrent Neural Net Language Model, Log Linear Models, CBOW Models, Continuous Skip Gram Model and various other related topics. I have also read the Paper which is named as Distributed Representations of Words and Phrases and their Compositionality. Here, I have read about Skip Gram Model, Hierarchical Softmax, Negative Sampling, Subsampling and Additive Compositionality. I have presented the Implementation of Positionwise Feed Forward Class, Embedding, Positional Encoding and Feed Forward using PyTorch here in the Snapshot. Actually, It is the continuation of yesterday’s Snapshot. I hope you will also spend some time reading the Topics and Papers mentioned above about Skip Gram Model. Excited about the days ahead!! - Efficient Estimation of Word Representations in Vector Space - Distributed Representations of Words and Phrases and their Compositionality\n\nDay60 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read Papers about GloVe and Recurrent Neural Network Language Model. I have read a Paper which is named as GloVe : Global Vectors for Word Representation. Here, I have read about Matrix Factorization Methods, The GloVe Model and Complexity of the Model, Evaluation Methods, Word Analogies, Model Analysis and various other related topics. Similarly, I have read another Paper which is named as A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. Here, I have read about Language Modeling and Initialization, Speech Recognition and various other related topics. I have presented the Implementation of Transformer Model, Batches and Masking using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. I am excited about the days ahead!! - GloVe : Global Vectors for Word Representation - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\n\nDay61 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Paper about Neural Machine Translation with Attention and Dynamic Memory Network. I have read the Paper which is named as Neural Machine Translation By Jointly Learning To Align and Translate. Here, I have read the Topics related to Neural Machine Translation such as RNN Encoder and Decoder, Translation and to Align and Qualitative and Quantitative Analysis and more related to the same. I have also started reading another Paper which is named as Dynamic Memory Networks for Natural Language Processing. I have just started reading this Paper and I will complete it soon. I have presented the Implementation of Training the Loop of Transformer Model along with the Implementation of Batching using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. I am excited about the days ahead!! - Neural Machine Translation By Jointly Learning To Align and Translate - Dynamic Memory Networks for Natural Language Processing\n\nDay62 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Dynamic Memory Networks and Recursive Neural Networks. I have read the Paper which is named as Dynamic Memory Networks for Natural Language Processing. Here, I have read about the Dynamic Memory Networks and Episodic Memory Module, Attention and Memory Update Mechanism and various Topics related to the same. I have also read the Paper which is named as Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. Here, I have read about Semantic Vector Space and Compositionality in Vector Space, Recursive Neural Tensor Networks, Backpropagation and various Topics related to the same. I have presented the Implementation of Optimizer along with Learning Rates using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Dynamic Memory Networks for Natural Language Processing - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n\nDay63 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Stanza: NLP Toolkit and Transfer Learning to study Linguistic Structure. I have read the Paper which is named as Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Here, I have read about Neural Multilingual NLP Pipeline, Dependency Parsing and Lemmatization, NER and various Topics related to the same. I have also read the Paper which is named as Learning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models. Here, I have read about LSTM Architecture and Training, Random Baselines, Recursive Structure, Non Linguistic Structure and various Topics related to the same. I have presented the Implementation of Label Smoothing and Feed Forward using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Stanza: A Python Natural Language Processing Toolkit for Many Human Languages - Learning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models\n\nDay64 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Long Short Term Memory and Stanford CoreNLP Toolkit. I have read the Paper which is named as Improved Semantic Representations From Tree Structured Long Short Term Memory Networks. Here, I have read about Bidirectional LSTM and Multilayer LSTM, Tree Structured LSTM, Semantic Relatedness and Sentiment Classification and various Topics related to the same. I have read another Paper which is named as The Stanford CoreNLP Natural Language Processing Toolkit. Here, I have read about Model Design and Development, Elementary Usage, Annotators, Tokenization and Lemmatization and various Topics related to the same. I have presented the Implementation of Data Generator, Loss Computation and Greedy Decoding in Transformer Model using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Improved Semantic Representations From Tree Structured Long Short Term Memory Networks - The Stanford CoreNLP Natural Language Processing Toolkit\n\nDay65 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Zero Shot Learning and Syntax Sensitive Dependencies in LSTM. I have read the Paper which is named as Zero Shot Learning Through Cross Modal Transfer. Here, I have read about Zero Shot and One Shot Learning, Word and Image Representations, Zero Shot Learning Models, Novelty Detection and various Topics related to the same. I have also read another Paper which is named as Assessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies. Here, I have read about LSTM Model and Baselines, Number Prediction, Relative Clauses, Language Modeling and various Topics related to the same. I have presented the Implementation of Greedy Decoding and Iterators using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Zero Shot Learning Through Cross Modal Transfer - Assessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies\n\nDay66 of 66DaysOfData! - Today I have read the Paper which is named as Finding Universal Grammatical Relations in Multilingual BERT. Here, I have read about BERT, Cross Lingual Probing, Universal Grammatical Relations, Finer Grained Analysis, Determiners and various Topics related to the same. I have also spent some time going through the Documentation of my Journey in GitHub. Today, I have completed this Journey and I am so grateful and thankful to meet lots of amazing persons along the way. Actually, Ken Jee is one of the amazing persons who started this Journey. He has always been supportive and a great example to the Data Science Aspirants and I am so thankful to him. I have been Learning and Implementing Natural Language Processing for couple of months now. I will continue my Learning with new Journey and new Topics. I hope you have also learned something interesting in Natural Language Processing until this day. Personally, I am so thankful to you, the one who has come along this way and supported me since the start of the Journey. We will start to learn new Topics with new hope and energy. Thank you!! - Finding Universal Grammatical Relations in Multilingual BERT"
  },
  {
    "objectID": "posts/MS/mistral.html",
    "href": "posts/MS/mistral.html",
    "title": "Model Sharding",
    "section": "",
    "text": "Introduction\nModel Sharding is a technique used to distribute the model parameters, gradients, and optimizer states across multiple GPUs. In this technique, the model is divided into multiple portions, and each portion is assigned to a different GPU.\n\n\nWhy Model Sharding?\nIn the traditional data parallelism approach, each GPU maintains a full copy of the model parameters, gradients, and optimizer states which requires the entire model to fit into the memory of each GPUs. This becomes challenging when the model size is very large and the memory of each GPU is limited. Model sharding is a solution to this problem. It allows the model to be distributed across multiple GPUs, and each GPU only maintains a portion of the model which allows the model to scale to larger sizes.\n\n\nModel Sharding in Mistral\nMistral Architecture Details based on source code:\n\nThe architecture of Mistral contains an Embedding Layer, Transformer Block that contains [Attention Layers, MOE Layers or FeedForward Layers with or without Normalization Layers], and a Normalization Layer and a Linear Output Layer with Softmax.\nThe Transformer Block is repeated n times where n is the number of layers in the model. So, the basic architecture of Mistral becomes Embedding Layer -&gt; Transformer Block -&gt; Normalization Layer -&gt; Linear Output Layer with Softmax.\n\n\n\n\nFig a. Basic architecture of Mistral\n\n\nModel Sharding in Mistral is implemented in such a way that a number of layers of the model are assigned to one GPU, and the rest to other GPUs and so on which basically distributes the model across multiple GPUs. Suppose we have the model with 32 layers and using 4 GPUs, then 8 layers will be assigned to each GPU. The Embedding Layer along with the first 8 layers of Transformer Block (TB) will be assigned to the first GPU, the next 8 layers of TB to the second GPU, the next 8 layers of TB to the third GPU, and the last 8 layers of TB with the Normalization Layer and Linear Output Layer with Softmax will be assigned to the fourth or last GPU.\n\n\n\nFig b. Model Sharding in Mistral with 32 layers and 4 GPUs.\n\n\n\n\nConclusion\nModel Sharding is a technique used to distribute the model across multiple GPUs where the output of one GPU is used as the input to the next GPU. This technique allows the model to scale to larger sizes. Mistral uses this technique to distribute the model across multiple GPUs. This technique is very useful when the model size is very large (in terms of parameters in millions or trillions and the memory of each GPU is limited to fit the entire model in each single GPUs.)\n\n\nReferences\n\nMistral Source Code\nLlama2 Implementation\nAttention Is All You Need"
  },
  {
    "objectID": "posts/Mistral/mistral_long.html",
    "href": "posts/Mistral/mistral_long.html",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "",
    "text": "Attention mechanism is a key component in Transformer models. It allows the model to focus on different parts of the input sequence and derive the relationship between tokens of the input sequence. In other words, Attention refers to the mechanism of sharing the information between tokens in the input sequence. For the computation of attention in Transormer Models:\n\nWe multiply the query matrix with the transpose of the key matrix to get the attention scores. The attention scores are then fed to the softmax function to get the attention weights. These attention weights are then multiplied with the value matrix to get the context vector or attention output.\n\nIn vanilla Transformer models, the attention mechanism uses causal mask which means that each token in the input sequence can only attend to itself and all the tokens before it. This approach ensures that the model is causal and it can only use the information from the past tokens to predict the future tokens. However, the number of operations in the attention mechanism is quadratic with respect to the length of the input sequence and the memory requirement is also linear with respect to the length of the input sequence which incurs higher latency and smaller throughput from the model at inference time.\n\n\n\nFig a. Vanilla Attention with causal mask taken from Mistral paper.\n\n\nSo, to address these limitations, Sliding Window Attention mechanism was used in the Mistral model. The Sliding Window Attention is a variant of the attention mechanism which uses a fixed window size to select the tokens from the past to attend for computation of attention output. In other words, each token in the input sequence can only attend at most W tokens from the past where W is the window size. This approach increases the inference speed and reduces the memory usage of the model. This mechanism still ensures that the model is causal but it does not use the entire tokens from the past, whereas it uses a fixed number of tokens from the past to compute the attention output.\n\n\n\nFig b. Sliding Window Attention with window size 3.\n\n\nIn the Sliding Window Attention, the tokens outside the sliding window still influence the next word prediction because at each attention layer, the information can move forward by W tokens at most, but after the next layer, the information can move forward by 2W tokens and so on, as the hidden state in position i of the layer k, attends to all the hidden states from position i-W to i of the layer k-1 since the layers are stacked on top of each other in Transformer models.\nSuppose, we have a input sequence with 10 tokens, a Transformer model with 4 layers and using a window size of 4,then the information flow from one layer to another using Sliding Window Attention is given below:\n\nIn first layer, we use the first 4 tokens to compute the attention output for the next token.\nIn second layer, we use the information from the previous 4 tokens and the next 3 tokens to compute the attention output for the next token.\nIn third layer, we use the information from the previous tokens and the next 3 tokens compute the attention output for the next token.\nIn fourth layer, we use the information of the entire input sequence to compute the attention output for the next token. At this point, the information has propagated from the first token to the last token in the input sequence.\n\n\n\n\nFig c. Information flow from one layer to another using Sliding Window Attention."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#sliding-window-attention",
    "href": "posts/Mistral/mistral_long.html#sliding-window-attention",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "",
    "text": "Attention mechanism is a key component in Transformer models. It allows the model to focus on different parts of the input sequence and derive the relationship between tokens of the input sequence. In other words, Attention refers to the mechanism of sharing the information between tokens in the input sequence. For the computation of attention in Transormer Models:\n\nWe multiply the query matrix with the transpose of the key matrix to get the attention scores. The attention scores are then fed to the softmax function to get the attention weights. These attention weights are then multiplied with the value matrix to get the context vector or attention output.\n\nIn vanilla Transformer models, the attention mechanism uses causal mask which means that each token in the input sequence can only attend to itself and all the tokens before it. This approach ensures that the model is causal and it can only use the information from the past tokens to predict the future tokens. However, the number of operations in the attention mechanism is quadratic with respect to the length of the input sequence and the memory requirement is also linear with respect to the length of the input sequence which incurs higher latency and smaller throughput from the model at inference time.\n\n\n\nFig a. Vanilla Attention with causal mask taken from Mistral paper.\n\n\nSo, to address these limitations, Sliding Window Attention mechanism was used in the Mistral model. The Sliding Window Attention is a variant of the attention mechanism which uses a fixed window size to select the tokens from the past to attend for computation of attention output. In other words, each token in the input sequence can only attend at most W tokens from the past where W is the window size. This approach increases the inference speed and reduces the memory usage of the model. This mechanism still ensures that the model is causal but it does not use the entire tokens from the past, whereas it uses a fixed number of tokens from the past to compute the attention output.\n\n\n\nFig b. Sliding Window Attention with window size 3.\n\n\nIn the Sliding Window Attention, the tokens outside the sliding window still influence the next word prediction because at each attention layer, the information can move forward by W tokens at most, but after the next layer, the information can move forward by 2W tokens and so on, as the hidden state in position i of the layer k, attends to all the hidden states from position i-W to i of the layer k-1 since the layers are stacked on top of each other in Transformer models.\nSuppose, we have a input sequence with 10 tokens, a Transformer model with 4 layers and using a window size of 4,then the information flow from one layer to another using Sliding Window Attention is given below:\n\nIn first layer, we use the first 4 tokens to compute the attention output for the next token.\nIn second layer, we use the information from the previous 4 tokens and the next 3 tokens to compute the attention output for the next token.\nIn third layer, we use the information from the previous tokens and the next 3 tokens compute the attention output for the next token.\nIn fourth layer, we use the information of the entire input sequence to compute the attention output for the next token. At this point, the information has propagated from the first token to the last token in the input sequence.\n\n\n\n\nFig c. Information flow from one layer to another using Sliding Window Attention."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#rotating-buffer-cache",
    "href": "posts/Mistral/mistral_long.html#rotating-buffer-cache",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Rotating Buffer Cache",
    "text": "Rotating Buffer Cache\nRotating Buffer Cache is a mechanism used in the Mistral model which limits the size of KV cache to a fixed size. In my blog post on Understanding KV Cache, I have explained the needs and limitations of KV cache along with implementation. In the paper of Mistral, they mentioned that: when we use a sequence length of 32k tokens, the Rotating Buffer Cache reduces the cache memory usage by 8x, without impacting the model quality. While using KV cache, the memory usage of GPU or CPU increases linearly with respect to the length of the input sequence because we need to save KV cache for each layers of the model.\n\nRotating Buffer Cache for Sliding Window Attention\nWhen we use Sliding Window Attention mechanism, we have the fixed attention span of W tokens which means that we can limit the size of the KV cache using the Rotating Buffer Cache also called Rolling Buffer Cache. The cache has a fixed size of W where W is the window size, and the Keys and Values for the timestep i are stored in the positions i mod W of the cache. Similarly, when the position i is larger than W, the previous Keys and Values are overwritten by the new Keys and Values in the cache. This approach keeps the cache size fixed and reduces the linear memory usage of the model with respect to the length of the input sequence.\n\n\n\nFig d. Rotating Buffer Cache with cache size W=4\n\n\nIn the above figure, the cache size is W=4 and we have 3 input sequences. The Keys and Values are stored by following operations:\n\nWe have 3 input sequences:\n\n“This is an example of …”\n“Mistral is a good …”\n“The cat sat on the mat …”\n\nFor the first input sequence, at timestep i, “an” is stored in the cache position 2 mod 4 = 2. At timestep i+1, “example” is stored in the cache position 3 mod 4 = 3. At timestep i+2, “of” is stored in the cache position 4 mod 4 = 0.\nFor the second input sequence, at timestep i, “is” is stored in the cache position 1 mod 4 = 1. At timestep i+1, “a” is stored in the cache position 2 mod 4 = 2. At timestep i+2, “good” is stored in the cache position 3 mod 4 = 3.\nFor the third input sequence, at timestep i, “on” is stored in the cache position 3 mod 4 = 3. At timestep i+1, “the” is stored in the cache position 4 mod 4 = 0. At timestep i+2, “mat” is stored in the cache position 1 mod 4 = 1."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#pre-filling-and-chunking",
    "href": "posts/Mistral/mistral_long.html#pre-filling-and-chunking",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Pre-Filling and Chunking",
    "text": "Pre-Filling and Chunking\nWhen we generate the sequence using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. The output of the model when we use self attention mechanism without KV cache is a sequence of tokens whereas the output of the model when we use KV cache is a required final token. Therefore, KV cache makes the inference faster.\nAt inference, the prompt sequence is known in advance, and we can pre-fill the KV cache with the prompt sequence. In the Sliding Window Attention mechanism, we have the fixed size of the cache and if the prompt sequence is larger than the cache size, we can chunk the prompt sequence into smaller sequences and pre-fill the cache with these smaller sequences. In this approach, we use the chunk size of W where W is the window size.\nSuppose, we have a prompt sequence “The cat sat on the mat and saw the dog go to” and the window size is W=4, then we can chunk the prompt sequence into smaller sequences as follows: - “The cat sat on” - “the mat and saw” - “the dog go to”\n\n\n\nFig e. Pre-fill and Chunking representation taken from Mistral\n\n\nIn the above figure, the prompt sequence is chunked into smaller sequences and the cache is pre-filled with these sequences. For the third chunk “the dog go to”: it attends itself using the causal mask which is the rightmost block, and it attends the previous block using the sliding window attention mechanism, and it does not attend any past tokens that is outside the window size of sliding window attention.\nTherefore, pre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences.\n\nBlockDiagonalCausalMask and BlockDiagonalMask during Pre-Filling\nWe use the Pre-Filling and Chunking mechanism to pre-fill the KV cache with the prompt sequence. Both the BlockDiagonalCausalMask and BlockDiagonalMask are used to pre-fill the cache and generate the mask of each chunk of the prompt sequence. The generated mask is then used to compute the attention by the Attention Layer in Mistral model. The generation of the mask is done by the following operations:\n\nLet’s say we have a prompt sequence “The cat sat on” and we want to generate the 4 new tokens. The cache size will be max_tokens + prompt_length + 1 = 4 + 4 + 1 = 9.\nSuppose, we consider the chunk size of 2, then the prompt sequence is divided into 3 chunks because while encoding the prompt sequence, we need to add the start token at index 0 during the inference, so the prompt sequence is divided into 3 chunks: “[start token] The,”cat sat”, “on”. You can find the implementation of chunking in here.\nSo, the prefill or mask for the first chunk is generated by BlockDiagonalCausalMask and the prefill or mask for the second (or subsequent chunks if any) is generated by BlockDiagonalMask. In our example above, we have the last chunk that doesn’t contain the complete tokens as the chunk size has only one token, and in such cases, the prefill or mask is generated by BlockDiagonalCausalWithOffsetPaddedKeysMask.\n\n\n\n\nFig. Mask sample generated by BlockDiagonalCausalMask.\n\n\n\n\n\nFig. Mask sample generated by BlockDiagonalMask during Pre-filling phase.\n\n\nSo, the BlockDiagonalCausalMask is used to pre-fill the cache with the first chunk of the prompt sequence and the BlockDiagonalMask is used to pre-fill the cache with the subsequent chunks of the prompt sequence. You can find the implementation of these masks in the cache.py."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation",
    "href": "posts/Mistral/mistral_long.html#blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "BlockDiagonalCausalWithOffsetPaddedKeysMask for Token Generation",
    "text": "BlockDiagonalCausalWithOffsetPaddedKeysMask for Token Generation\nWhen we generate the tokens using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. In Mistral, the BlockDiagonalCausalWithOffsetPaddedKeysMask is used to generate the mask when we feed a single token to the model to generate the next token. Therefore, the BlockDiagonalCausalWithOffsetPaddedKeysMask is used to generate the mask for the token generation. You can find the implementation of generating a new token in here which then uses the BlockDiagonalCausalWithOffsetPaddedKeysMask to generate the mask for the generation of the next token.\n\nHow multiple prompts are handled in Mistral?\nIn Mistral model, multiple prompts are packed into a single tensor during pre-filling phase at inference time and the corresponding mask is generated using the BlockDiagonalCausalMask , BlockDiagonalMask and BlockDiagonalCausalWithOffsetPaddedKeysMask. The packed tensor is then send to the model to pre-fill the cache and generate the mask for the token generation. You can find the implementation of packing the prompts into a single tensor and generating the mask in here."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#kv-cache",
    "href": "posts/Mistral/mistral_long.html#kv-cache",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "KV Cache",
    "text": "KV Cache\nKV cache, short for Key & Value cache, is a technique used to accelerate the inference process in Large Language Models (LLMs), particularly in autoregressive models i.e. the current token depends on the previous tokens in a sequence. In the KV cache, the output of the model from previous time step is appended to the cache of key and value matrices of the current time step but the query matrix is updated at each time step to generate the next token. This way of caching the previous keys and values ensures that the model does not repeat the computations at each time step. This significantly reduces the size of the matrices used in the computation which makes the inference process faster (matrix multiplication faster). I have explained KV cache in detail along with its implementation in my blog post on Understanding KV Cache.\n\nKV Cache in Pre-Filling and Chunking\nPre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the long prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences as explained above. Since, we divide the prompt sequence into smaller sequences or chunks to pre-fill the cache, the dimensions of the KV cache changes at each iteration of the pre-filling process. The complete operation of pre-filling is done by the following operations:\n\nLet’s say we have a prompt sequence “This is another great test” and after encoding we will have encoded prompt sequence of length 6 because we need to add the start token at index 0 during the inference.\nSuppose, we consider the chunk size of 2, then the prompt sequence is divided into 3 chunks: “[start token] This”, “is another”, “great test”.\nFor the first chunk, the KV cache sequence length is 0, and since the chunk size is 2, the KV cache dimension is [2, 8, 128] where 2 is sequence length, 8 is the number of KV heads, and 128 is the head dimension.\nFor the second chunk, the KV cache sequence length is 2, and since the chunk size is 2, the KV cache dimension now becomes [4, 8, 128] where 4 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.\nFor the third and last chunk, the KV cache sequence length is 4, and since the chunk size is 2, the KV cache dimension now becomes [6, 8, 128] where 6 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.\n\nSo, the KV cache is pre-filled with the prompt sequence and the dimensions of the KV cache changes at each iteration of the pre-filling process. You can check the dimensions of the KV cache at each iteration of the pre-filling process in the model.py."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#conclusion",
    "href": "posts/Mistral/mistral_long.html#conclusion",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I have explained the Sliding Window Attention mechanism, Rotating Buffer Cache, Pre-Filling and Chunking, BlockDiagonalCausalMask, BlockDiagonalMask, BlockDiagonalCausalWithOffsetPaddedKeysMask, and KV Cache in Mistral model. These techniques are used to accelerate the inference process as well as fine-tuning process in Large Language Models (LLMs). We prepare this blog post based on the source code of Mistral model and the paper of Mistral. You can learn more novel techniques used in Mistral model by reading my previous blog post on Model Sharding and Mixture of Experts."
  },
  {
    "objectID": "posts/Mistral/mistral_long.html#references",
    "href": "posts/Mistral/mistral_long.html#references",
    "title": "Comprehensive Understanding of Mistral Model",
    "section": "References",
    "text": "References\n\nMistral Source Code\nLlama2 Implementation\nAttention Is All You Need"
  },
  {
    "objectID": "posts/Llama1/llama1.html",
    "href": "posts/Llama1/llama1.html",
    "title": "LLaMA: Open and Efficient LLM Notes",
    "section": "",
    "text": "In this article, I will be sharing the notes and concepts which I have learned while reading the papers and while discussing with Umar. The ultimate goal that I have on my mind is to build Llama 2 from scratch.\n Figure 1: The Llama2 architecture prepared by Umar Jamil.\nThe Attention Is All You Need is the first paper that I read on this journey. This paper introduces the Transformer architecture, which is the architecture used in Llama.\n\nWhy was the Transformer architecture introduced?\nThe Transformer architecture was introduced to resolve the issues on Recurrent Neural Networks (RNNs). RNNs are sequential in nature, which means that the hidden state of a token depends on the hidden state of the previous token in the sequence. This makes RNNs not parallelizable. RNNs are not suitable for capturing long-range dependencies in a sequence. The Transformer architecture is parallelizable because it uses the attention mechanism to capture the relationships between tokens in a sequence. Since, we use dot products to calculate the attention, we can parallelize the calculations of the attention.\nI have an article that explains the fundamentals terms and concepts of the Transformer architecture. You can read it here.\n\n\nWhy do we need Positional Encodings? Which component of the Transformer architecture uses positional encodings?\nIn the transformer architecture, we convert the tokens in a sequence to embedding vectors. The embedding vectors (encoded with position inforamtion) are then fed to the encoder and decoder. The encoder block contains multi-head attention and feed-forward layers. The decoder block contains masked multi-head attention, multi-head attention and feed-forward layers. The embedding vectors does not contain any information about the position of the tokens in the sequence. The learned positional encodings are added to the embedding vectors to give information about the position of the tokens in the sequence. The positional encodings are added to the embedding vectors before they are fed to the encoder and decoder blocks. If we do not add positional encodings to the embedding vectors, each unique token will have the same embedding vector irrespective of its position in the sequence.\n Figure 2: Positional Encodings\nThe attention mechanism is used to capture the relationships between tokens in a sequence so that, the position encodings are used by the self-attention layer of the transformer architecture.\n\n\nWhy the multi-head attention in decoder block of the Transformer architecture is called cross-attention?\nThe multi-head attention in the decoder block of the Transformer architecture is called cross-attention because it uses the encoder output only as the key and value vectors. The query vectors are the output of the masked multi-head attention layer of the decoder block. The masked multi-head attention layer is used to prevent the decoder from attending to future tokens in the sequence. The cross-attention layer is used to capture the relationships between the tokens in the input sequence and the tokens in the output sequence.\nIn the Transformer architecture, the length of queries, keys, and values vectors are not always same but the embedding dimension should always same becuase we use dot product to calculate the attention. But the length of keys and values vectors are always same.\nThen, I read the Root Mean Square Layer Normalization because RMSNorm normalizing function was used in Llama architecture.\n\n\nWhat is the difference between LayerNorm and RMSNorm?\nLayerNorm is the normalization technique that is used in the Transformer architecture which stabilizes the training of any neural networks by regularizing neuron dynamics via mean and variance statistics. The vanilla neural networks might suffer from internal covariate shift problem, where a layer’s input distribution changes as the previous layers are updated. This could negatively affect the stability of parameters gradient calculation delaying model convergence. To reduce this shift, LayerNorm normalizes the summed inputs so as to fix their mean and variance to 0 and 1 respectively.\n Figure 3: Layer Normalization & Root Mean Square Normalization\nLayerNorm has re-centering and re-scaling invariance properties where the former property enables the model to be insensitive to shift noises on both inputs and weights, and the latter property keeps the output representations intact when both the inputs and weights are randomly scaled. RMSNorm only focuses on re-scaling invariance and regularizes the summed inputs simply by Root Mean Square (RMS) statistics.\n\nSince, RMSNorm doesn’t require the calculation of mean static, it is computationally efficient and accelerates the running speed.\nAfter that, I read the GLU Variants Improve Transformer because the Llama architecture replace the ReLU non-linearity from FeedForward block with SwiGLU activation function to improve the performance.\nIn the Transformer architecture, there are two layers: multi-head attention and position-wise feedforward networks (FFN). The FFN takes a vector x which is the hidden representation at a particular position in the sequence and passes it through two learned linear transformmations with a non-linear activation function in between.\nThe Llama architecture uses SwiGLU activation function instead of ReLU activation function in the FFN block. SwiGLU is a variant of GLU activation function that uses 3 matrices and has no computational overhead.\n Figure 4: SwiGLU activation function\nAfter that, I read the RoFormer: Enhanced Transformer with Rotary Position Embedding because the Llama architecture uses Rotary Position Embedding (RoPE) to encode the positional information to the attention mechanism instead of using the learned absolute positional encodings as done in the original Transformer architecture.\n\n\nRotary Position Embedding (RoPE)\nRotary Position Embedding encodes the absolute position with the rotation matrix and naturally incorporates the explicit relative position dependency into the self-attention mechanism. After we multiply the input embedding with parameterized matrices, the two embedding vectors which correspond to query and key is converted into a complex number representation. Then, the complex number is represented as a multiplication matrix which will contain the rotation matrix and weight matrices. The rotation matrix is used to encode the absolute position information because the degree of rotation is proportional to the position of the token in the sequence. Then we calculate the inner product of the rotated query and key vectors to calculate the attention scores which incorporates the relative position information and the inner product of query and key embedding vectors only depends on the distance between them.\n Figure 5: Multiplication Matrix of RoPE\nRotary Position Embedding uses a rotation matrix to encode positional information to the attention rather than adding absolute position encoding vectors to the queries and keys as done in relative positional embedding which resolves the essence of extra computation as well. RoPE uses the concept of complex numbers and Euler’s formula to encode positional information to the attention mechanism.\n Figure 6: Rotary Position Embedding\nAbsolute Positional Encodings Vs Relative Positional Encodings - Absolute positional encodings are fixed vectors that are added to the embedding vectors of a token to represent its absolute position in the sentence. So, it deals with one token at a time. You can think of it as the pair (lantitue, longitude) on a map: each point on earth has a unique pair of coordinates. - Relative positional encodings, on the other hand, deals with two tokens at a time and it is involved when we calculate the attention: since the attention mechanism captures the “intensity” of how two words are related to each other, relative postional encodings tell the attention mechanism the distance between two words involved in it. So, give two tokens, we create a vector that represents the distance between them. Relative positional encodings were introduced in the paper “Self-Attention with Relative Position Representations” by Shaw et al. (2018)“."
  },
  {
    "objectID": "posts/GQA/gqa.html",
    "href": "posts/GQA/gqa.html",
    "title": "Grouped Query Attention (GQA)",
    "section": "",
    "text": "In this article, we will discuss the Grouped Query Attention. We will start with the introduction of the Grouped Query Attention, then we will discuss the limitations of Multi-Head Attention (MHA) and Multi-Query Attention (MQA), and why we need a Grouped Query Attention. We will also discuss the limitations of the Grouped Query Attention and how to implement GQA in the PyTorch library.\n\nIntroduction\nGrouped Query Attention (GQA) is the attention mechanism introduced in the GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints paper which is a generalization of the Multi-Head Attention (MHA) and Multi-Query Attention (MQA) which uses an intermediate (more than one, less than number of query heads or attention heads) number of key-value heads and achieves quality close to multi-head attention with comparable speed to multi-query attention. GQA is not applied to the encoder self-attention layers because the encoder representations are computed in parallel and the memory bandwith overhead is not a bottleneck.\n\n\n\nFigure 1 taken from GQA paper.\n\n\nAs shown in the figure, the MHA has H query, key and value heads. MQA shares single key and value across all query heads and GQA shares single key and value heads for each group of query heads. So, GQA is a generalization of MHA and MQA.\n\n\nWhat are the limitations of MHA and MQA?\nIn the Transformer model with KV cache, the autoregressive decoder inference is the bottleneck because of memory bandwith overhead for loading all the attention keys and values at every decoding step. The matrix multiplication for the computation of attention is fast but the data transfer between the memory and the processor is slow. So, the multi-head attention (MHA) with KV cache is not efficient for autoregressive decoding since the memory access of all the keys and values is required at every decoding step which is slow.\nOn the other hand, the multi-query attention (MQA) uses a single key and value heads for all the query heads which sharply reduces the H dimension from key and value heads to 1. This reduces the memory bandwith overhead for loading the attention keys and values at every decoding step but it leads to quality degradation and training instability.\nLarge Language Models (LLMs) generally scale the number of heads, such that multi-query attention (MQA) represents a more aggressive cut in both memory bandwith and quality. So, there is a need for Grouped Query Attention that can achieve quality close to multi-head attention with comparable speed to multi-query attention.\n\n\nWhy do we need a Grouped Query Attention?\nGrouped Query Attention (GQA) divides the query heads into groups and shares the key and value heads for each group in such a way to generate the interpolated model of multi-head and multi-query attention that achieves quality close to multi-head attention with comparable speed to multi-query attention. So, we need a Grouped Query Attention.\n\n\nHow to implement a Grouped Query Attention?\nWhile working on the implementation of the Llama 2: Open Foundation and Fine-Tuned Chat Models, I implemented the Grouped Query Attention.\n\n\n\nFigure 2 presents the constructor of GQA.\n\n\nThe constructor of GQA is shown in the figure 2. The constructor of GQA initializes the number of query heads, key and value heads, and the 3 parameterized matrices which are trainable.\n\n\n\nFigure 2 presents the first part of GQA.\n\n\nIn the forward method of GQA, the query, key and value are multiplied with the parameterized matrices and then we are applying the Rotary Positional Embedding to the query and key. You can learn more about the Rotary Positional Embedding from my article LLAMA: OPEN AND EFFICIENT LLM NOTES. Basically, the Rotary Positional Embedding uses a rotation matrix to encode the positional encoding to the attention rather than adding the positional encoding vectors to the query and key as done in Relative Positional Embedding which resolves the essence of extra computation.\n\n\n\nFigure 3 presents the last part of GQA.\n\n\nIn the last part of the forward method of GQA as shown in the figure 3, the key and value are repeated for the number of query heads and then the query, key and value are used to compute the attention. I have removed the KV cache from the above code for simplicity but you can see the detailed implementation of enabling and disabling the KV cache in my Llama 2 implementation here.\n\n\nConclusion\nSo, Grouped Query Attention is an interpolation of multi-head attention and multi-query attention that achieves quality close to multi-head attention with comparable speed to multi-query attention. In this article, we discussed the Grouped Query Attention. We started with the introduction of the Grouped Query Attention, then we discussed the limitations of Multi-Head Attention and Multi-Query Attention, and why we need a Grouped Query Attention. We also see the implementation of GQA in the PyTorch library. You can also check my implementation of Llama 2 here.\n\n\nReferences\n\nUmar’s video on GQA\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nLLAMA: OPEN AND EFFICIENT LLM NOTES\nLlama2 Implementation\nLlama 2: Open Foundation and Fine-Tuned Chat Models\nAttention Is All You Need"
  },
  {
    "objectID": "posts/EST/scaling.html",
    "href": "posts/EST/scaling.html",
    "title": "Scaling Transformer Models",
    "section": "",
    "text": "Language models are the probabilistic models that assigns probability to the sequence of words. In other words, language models assigns the probability of generating a next token given the sequence of previous tokens. These models generate the complete answer for the given prompt using the iterative approach of predicting the next token and appending the generated token to the input prompt to generate the next token until the sequence of desired length is generated. We need to understand the engineering tradeoffs for inference of these large language models (LLMs) because use cases of these models are growing rapidly throughout application areas."
  },
  {
    "objectID": "posts/EST/scaling.html#introduction-to-llms",
    "href": "posts/EST/scaling.html#introduction-to-llms",
    "title": "Scaling Transformer Models",
    "section": "",
    "text": "Language models are the probabilistic models that assigns probability to the sequence of words. In other words, language models assigns the probability of generating a next token given the sequence of previous tokens. These models generate the complete answer for the given prompt using the iterative approach of predicting the next token and appending the generated token to the input prompt to generate the next token until the sequence of desired length is generated. We need to understand the engineering tradeoffs for inference of these large language models (LLMs) because use cases of these models are growing rapidly throughout application areas."
  },
  {
    "objectID": "posts/EST/scaling.html#parallel-training-of-llms",
    "href": "posts/EST/scaling.html#parallel-training-of-llms",
    "title": "Scaling Transformer Models",
    "section": "Parallel Training of LLMs",
    "text": "Parallel Training of LLMs\nLet’s consider that we want to train a transformer model for the language translation task where we want to convert the english sentence to nepali sentence. Our english text is “I love you so much” and the corresponding nepali text is “म तपाईंलाई एकदम धेरै माया गर्छु”.\nThe training process can be broken down into the following steps: 1. First, we need to append the start token &lt;SOS&gt; and end token &lt;EOS&gt; to the source sentence which becomes &lt;SOS&gt; I love you so much &lt;EOS&gt;. Then we will prepare the encoder input by tokenizing, embedding, and adding positional encoding to the source sentence. The encoder input is then passed through the encoder which gives the encoder output which is used as the key and value for the decoder. 2. The decoder input is prepared by appending only the start token &lt;SOS&gt; to the target sentence which becomes &lt;SOS&gt; म तपाईंलाई एकदम धेरै माया गर्छु and undergoes the same process of tokenizing, embedding, and adding positional encoding. The decoder input is then passed through the decoder along with the encoder output which gives the decoder output. 3. The decoder output is then passed through the projection layer and softmax layer to get the probability distribution of the next token. We then calculate the cross-entropy loss between the predicted tokens and the actual target tokens. At each timestep, the model obtains the complete sequence of tokens in parallel during the training process.\n\n\n\nFig. Training process at timestep 1.\n\n\nAs shown in the figure above, language models are trained using the sequence parallelism approach where the model generates the desired sequence of tokens in parallel at each timestep."
  },
  {
    "objectID": "posts/EST/scaling.html#scaling-llms",
    "href": "posts/EST/scaling.html#scaling-llms",
    "title": "Scaling Transformer Models",
    "section": "Scaling LLMs",
    "text": "Scaling LLMs\nThe efficient scaling of large language models can unlock the new capabilities & applications but has fundamental tradeoffs in terms of inference cost, memory footprint, and parallelizability. The inference cost of LLMs can be measured in terms of the following metrics:\n\nLatency: The latency is the total time taken for inference of the model which can be broken down into the time to process the input tokens present at the start of the inference i.e. prefill and the time to autoregressively generate the output tokens i.e. decode. The decode latency can also be measured per step i.e. divided by the number of tokens in each sequence.\nThroughput: The throughput of prefill or decode is the number of tokens processed or generated per second.\nModel FLOPS utilization (MFU): The MFU is the ratio of the observed throughput to the theoretical maximum throughput of the model. This metric should be measured with no memory or communication overhead."
  },
  {
    "objectID": "posts/EST/scaling.html#challenges-at-inference",
    "href": "posts/EST/scaling.html#challenges-at-inference",
    "title": "Scaling Transformer Models",
    "section": "Challenges at Inference",
    "text": "Challenges at Inference\nThe generative inference of LLMs is challenging due to the following reasons:\n\nLarge memory footprint: Large language models have large memory footprint both due to the trained model parameters as well as the transient memory state needed for decoding. The KV cache 4 for each layers must be stored in memory for the entire duration of the decoding process. Thus, the model parameters usually do not fit in a single GPU memory, which requires the model to be partitioned across multiple GPUs 2.\nLow parallelizability: Generative inference of LLMs is inherently sequential due to the autoregressive nature and the computation for each token is dependent on the previous tokens. This makes it difficult to parallelize the inference process.\n\nTherefore, the inference cost of LLMs increases quadratically with the sequence length from the attention mechanism. The attention mechanism computes the attention scores between each tokens in the sequence 3.\nLet’s consider the task mentioned above where we want to convert the english sentence to nepali sentence. The input english text is “I love you so much” and the corresponding nepali text is “म तपाईंलाई एकदम धेरै माया गर्छु”.\nThe inference process of generative LLMs can be broken down into the following steps: 1. So, we need to append the start token &lt;SOS&gt; and end token &lt;EOS&gt; to the source sentence which becomes &lt;SOS&gt; I love you so much &lt;EOS&gt;. Then we will prepare the encoder input by tokenizing, embedding, and adding positional encoding to the source sentence. The encoder input is then passed through the encoder which gives the encoder output which is used as the key and value for the decoder. Please note that the encoder output remains the same for all the timesteps at inference. 2. The first decoder input is just the start token &lt;SOS&gt; which undergoes the same process of tokenizing, embedding, and adding positional encoding. The decoder input is then passed through the decoder along with the encoder output which gives the decoder output. The decoder output is then passed through the projection layer and softmax layer to get the probability distribution of the next token which is likely to be “म” in this case. 3. The generated token “म” is then appended to the decoder input which becomes “&lt;SOS&gt; म” and undergoes the same process of tokenizing, embedding, and adding positional encoding. The decoder input is then passed through the decoder along with the encoder output which gives the decoder output. The same process is repeated until the end token &lt;EOS&gt; is generated in the sequence and the encoder output will be the same for all the timesteps until the end token is generated.\n\n\n\nFig. Inference process at timestep 1.\n\n\nAs shown in the figure above, the inference process at timestep 1 will generate the first token of the nepali sentence “म” using the input english text “I love you so much”.\n\n\n\nFig. Inference process at timestep 2.\n\n\nSimilarly, the inference process at timestep 2 will generate the second token of the nepali sentence “तपाईंलाई” which is appended to the decoder input and passed through the decoder to generate the next token.\n\n\n\nFig. Inference process at timestep N.\n\n\nFinally, the inference process at timestep N will generate the &lt;EOS&gt; token which will be the end of the inference process and the complete nepali sentence will be generated. In this way, the inference process of generative LLMs is autoregressive and sequential in nature.\n\nImplementation of Inference\nThe inference process of generative LLMs can be implemented using the following code snippet. The code snippet follows the same process as mentioned above where we generate the next token at each timestep and append it to the decoder input until the end token is generated. You can find the complete code implementation here 5.\n# translate the sentence.\nmodel.eval()\nwith torch.no_grad():\n    # precompute the encoder output and reuse it for every generation step.\n    source = tokenizer_src.encode(sentence)\n    source = torch.cat(\n        [\n            torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64),\n            torch.tensor(source.ids, dtype=torch.int64),\n            torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64),\n            torch.tensor(\n                [tokenizer_src.token_to_id(\"[PAD]\")]\n                * (seq_len - len(source.ids) - 2),\n                dtype=torch.int64,\n            ),\n        ],\n        dim=0,\n    ).to(device)\n    source_mask = (\n        (source != tokenizer_src.token_to_id(\"[PAD]\"))\n        .unsqueeze(0)\n        .unsqueeze(0)\n        .int()\n        .to(device)\n    )\n    encoder_output = model.encode(source, source_mask)\n\n    # initialize the decoder input with the sos token.\n    decoder_input = (\n        torch.empty(1, 1)\n        .fill_(tokenizer_tgt.token_to_id(\"[SOS]\"))\n        .type_as(source)\n        .to(device)\n    )\n\n    # generate the translation word by word.\n    while decoder_input.size(1) &lt; seq_len:\n        # build mask for target and calculate output.\n        decoder_mask = (\n            torch.triu(\n                torch.ones((1, decoder_input.size(1), decoder_input.size(1))),\n                diagonal=1,\n            )\n            .type(torch.int)\n            .type_as(source_mask)\n            .to(device)\n        )\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # project next token.\n        prob = model.project(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat(\n            [\n                decoder_input,\n                torch.empty(1, 1)\n                .type_as(source)\n                .fill_(next_word.item())\n                .to(device),\n            ],\n            dim=1,\n        )\n\n        # break if we predict the end of sentence token.\n        if next_word == tokenizer_tgt.token_to_id(\"[EOS]\"):\n            break"
  },
  {
    "objectID": "posts/EST/scaling.html#parallel-transformer-blocks",
    "href": "posts/EST/scaling.html#parallel-transformer-blocks",
    "title": "Scaling Transformer Models",
    "section": "Parallel Transformer Blocks",
    "text": "Parallel Transformer Blocks\nThe parallel formulation of transformer blocks was introduced by Wang & Komatsuzaki 7. They mentioned that the parallel formulation results in roughly 15% faster training speed at large scales, since the FFN and Attention input matrix multiplication can be fused.\nSpecifically, the standard “serialized” formulation can be written as:\n\\(y = x + FFN(LayerNorm(x + Attention(LayerNorm(x))))\\)\nWhereas, the parallel formulation can be written as:\n\\(y = x + FFN(LayerNorm(x)) + Attention(LayerNorm(x))\\)\nThere are inference latency gains from the parallel formulation of each transformer block instead of the serialized formulation, where the feedforward layer and attention layer can be computed in parallel from the layer normalization output and summed together to get the final output.\nThe major benefits of the parallel formulation of transformer blocks are: 1. There is only one layer normalization operation per layer instead of two which reduces latency at small batch sizes. 2. The input matrices of the feedforward layer can be fused with the query projection matrix \\(W_q\\) of the attention layer, the key and value projection matrices \\(W_k\\) and \\(W_v\\) can be fused in the attention layer, and the output of the feedforward layer can be fused with the output projection matrix \\(W_o\\) of the attention layer. This fusion operation results in higher FLOPS utilization because the larger matrix multiplications run more efficiently.\n\nImplementation of Parallel Transformer Blocks\nThe parallel transformer blocks can be implemented using the following code snippet. The code snippet follows the same process as mentioned above where the feedforward layer and attention layer can be computed in parallel from the layer normalization output and summed together to get the final output. You can find the complete code implementation here 8.\nclass ParallelTransformerBlock(nn.Module):\n    \"\"\"\n    Mentioned in the paper: https://arxiv.org/pdf/2204.02311 at model architecture section.\n    \"\"\"\n\n    def __init__(\n        self,\n        args: ModelArgs,\n    ):\n        super().__init__()\n        self.attention = GroupedQueryAttentionLLAMA(args)\n        self.feed_forward = FeedForwardBlock(args)\n\n        # normalization before attention block and feed forward block\n        self.rms_norm = RMSNorm(args)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        start_pos: int,\n        freqs_complex: torch.Tensor,\n        mask: Optional[torch.Tensor],\n    ):\n        norm_x = self.rms_norm(x)\n\n        # (batch_size, seq_len, d_model) --&gt; (batch_size, seq_len, d_model)\n        h = x + self.attention.forward(\n            norm_x,\n            freqs_complex=freqs_complex,\n            start_pos=start_pos,\n            mask=mask,\n        )\n\n        out = h + self.feed_forward.forward(norm_x)\n        return out"
  },
  {
    "objectID": "posts/EST/scaling.html#conclusions",
    "href": "posts/EST/scaling.html#conclusions",
    "title": "Scaling Transformer Models",
    "section": "Conclusions",
    "text": "Conclusions\nTherefore, this blog post provides an overview of the inference process of generative LLMs and the challenges faced during the inference process. The inference process of generative LLMs is autoregressive and sequential in nature which makes it difficult to parallelize the inference process. The parallel transformer blocks can be used to reduce the inference latency by computing the feedforward layer and attention layer in parallel from the layer normalization output and summed together to get the final output. This blog also explains the parallel training of LLMs and the engineering tradeoffs for inference of these large language models."
  },
  {
    "objectID": "posts/EST/scaling.html#references",
    "href": "posts/EST/scaling.html#references",
    "title": "Scaling Transformer Models",
    "section": "References",
    "text": "References\n\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., & Dean, J. (2022). Efficiently Scaling Transformer Inference. ArXiv. /abs/2211.05102\nModel Sharding by Thinam Tamang.\nGrouped Query Attention by Thinam Tamang.\nUnderstanding KV Cache by Thinam Tamang.\nTransformer Implementation by Thinam Tamang.\nUmar’s YouTube Channel.\nGPT-J-6B: A 6 Billion Parameter Autoregressive Language Model\nLLama Implementation by Thinam Tamang."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-sources",
    "href": "posts/DataEng/DataEngineering.html#data-sources",
    "title": "Data Engineering Fundamentals",
    "section": "Data Sources",
    "text": "Data Sources\nUser input data can be text, images, videos, uploaded files, etc. It requires more heavy-duty checking and processing. User input data tends to require fast processing as well.\nSystem-generated data is the data generated by different components of your systems which includes various types of logs and system output such as model predictions. The logs of data provide visibility into system performance which can be used for debugging and potentially improving the application."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-formats",
    "href": "posts/DataEng/DataEngineering.html#data-formats",
    "title": "Data Engineering Fundamentals",
    "section": "Data Formats",
    "text": "Data Formats\nThe process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization.\nJSON\nJSON, JavaScript Object Notation was derived from JavaScript but it’s language-independent. It’s key-value pair paradigm is simple but powerful, capable of handling data of different levels of structuredness.\nRow-Major Versus Column-Major Format\nCSV (Comma-separated values) is row-major, which means consecutive elements in a row are stored next to each other in memory. Parquet is column-major, which means consecutive elements in a column are sotred next to each other. Row-major formats are better when you have to do a lot of writes, whereas column-major ones are better when you have to do a lot of column-based reads.\nText Versus Binary Format\nCSV and JSON are text files, whereas Parquet files are binary files. Text files are files that are in plain text, which means they are human-readable. Binary files are nontext files and contains onlys 0s and 1s. Binary Files are more compact.\nAWS recommends using the Parquet format because the Parquet format is up to 2x faster to unload and consumes up to 6x less storage in Amazon s3, compared to text formats."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-models",
    "href": "posts/DataEng/DataEngineering.html#data-models",
    "title": "Data Engineering Fundamentals",
    "section": "Data Models",
    "text": "Data Models\nData models describe how data is represented. The attributes of the entities present in the model make up a data model.\nRelational Model\nRelational model was invented by Edgar F. Codd in 1970. In this model, data is organized into relations; each relation is a set of tuples. A table is an accepted visual representation of the relation, and each row of a table makes up a tuple.\nDocument Model\nThe document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare. The document model is built from around the concepts of “document” which is often a single continuous string encoded as JSON, XML, or a binary format like BSON (Binary JSON).\nGraph Model\nThe graph model goes in the opposite direction of targeting use cases where relationships between data items are common and important. The graph model is built around the concepts of “graph” which consists of nodes and edges, where the edges represent the relationships between the nodes. A database that uses graph structures to store its data is caled a graph database.\nStructured Versus Unstructured Data\nStructured data follows a predefined data model, also known as a data schema which makes your data easier to analyze. Unstructured data doesn’t adhere to a predefined data schema. Even though unstructured data doesn’t adhere to a schema, it might still contain intrinsic patterns that help you extract structures.\nA repository for storing structured data is called a data warehouse. A repository for storing unstructured data is called a data lake. Data lakes are usually used to store raw data before processing. Data warehouses are used to store data that has been processed into formats ready to be used."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#etl-extract-transform-and-load",
    "href": "posts/DataEng/DataEngineering.html#etl-extract-transform-and-load",
    "title": "Data Engineering Fundamentals",
    "section": "ETL: Extract, Transform, and Load",
    "text": "ETL: Extract, Transform, and Load\nWhen data is extracted from different sources, it’s first transformed into the desired format before being loaded into the target destination such as a database or a data warehouse. This process is called ETL, which stands for Extract, Transform, and Load.\nExtract is extracting the data from all data sources. In extraction phase, we need to validate the data and reject the data that doesn’t meet our requirements. Transform is the meaty part of the process, where most of the data processing is done. We can apply operations such as transposing, deduplicating, sorting, aggregating, deriving new features, and more data validation. Load is deciding how and how often to load the transformed data into the target destination, which can be a file, a database, or a data warehouse."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#batch-processing-versus-stream-processing",
    "href": "posts/DataEng/DataEngineering.html#batch-processing-versus-stream-processing",
    "title": "Data Engineering Fundamentals",
    "section": "Batch Processing Versus Stream Processing",
    "text": "Batch Processing Versus Stream Processing\nWhen data is processed in batch jobs, we refer to it as batch processing. When we have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that we have streaming data and stream processing refers to doing computation on streaming data.\nStream processing, when done right, can give low latency because we can process data as soon as data is generated, without having to first write it into databases. Stream processing is used to compute features that change quickly and it is more difficult because the data amount is unbounded and the data comes in at variable rates and speeds."
  },
  {
    "objectID": "posts/ClassifyImage/Fundamentals.html",
    "href": "posts/ClassifyImage/Fundamentals.html",
    "title": "Fundamentals of Image Classification",
    "section": "",
    "text": "Image Classification\n1. Image Classification is the task of using computer vision and machine learning algorithms to extract meaning from an image. It is the task of assigning a label to an image from a predefined set of categories.\n2. Semantic Gap is the difference between how a human perceives the contents of an image versus how an image can be represented in a way a computer can understand the process.\n3. Feature Extraction is the process of taking an input image, applying an algorithm, and obtaining a feature vector that quantifies the image.\n4. The common supervised learning algorithms include Logistic Regression, Support Vector Machines, Random Forests, and Artificial Neural Networks.\n5. Unsupervised Learning which is also called self-taught learning has no labels associated with the input data and thus we cannot correct our model if it makes an incorrect prediction.\n6. K-Nearest Neighbor classifier doesn’t actually learn anything, but it directly relies on the distance between feature vectors.\n7. A learning model that summarizes data with a set of parameters of fixed size which is independent of the number of training examples is called a Parametric Model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs. – Russell and Norvig.\n8. Parameterization is the process of defining the necessary parameters of a given model. In machine learning, parameterization involves in four key components: data, a scoring function, a loss function, and weights and biases.\n9. The scoring function accepts the data as an input and maps the data to class labels. A loss function quantifies how well our predicted class labels agree with our ground-truth labels.\n10. Softmax Classifiers give probabilities for each class label while hinge loss gives the margin scores.\n11. The gradient descent method is an iterative optimization algorithm that operates over a loss landscape also called and optimization surface. Also, gradient descent refers to the process of attempting to optimize the parameters for low loss and high classification accuracy via an iterative process of taking a step in the direction that minimize loss.\n\nFigure: The naive loss visualized as a 2D plot.\nAs shown in figure, the loss landscape has many peaks and valleys. Each peak is a local maximum that represents very high regions of loss. The local maximum with largest loss across the entire loss landscape is the global maximum. Similarly, the local minimum represents many small regions of loss. The local minimum with the smallest loss across the loss landscape is the global minimum.\n12. An optimization algorithm may not be guaranteed to arrive at even a local minimum in a reasonable amount of time, but it often finds a very low value of the loss function quickly enough to be useful. – Goodfellow.\n13. Stochastic Gradient Descent (SGD) is a simple modification to the standard gradient descent algorithm that computes the gradient and updates the weight matrix on small batches of training data, rather than the entire training set.\n14. Momentum is a method used to accelerate Stochastic Gradient Descent (SGD), enabling it to learn faster by focusing on dimensions whose gradient point in the same direction. Nesterov’s Acceleration can be conceptualized as a corrective update to the momentum which lets us obtain an approximate idea of where our parameters will be after the update.\n13. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are collectively known as Regularization. – Goodfellow.\n\nFigure: An example of model fitting.\nThe figure presents an example of underfitting (orange line), overfitting (blue line), and generalizing (green line). The goal of deep learning classifiers is to obtain these types of “green functions” that fit the training data nicely, but avoid overfitting. Regularization helps to obtain the desired fit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "Scaling Transformer Models\n\n\n\n\n\n\nTransformer\n\n\nOptimization\n\n\nLLMs\n\n\nInference\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nOdds Ratio Preference Optimization (ORPO)\n\n\n\n\n\n\nReinforcement Learning\n\n\nORPO\n\n\nRLHF\n\n\nLLMs\n\n\nPPO\n\n\n\n\n\n\n\n\n\nMay 3, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nProximal Policy Optimization (PPO)\n\n\n\n\n\n\nReinforcement Learning\n\n\nPGO\n\n\nRLHF\n\n\nLLMs\n\n\nPPO\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Gradient Optimization\n\n\n\n\n\n\nReinforcement Learning\n\n\nPGO\n\n\nRLHF\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nApr 14, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nDeep Learning\n\n\nRLHF\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Understanding of Mistral Model\n\n\n\n\n\n\nMixture of Experts\n\n\nMistral\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 9, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nMixture of Experts in Mistral\n\n\n\n\n\n\nMixture of Experts\n\n\nMistral\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nModel Sharding\n\n\n\n\n\n\nModel Sharding\n\n\nMistral\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding KV Cache\n\n\n\n\n\n\nllama\n\n\nKV cache\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nGrouped Query Attention (GQA)\n\n\n\n\n\n\nllama\n\n\ngrouped query attention\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nLLaMA: Open and Efficient LLM Notes\n\n\n\n\n\n\nllama\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nGit & GitHub\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\nversion control\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Attention & Transformer\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nWord Vectors\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nData Engineering Fundamentals\n\n\n\n\n\n\nmachine learning\n\n\ndata engineering\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nData Fundamentals\n\n\n\n\n\n\nmachine learning\n\n\ndata preparation\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nMar 27, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nPattern Recognition & ML\n\n\n\n\n\n\nmachine learning\n\n\npattern recognition\n\n\n\n\n\n\n\n\n\nFeb 10, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks Architectures\n\n\n\n\n\n\nconvolutional neural networks\n\n\n\n\n\n\n\n\n\nJan 22, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of CNNs\n\n\n\n\n\n\nmachine learning\n\n\nconvolutional neural network\n\n\n\n\n\n\n\n\n\nDec 31, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Image Classification\n\n\n\n\n\n\ncomputer vision\n\n\nimage classification\n\n\n\n\n\n\n\n\n\nDec 6, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\n\n\n\n\n\n\nJourney of 66DaysOfData in Natural Language Processing\n\n\n\n\n\n\nnatural language processing\n\n\n\n\n\n\n\n\n\nOct 15, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Thinam Tamang",
    "section": "",
    "text": "Experienced Machine Learning Scientist with 4+ years of proven expertise in natural language processing, machine learning, deep learning, and data science. Passionate about leveraging artificial intelligence and machine learning to drive innovation, solve complex problems, and gain valuable insights.\nSolving problems and gaining insights with the help of machine learning algorithms have always felt like superpowers to me. I am here to apply my skills and knowledge to tackle challenges head-on, continuously learning and growing in the process.\n\nPublication:\n\nClassification of White Blood Cells: A Comprehensive Study Using Transfer Learning Based on Convolutional Neural Networks\nToward insights on antimicrobial selectivity of host defense peptides via machine learning model interpretation\n\n\n\nExperience:\n\nSpeechify | Machine Learning Scientist | Dec 2022 - Present\nRippeyAI | NLP Engineer | May 2022 - Dec 2022\nGenese Solution | Data Scientist | Feb 2022 - May 2022\nCenter of Data Mining and Biomedical Informatics | Research Assistant | Nov 2020 - Feb 2022\nKharpann Enterprises | NLP Engineer | Apr 2021 - Aug 2021"
  },
  {
    "objectID": "posts/Attention/Transformer.html#recurrent-models",
    "href": "posts/Attention/Transformer.html#recurrent-models",
    "title": "Self-Attention & Transformer",
    "section": "Recurrent Models",
    "text": "Recurrent Models\n\nThe de facto strategy in Natural Language Processing (NLP) is to encode sentences with a bidirectional LSTM models. For example: Source sentence in a translation.\nWe should define our output (parse, sentence, summary) as a sequence, and use an LSTm model to generate it.\nUse of attention mechanism allows flexible access to the memory. We use attention mechanism to take a representation from our decoder and look back to treat the encoded representation as a memory, that we can reference and pick out what’s important to any given time.\n\n\nIssues with Recurrent Models\n\nLinear interaction distance.\n\nRecurrent Neural Networks (RNN) are unrolled left-to-right.\nRNNs encode linear locality which means the nearby words often affect each other’s meaning in the sentence.\nRNNs take O(sequence length) steps for distant words pairs to interact which means that it is hard to learn long-distance dependencies because of the gradient problems.\nLinear order of words is sort of baked into the model because we have to unroll the RNN throughout the sequence and linear order isn’t the right way to think about sentences.\n\nLack of parallelizability.\n\nForward and backward passes have O(sequence length) unparallelizable operations.\nThough GPUs can perform a bunch of independent computations at once, a future RNN hidden states can’t be computed in full before past RNN hidden states have been computed.\n\nSo, RNNs inhibits training on very large dataset."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-windows-model",
    "href": "posts/Attention/Transformer.html#word-windows-model",
    "title": "Self-Attention & Transformer",
    "section": "Word Windows Model",
    "text": "Word Windows Model\n\nWord window models aggregate local context. Number of unparallelizable operations doesn’t increase sequence length.\nStacking word window layers allows interaction between farther words.\nMaximum interaction distance = sequence length / window size. But if the sentences are too long, we will just ignore the long-distance context."
  },
  {
    "objectID": "posts/Attention/Transformer.html#attention-model",
    "href": "posts/Attention/Transformer.html#attention-model",
    "title": "Self-Attention & Transformer",
    "section": "Attention Model",
    "text": "Attention Model\n\nAttention model treats each word’s representation as a query to access and incorporate information for a set of values. Example: In a machine translation system, the set of values were all of the encoder states for the source sentences.\nNumber of unparallelizable operations doesn’t increase sequence length.\nMaximum interaction distance is O(1) since all the wrods interact at every layer.\n\n\nSelf-Attention Model\n\nAttention model operates on queries, keys, and values.\nIn self-attention models, the queries, keys, and values are drawn from the same source sentences.\nSince, self-attention mechanism doesn’t build in order information, we need to encode the order of the sentences in our keys, queries, and values. We will consider representing each sequence index as a vector and add it to our inputs in *self-attention** block.\nThe position representation vectors are represented through sinusoids. Sinusoidal position representations concatenate functions of varying periods. Learned absolute position representations are flexible to be learned to fit the data on each position.\n\n\n\nBarriers & Solutions for Self-Attention Model\n\n\n\n\n\n\n\nBarriers\nSolution\n\n\n\n\n1. Doesn’t have an inherent notion of order.\n1. Add position representations to the inputs.\n\n\n2. No nolinearities to produce the deep learning magic. But it’s all just the weighted averages.\n2. Apply the same feedforward networks to each self-attention output.\n\n\n3. Need to ensure that we don’t look at the future outputs when predicting a sequence. Like in machine translation or language modeling.\n3. Mask out the future by artificially setting the attention weights to 0.\n\n\n\nThe necessities for a self-attention model are as follows:\n\nSelf-attention:\n\nThe basis of the method or implementation process.\n\nPosition representations:\n\nSpecify the sequence order, since self-attention is an unordered function of its inputs.\n\nNonlinearities:\n\nAt the output of the self-attention block.\nFrequently implemented as a simple feedforward network.\n\nMasking:\n\nIn order to parallelize operations while not looking at the future.\nKeeps information about the future from leaking to the past."
  },
  {
    "objectID": "posts/Attention/Transformer.html#the-transformer",
    "href": "posts/Attention/Transformer.html#the-transformer",
    "title": "Self-Attention & Transformer",
    "section": "The Transformer",
    "text": "The Transformer\n\nWe take the dot product of the query-key in one matrix multiplication.\nThen we apply softmax and compute the weighted average with another matrix multiplication.\nWe define multiple attention heads through multiple query, key, and value matrices.\nResidual connections are thought to make the loss landscape considerably smoother and thus enhances easier training.\nLayer normalization is a trick to help models train faster. It cuts down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer.\nScaled Dot Product attention is necessary when dimensionality d becomes large, dot products between vectors tend to become large and because of this, inputs to the softmax function can be large, making the gradients small.\nTransformers parallelizability allows for efficient pretraining, and have made them the de facto standard."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "href": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "title": "Self-Attention & Transformer",
    "section": "Word Structure and Subword Models",
    "text": "Word Structure and Subword Models\n\nIn a language’s vocabulary, we assume that a fixed vocabulary of tens of thousands of words are built from the training dataset. All other novel words which are seen only at test time are mapped to a single unknown token UNK.\nFinite vocabulary assumptions in not an ideal solution in many languages. Many languages exhibit complex morphology or word structure which is more word types, each occurring in fewer times."
  },
  {
    "objectID": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "href": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "title": "Self-Attention & Transformer",
    "section": "Byte-Pair Encoding Algorithm",
    "text": "Byte-Pair Encoding Algorithm\nSubword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level. The dominant modern paradigm is to learn a vocabulary of parts of words also known as subword tokens. At the training and testing time, each word is split into a sequence of known subwords.\nByte-pair encoding is a simple, and effective startegy for defining a subword vocabulary: - Start with a vocabulary containing only characters and an end-of-word symbol. - Using a corpus of text, find the most common adjacent characters known as subwords. - Replace instances of the character pair with the new subword and iterate until the desired vocab size is met.\nThis technique was originally used in NLP for machine translation, and now a similar method WordPiece is used in pretrained models.\n\nPretrained Word Embeddings & Models\n\nAlmost all parameters in NLP networks are initialized via pretraining which is similar to initializing the Word2Vec parameters.\nThe pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts.\nThis has been exceptionally effective at building strong:\n\nrepresentations of language.\nparameter initializations for strong NLP models.\nprobability distributions over language that we can sample from.\n\n\n\n\nGenerative Pretrained Transformer (GPT)\n\nGPT is a decoder only Transformer model with 12 layers.\nGPT contains 768 dimensional hidden states, and 3072 dimensional feed-forward hidden layers.\nA subword vocabulary called Byte-Pair encoding with 40,000 merges.\nGPT models are trained on book corpus and contains over 7000 unique books which contains long spans of contiguous text, for learning long-distance dependencies.\n\n\n\nBidirectional Encoder Representations from Transformers (BERT)\nDevlin et al., 2018 proposed the Masked LM objective and released the weights of a pretrained Transformer and labeled BERT.\nSome of the details about Masked Language Model for BERT are: - Predict a random 15% of subword tokens. - Replace input word with [MASK] 80% of the time. - Replace input word with a random vocabulary token 10% of the time. - Leave input word unchanged 10% of the time but still predict.\nSome of the details about BERT: - Two models were released: - BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params. - BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params. - Trained on: - Books Corpus (800 million words) - English Wikipedia (2500 million words) - Pretraining is expensive and impractical on a single GPU: - BERT was pretrained with 64 TPU chips for a total of 4 days. - TPU are special tensor operations acceleration hardware. - Finetuning is practical and common on a single GPU."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#convolutional-neural-networks",
    "href": "posts/CNNs/CNNs.html#convolutional-neural-networks",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nThe five architectures of CNNs that have been pre-trained on the ImageNet dataset and, are present in the Keras library are mentioned below:\n\nVGG16\nVGG19\nResNet50\nInception V3\nXception"
  },
  {
    "objectID": "posts/CNNs/CNNs.html#parameterized-learning",
    "href": "posts/CNNs/CNNs.html#parameterized-learning",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Parameterized Learning",
    "text": "Parameterized Learning\nThe point of parameterized learning is to define a machine learning model that can learn patterns from our input data during training time, but have the testing process be must faster and to obtain a model that can be defined using a small number of parameters that can easily represent the network regardless of training size."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#vgg16-vgg19",
    "href": "posts/CNNs/CNNs.html#vgg16-vgg19",
    "title": "Convolutional Neural Networks Architectures",
    "section": "VGG16 & VGG19",
    "text": "VGG16 & VGG19\n\nFig: A visualization of the VGG architecture.\nThe figure presents the visualization of the VGG architecture. Images with 224 x 224 x3 dimensions are inputted to the network. Convolutions filters of only 3 x 3 are then applied with more convolutions stacked on top of each other prior to max pooling operations deeper in the architecture.\nThe VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, Very Deep Convolutional Networks for Large Scale Image Recognition. The VGG family of networks is characterized by using only 3 x 3 convolutional layers stacked on top of each other in increasing depth. The volume size is reduced by max pooling. Two fully-connected layers are then followed by a softmax classifier."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#resnet",
    "href": "posts/CNNs/CNNs.html#resnet",
    "title": "Convolutional Neural Networks Architectures",
    "section": "ResNet",
    "text": "ResNet\n\nFig: Left: The original residual module. Right: The updated residual module using pre-activation.\nThe ResNet module was introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition. The ResNet architecture has become a seminal work in the deep learning literature, demonstrating that extremely deep networks can be trained using standard SGD through the use of residual modules. Accuracy can be obtained by updating the residual module to use identity mappings."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#inception-v3",
    "href": "posts/CNNs/CNNs.html#inception-v3",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Inception V3",
    "text": "Inception V3\n\nFig: The original Inception module used in GoogLeNet.\nThe Inception module was introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions. The goal of the Inception module is to act as multi-level feature extractor by computing 1 x 1, 3 x 3, and 5 x 5 convolutions within the same module of the network. The output of these filters are then stacked along the channel dimension before being fed into the next layer in the network. The original incarnation of this architecture was called GoogLeNet."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#xception",
    "href": "posts/CNNs/CNNs.html#xception",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Xception",
    "text": "Xception\nXception module was introduced by Francois Chollet in their 2016 paper, Xception: Deep Learning with Depthwise Separable Convolutions. Xception is an extension to the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions."
  },
  {
    "objectID": "posts/Dataset/DataPreparation.html",
    "href": "posts/Dataset/DataPreparation.html",
    "title": "Data Fundamentals",
    "section": "",
    "text": "Outliers\nOutliers are examples that look dissimilar to the majority of examples from the dataset. Dissimilarity is measured by some distance metric, such as Euclidean distance. Deleting outliers from the training dataset is not considered scientifically significant, especially in small datasets. In the big data context, outliers don’t typically have a significant impact on the model.\n\n\nData Leakage\nData leakage, also known as target leakage, is a problem affecting several stages of the machine learning life cycle, from data collection to model evaluation. Data leakage in supervised learning is the unintentional introduction of information about the target that shouldn’t be made available.\n\n\nWhat is Good Data\n\nGood data is informative.\nGood data has good coverage.\nGood data reflects real inputs.\nGood data is unbiased.\nGood data is not a result of a feedback loop.\nGood data has consistent labels.\nGood data is big enough to allow generalization.\n\n\n\nData Augmentation\nThe most effective strategy applied to images to get more labeled examples without additional labeling is called data augmentation. The simple operations are flip, rotation, crop, color shift, noise addition, perspective correction, contrast change, and information loss.\nMixup is the popular technique of data augmentation which consists of training the model on a mix of the images from the training set. Instead of training the model on the raw images, we take two images and use for training their linear combination:\nmixup_image = t x image₁ + (1 - t) x image₂\nmixup_target = t x target₁ + (1 - t) x target₂\n\n\nOversampling\nOversampling is a technique to mitigate the class imbalance by making multiple copies of minority class examples. Two popular algorithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\n\n\nUndersampling\nUndersampling is a technique to mitigate the class imbalance by removing some examples from the training set of the majority class based on the property called Tomek links. A Tomek link exists between two examples xi and xj belonging to two classes if there’s no other examples xk in the dataset closer to either xi or xj than the latter two are to each other.\n\n\nData Sampling\nIn probability sampling, all examples have a chance of being selected and it involves randomness. Nonprobability sampling is not random and it follows a fixed deterministic sequence of heuristic actions which means that some examples don’t have a chance of being selected, no matter how many samples we build.\nIn stratified sampling, we first divide our dataset into groups called strata and then randomly select examples from each stratum, like in simple random sampling. It often improves the representativeness of the sample by reducing its bias.\n\n\nData Versioning\nIf data is held and updated in multiple places, we might need to keep track of versions. Versioning the data is also needed if we frequently update the model by collecting more data, especially in an automated way. Data versioning can be implemented in several levels of complexity.\nLevel 0: data is unversioned. Level 1: data is versioned as a snapshot at training time. Level 2: both data and code are versioned as one asset. Level 3: using or building a specialized data versioning solution.\n\n\nData Lake\nA data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or files. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#the-perfect-commit",
    "href": "posts/Git & GitHub/Git.html#the-perfect-commit",
    "title": "Git & GitHub",
    "section": "The Perfect Commit:",
    "text": "The Perfect Commit:\n\nAdd the right changes.\nCompose a good commit message.\nAdd the changes to a commit that is only related to the single topic rather than adding all the changes to a single commit. Git staging is vital in selecting the correct files for perfect commit.\n\ngit add -p file: Commit only a patch of file.\ngit commit: Write a subject with one space and write body of commit."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#git-branching-strategies",
    "href": "posts/Git & GitHub/Git.html#git-branching-strategies",
    "title": "Git & GitHub",
    "section": "Git Branching Strategies",
    "text": "Git Branching Strategies\n\nGit allows you to create branches - but it doesn’t tell you how to use them.\nWe need a written best practice of how work is ideally structured in our team - to avoid mistakes and collisions.\nIt highly depends on our team and team size, on our project, and on how we handle releases.\nIt helps to onboard new team members with proper documentation."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#integrating-changes-structuring-releases",
    "href": "posts/Git & GitHub/Git.html#integrating-changes-structuring-releases",
    "title": "Git & GitHub",
    "section": "Integrating Changes & Structuring Releases",
    "text": "Integrating Changes & Structuring Releases\n\nMainline Development (“Always Be Integrating”).\nState, Release, and Feature Branches.\n\nLong Running Branches:\n\nExist through the complete lifecycle of the project.\nOften, they mirror “stages” in the development life cycle.\nCommon convention: no direct commits.\n\nShort-Lived Branches:\n\nFor new features, bug fixes, refactorings, and experiments.\nWill be deleted after integration (merge/ rebase).\n\n\n\nPull Requests\n\nCommunicating About and Reviewing Code."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#cherry-picking",
    "href": "posts/Git & GitHub/Git.html#cherry-picking",
    "title": "Git & GitHub",
    "section": "Cherry Picking",
    "text": "Cherry Picking\n\nPick one specific commit and move it to another branch.\n\ngit checkout branch\ngit cherry-pick #hash"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#reflog",
    "href": "posts/Git & GitHub/Git.html#reflog",
    "title": "Git & GitHub",
    "section": "Reflog",
    "text": "Reflog\n\nA protocol or journal of every movement of the HEAD pointer.\nIt will be useful in recovering Deleted Commits & Deleted Branches.\n\ngit reflog\ngit reset/branch (branch-name) #hash"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#submodules",
    "href": "posts/Git & GitHub/Git.html#submodules",
    "title": "Git & GitHub",
    "section": "Submodules",
    "text": "Submodules\n\nmkdir lib & cd lib for standards.\ngit submodule add {url}\n\nWhen we create a submodule, the actual contents of the submodule are not stored in the parent repository. The parent repo only stores the submodule remote URL. The information of submodules is stored in .git modules & .git config files.\nWhen we clone a repo with the default git clone {url}, the submodules folders, however, are stayed empty. We can get the contents of submodules using the command:\n\nNavigate to the root directory of your cloned repository.\ngit submodule init\ngit submodule update\n\nWe can achieve the above directly while cloning the repo like this:\n\ngit clone --recurse-submodules {url}"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#search-find",
    "href": "posts/Git & GitHub/Git.html#search-find",
    "title": "Git & GitHub",
    "section": "Search & Find",
    "text": "Search & Find\n\nFiltering out commit history\n\nBy date –before / –after\nBy message –grep\nBy author –author\nBy file –&lt;filename&gt;\nBy branch &lt;branch&gt;\n\ngit log branch..main will show all the commits that are in the main but not in branch which comes in handy in merging."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#basic-git-commands",
    "href": "posts/Git & GitHub/Git.html#basic-git-commands",
    "title": "Git & GitHub",
    "section": "Basic Git Commands",
    "text": "Basic Git Commands\n\ngit init: Initialize a local Git repository.\ngit add &lt;file&gt;: Add files to staging area.\ngit status: Check status of working tree.\ngit commit: Commit changes in index.\ngit push: Push to remote repository.\ngit pull: Pull latest from remote repository.\ngit clone url: Clone repository into a new directory.\ngit checkout branch: Switch branches or restore working tree files.\ngit branch: List, create, or delete branches.\ngit diff: Show changes between commits, commit and working tree, etc.\ngit log: Show commit logs.\ngit stash: Stash the changes in a dirty working directory away.\ngit restore --staged &lt;file&gt;: Unstage a file while retaining the changes in working directory.\ngit reset hash: Reset current HEAD to the specified state.\ngit stash pop: Remove a single stashed state from the stash list and apply it on top of the current working tree state.\ngit stash list: List the stash entries that you currently have.\ngit stash drop: Remove a single stashed state from the stash list.\ngit stash clear: Remove all the stash entries that you currently have.\ngit  remote -v: List all currently configured remote repositories.\ngit remote add origin &lt;url&gt;: Add a remote repository.\ngit remote set-url origin &lt;url&gt;: Change the url of the remote repository.\ngit remote add upstream &lt;url&gt;: Add a upstream repository.\ngit remote set-url upstream &lt;url&gt;: Change the url of the upstream repository.\ngit fetch --all --prune: Fetch all the remote branches and delete the remote branches that are deleted in the remote repository.\ngit reset --hard origin/&lt;branch&gt;: Reset the current branch to the remote branch.\ngit merge &lt;branch&gt;: Merge a branch into the current branch.\ngit rebase &lt;branch&gt;: Rebase the current branch onto the specified branch.\ngit rebase -i &lt;branch&gt;: Rebase the current branch onto the specified branch and squash commits.\ngit merge --allow-unrelated-histories {branch_name}: Merge two unrelated branches."
  },
  {
    "objectID": "posts/KV/kv.html",
    "href": "posts/KV/kv.html",
    "title": "Understanding KV Cache",
    "section": "",
    "text": "In this article, we will discuss the Key-Value cache. We will start with the introduction of the Key-Value cache, then we will discuss the problem, solution, limitations, and implementation of the Key-Value cache. You can also watch the video by Umar Jamil on the Key-Value cache here.\n\nIntroduction\nKV cache, short for Key Value cache, is a technique used to accelerate the inference process in Large Language Models (LLMs), particularly in autoregressive models i.e. the current token depends on the previous tokens in a sequence. In these models, the tokens are generated one by one which can be computationally expensive because it repeats certain computations at each time step. The KV cache technique is only used during the inference process and not during the training process.\nIn the KV cache, the output of the model from previous time step is appended to the cache of key and value matrices of the current time step but the query matrix is updated at each time step to generate the next token. This way of caching the previous keys and values ensures that the model does not repeat the computations at each time step. This significantly reduces the size of the matrices used in the computation which makes the inference process faster (matrix multiplication faster).\n\n\nWhy do we need a Key-Value cache?\nSuppose, we want to generate I saw a beautiful car [EOS] where [EOS] is the end of the sentence token. The model will generate the tokens one by one at each time step. If we want to predict the token car, the model will generate the token I first, then saw, then a, and so on. The problem is that the model will generate the token I every time it wants to predict the next token which is waste of computation because the token I is the same for every time step. So, we need a technique called Key-Value cache to solve this problem.\n\n\n\nFig a. The image taken from Umar’s video clearly presents the problem in typical Self-Attention computations.\n\n\nThe image above shows the attention computation at inference time step 4. We multiply the query matrix with the transpose of the key matrix to get the attention scores. Then we apply the softmax function to the attention scores to get the attention weights. Finally, we multiply the attention weights with the value matrix to get the attention output. This is the typical self-attention computation used in all transformer models. Since, the model is causal (i.e. the attention of a token only depends on the preceding token), we only need the last row of the attention scores matrix to ge the desired attention output for the current token. This means that we don’t need extra computations of matrix multiplication, which is resolved by the KV cache technique.\n\n\nHow does KV cache solve the problem?\nWith the KV cache, we cache the previous Key and Value matrices and focus only on computing the attention scores for the current token. The query matrix is updated at each time step to generate the next token. This way of caching the previous keys and values ensures that the model does not repeat the computations at each time step. This significantly reduces the size of the matrices used in the computation which makes the inference process faster.\n\n\n\nFig b. The image taken from Umar’s video clearly presents the efficient computation of attention with KV cache technique.\n\n\nThe image above shows the attention computation at inference time step 4 with KV cache. We cache the key and value matrices from the previous time step and only compute the attention scores for the current token. This way, we don’t need to repeat the computations at each time step and get the desired attention output for the current token as shown in the image above.\n\n\nWhat are the limitations of KV cache?\nThe only limitation of the KV cache is that the caching of the key and value requires more GPU memory (or CPU memory). At each time step during the generation process, the key and value matrices are appended to the cache which grows with the number of tokens generated and is stored in the memory. This becomes a bottleneck especially for large models with billions of parameters and long sequences.\nThe problem of the high memory utilization also happens without KV Cache, because without it, we’d still need to give all the sequence as Q, K and V. With KV-Cache, actually the memory utilization is less because the entire sequence is only saved for K and V. The high memory utilization comes from the fact that we need to save the KV-Cache for EACH LAYER, while without the KV-Cache, we can just delete the activations of the previous layers once the output is computed.\n\n\nHow to implement a KV cache?\nWhile working on the implementation of the Llama 2: Open Foundation and Fine-Tuned Chat Models, I implemented the KV cache in the Grouped Query Attention. The code snippet below shows the implementation of the KV cache.\n\n\n\nFig c. Implementation of KV cache in Grouped Query Attention.\n\n\n\n\nConclusion\nSo, while KV caching can significantly speed up the inference process, it does so at the cost of increased memory usage. In this article, we discussed the Key-Value cache. We started with the introduction of the Key-Value cache, then we discussed the problem, solution, limitations, and the implementation of the KV cache in the Grouped Query Attention. You can also check my implementation of Llama 2 here.\n\n\nReferences\n\nUmar’s video on KV cache\nLlama2 Implementation\nLlama 2: Open Foundation and Fine-Tuned Chat Models\nAttention Is All You Need"
  },
  {
    "objectID": "posts/Machine/MachineLearning.html",
    "href": "posts/Machine/MachineLearning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine Learning\nMachine learning can be defined as the process of solving a practical problem by collecting a dataset, and algorithmically training a statistical model based on that dataset.\n\n\nSupervised Learning\nThe goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector. For instance, a model created using a dataset of patients could take as input a feature vector describing a patient and output a probability that the patient has cancer.\n\n\nUnsupervised Learning\nThe goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practice problem. Clustering is useful for finding groups of similar objects in a large collection of objects, such as images or text documents.\n\n\nReinforcement Learning\nReinforcement learning is a subfield of machine learning where the machine also called an agent lives in an environment and is capable of perceiving the state of that environment as a vector of features. A common goal of reinforcement learning is to learn a function that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average long-term reward.\n\n\nRaw and Tidy Data\nRaw data is a collection of entities in their natural form: they cannot always be directly fed to machine learning algorithms. Tidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example.\n\n\nTraining and Holdout Sets\nThe first step in a machine learning project is to shuffle the examples and split them into three distinct sets: training, validation, and test. The learning algorithm uses the training set to produce the model. The validation set is used to choose the learning algorithm, and find the best configuration values for that learning algorithm, also known as hyperparameters. The test set is used to assess the model performance before delivering it to the client or putting the model into production.\n\n\nParameters vs. Hyperparameters\nHyperparameters are inputs of machine learning algorithms or pipelines that influence the performance of the model. They don’t belong to the training data and cannot be learned from it.\nParameters are variables that define the model trained by the learning algorithm. Parameters are directly modified by the learning algorithm based on the training data to find the optimal values of the parameters.\n\n\nWhen to use Machine Learning\n\nWhen the problem is too complex for coding.\nWhen the problem is constantly changing.\nWhen it is a perceptive problem such as speech, image, and video recognition.\nWhen it is an unstudied phenomenon.\nWhen the problem has a simple objective.\n\n\n\nMachine Learning Engineering\nMachine learning engineering is the use of scientific principles, tools and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE ecompasses all stages from data collection, to model training, to making the model available for use by the customers. MLE includes any activity that lets machine learning algorithms be implemented as a part of an effective production system.\nA machine learning project life cycle consists of the following steps:\n\nGoal definition.\nData collection and preparation.\nFeature engineering.\nModel training.\nModel evaluation.\nModel deployment.\nModel serving.\nModel monitoring.\nModel maintenance.\n\n\n\nBias\nBias in data is an inconsistency with the phenomenon that data represents. Selection bias is the tendency to skew your choice of data sources to those that are easily available, convenient, and cost-effective. Omitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction. Sampling bias also known as distribution shift occurs when the distribution of examples used for training doesn’t reflect the distribution of the inputs the model will receive in production.\nIt is usually impossible to know exactly what biases are present in a dataset. Biases can be avoided by questioning everything: who created the data, what were their motivations and quality criteria, and more importantly, how and why the data was created."
  },
  {
    "objectID": "posts/MOE/mistral.html",
    "href": "posts/MOE/mistral.html",
    "title": "Mixture of Experts in Mistral",
    "section": "",
    "text": "Introduction\nMixture of Experts (MoE) is a neural network that divides the list of Modules into specialized experts, each responsible for processing specific tokens or aspects of the input data. This approach aims to optimize the memory usage and computational efficiency by activating only the necessary experts for a given input which allows for more efficient pretraining and faster inference.\n\n\nMoE in Mistral\nIn my blog post on Model Sharding, I discussed the architecture of Mistral and how it uses model sharding to distribute the model parameters, gradients, and optimizer states across multiple GPUs along with diagrammatic representation.\nThe architecture of Mistral contains an Embedding Layer, Transformer Block that contains [Attention Layers, MOE Layers or FeedForward Layers with or without Normalization Layers], and a Normalization Layer and a Linear Output Layer with Softmax. The Transformer Block is repeated n times where n is the number of layers in the model. So, the basic architecture of Mistral becomes Embedding Layer -&gt; Transformer Block -&gt; Normalization Layer -&gt; Linear Output Layer with Softmax.\n\n\n\nFig a. Transformer Block of Mistral\n\n\nThe Transformer Block in Mistral contains Normalization Layers before Attention Layers and before Mixture of Experts (MoE) Layer. The MoE Layer contains a Linear Layer which is also known as the Gate Layer that converts the input to the dimensions equal to the number of experts. The experts are the FeedForward Layers which are repeated n times where n is the number of experts. We extract the top_k logits or weights from the output of the Gate Layer and use them to select the top_k experts. We then create the sparse tensor matrix and use it to get the output from the selected experts. The MoE Layer is used in place of the FeedForward Layer in the Transformer Block.\nAccording to the documentation of Mistral:\nSparse Mixture of Experts allows one to decouple throughput from memory costs by only activating subsets of the overall model for each token. In this approach, each token is assigned to one or more “experts” – a separate set of weights – and only processed by sunch experts. This division happens at feedforward layers of the model. The expert models specialize in different aspects of the data, allowing them to capture complex patterns and make more accurate predictions.\nIt means that MoE aims to balance the computation efficiency or throughput and memory requirements by activating only the necessary experts for a given input which takes place at the feedforward layers of the model. Each expert specializes in different aspects of the data which allows them to capture complex patterns in the data instead of using a single feedforward network for the entire data.\n\n\nImplementation\nAs shown in the diagram above, the MoE Layer contains a Linear Layer (Gate Layer) and a list of Modules of FeedForward Layers (Experts). The implementation of MoE Layer in Mistral is as follows:\n@dataclasses.dataclass\nclass MoeArgs(Serializable):\n    num_experts: int\n    num_experts_per_tok: int\n\n\nclass MoeLayer(nn.Module):\n    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoeArgs):\n        super().__init__()\n        assert len(experts) &gt; 0\n        self.experts = nn.ModuleList(experts)\n        self.gate = gate\n        self.args = moe_args\n\n    def forward(self, inputs: torch.Tensor):\n        gate_logits = self.gate(inputs)\n        weights, selected_experts = torch.topk(\n            gate_logits, self.args.num_experts_per_tok\n        )\n        weights = F.softmax(weights, dim=1, dtype=torch.float).to(inputs.dtype)\n        results = torch.zeros_like(inputs)\n        for i, expert in enumerate(self.experts):\n            batch_idx, nth_expert = torch.where(selected_experts == i)\n            results[batch_idx] += weights[batch_idx, nth_expert, None] * expert(\n                inputs[batch_idx]\n            )\n        return results\n\n\nConclusion\nMixture of Experts (MoE) is a neural network that divides the list of Modules into specialized experts in order to optimize the memory usage and computational efficiency by activating only the necessary experts for a given input. The MoE Layer is used in place of the FeedForward Layer in the Transformer Block of Mistral which contains a Linear Layer (Gate) and a list of Modules (FeedForward) Layers.\n\n\nReferences\n\nMistral Source Code\nLlama2 Implementation\nAttention Is All You Need"
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#neural-networks",
    "href": "posts/NeuralNetworks/CNN.html#neural-networks",
    "title": "Fundamentals of CNNs",
    "section": "Neural Networks",
    "text": "Neural Networks\nNeural networks are the building blocks of deep learning systems. A system is called a neural network if it contains a labeled, directed graph structure where each node in the graph performs some computation.\n\nFigure: A simple neural network that takes the weighted sum of inputs x and weights w which is then passed through the activation function to determine the output."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#activation-functions",
    "href": "posts/NeuralNetworks/CNN.html#activation-functions",
    "title": "Fundamentals of CNNs",
    "section": "Activation Functions",
    "text": "Activation Functions\n\nSigmoid Function is continuous and differentiable everywhere. It is symmetric around the y-axis. The outputs of the sigmoid are not zero centered. Saturated neurons essentially kill the gradient, since the delta of the gradient will be extremely small.\nRectified Linear Unit (ReLU) is zero for negative inputs but increases linearly for positive inputs. The ReLU function is not saturable and is also extremely computationally efficient. ReLU is the most popular activation function used in deep learning and has stronger biological motivations."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#feedforward-neural-networks",
    "href": "posts/NeuralNetworks/CNN.html#feedforward-neural-networks",
    "title": "Fundamentals of CNNs",
    "section": "Feedforward Neural Networks",
    "text": "Feedforward Neural Networks\nThe neural networks architecture in which a connection between nodes is only allowed from nodes in layer i to nodes in layer i + 1, with no backward or inter-layer connections are called feedforward neural networks. When feedforward neural networks include feedback connections i.e. output connections that feed back into the inputs, are called recurrent neural networks."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#backpropagation",
    "href": "posts/NeuralNetworks/CNN.html#backpropagation",
    "title": "Fundamentals of CNNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nBackpropagation is a generalization of the gradient descent algorithms that is specifically used to train multi-layer feedforward networks. The backpropagation algorithm consists of two phases:\n\nThe forward pass where we pass our inputs through the network to obtain our output classifications.\nThe backward pass i.e. weight update phase where we compute the gradient of the loss function and use the information to iteratively apply the chain rule to update the weights in neural networks.\n\nThe activation functions inside the network are differentiable, allowing the chain rule to be applied. Any layers inside the network that require updates to their weights and parameters must be compatible with backpropagation.\nOne Hot Encoding is the process of transforming integer labels into vector labels, where the index in the vector for label is set to 1 and 0 otherwise."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#convolutions",
    "href": "posts/NeuralNetworks/CNN.html#convolutions",
    "title": "Fundamentals of CNNs",
    "section": "Convolutions",
    "text": "Convolutions\nConvolutions are one of the most critical, fundamental building-blocks in computer vision and image processing. Convolution is an element-wise multiplication of two matrices followed by a sum."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#convolutional-layers",
    "href": "posts/NeuralNetworks/CNN.html#convolutional-layers",
    "title": "Fundamentals of CNNs",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\nIt accepts an input volume of size Winput  X Hinput X Dinput .The input sizes are normally square, so it’s common to see Winput = Hinput . The number of filters K controls the depth of the output volume. The output of the convolutional layer is Woutput X Houtput  X Doutput , where:\n\nWoutput = ((Winput - F + 2P)/S) + 1.\nHoutput = ((Hinput - F + 2P)/S) + 1.\nDoutput = K\n\nHere, K is the number of filters, which when used for convolution yields receptive field F. S is the stride and P is the amount of zero-padding."
  },
  {
    "objectID": "posts/ORPO/orpo.html",
    "href": "posts/ORPO/orpo.html",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "",
    "text": "In this blog post, we will discuss the reference model free monolithic odds ratio preference optimization algorithm (ORPO) proposed in the paper ORPO: Monolithic Preference Optimization without Reference Model. Most of the diagrams and equations are taken from the orginal paper."
  },
  {
    "objectID": "posts/ORPO/orpo.html#introduction",
    "href": "posts/ORPO/orpo.html#introduction",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "Introduction",
    "text": "Introduction\nIn order to make the pretrained large language models (LLMs) more useful in practice for general applications, it is important that they undergo the phases of instruction tuning and preference tuning or alignment.\nInstruction tuning is the process of fine-tuning the LLMs to follow the instructions given in the natural language text. This makes the LLMs more useful for the specific task at hand but it does not guarantee that the LLMs will generate the desired output which is not always harmful but can be in some cases.\nTherefore, Preference alignment is the process of fine-tuning the LLMs to generate the output that aligns with the preferences of the user. We use pairwise preference data (e.g. A is chosen and B is rejected) to train the LLMs to generate the output that aligns with the preferences of the user. In our previous blog post on PPO, we discussed how to train the Reward model using the pairwise preference data. In this blog post, we will discuss the ORPO algorithm which is a reference model free monolithic odds ratio preference optimization algorithm."
  },
  {
    "objectID": "posts/ORPO/orpo.html#supervised-fine-tuning-sft",
    "href": "posts/ORPO/orpo.html#supervised-fine-tuning-sft",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "Supervised Fine-tuning (SFT)",
    "text": "Supervised Fine-tuning (SFT)\nIn reinforcement learning algorithms, such as PPO (Proximal Policy Optimization), the reward model is trained in such a way to maximize the score for the chosen answer resulting in models that are trained with human preferences. DPO (Direct Policy Optimization) combines the above mentioned step (i.e. reward model training) into the preference optimization step.\nPreference alignment methods in the context of reinforcement learning often leverage the SFT to ensure the stable update of the online policy (i.e. model being optimized) in relation to the offline policy (i.e. model used for sampling). If we study the loss function used in SFT, the goal of the cross-entropy loss is to penalize the model if the predicted logits for the reference answers are lower.\n\\(L_{SFT} = -\\frac{1}{M} \\sum_{k=1}^{M}\\sum_{i=1}^{|V|} y_{i}^{k} \\log p_{i}^{k}\\)\nwhere, \\(y_{i}^{k}\\) is the boolean value indicating whether if the \\(i^{th}\\) token in the vocabulary is the label token, \\(p_{i}^{k}\\) is the probability of the \\(i^{th}\\) token and \\(M\\) is the length of the sequence.\nUsing the above loss function, the model gives no penalty for the logits of the non-answer tokens. Therefore, the log probabilities of the tokens in the rejected answers increase along with the chosen answers, which is not desirable in preference alignment.\n\n\n\nFig a. Log probabilities for chosen and rejected answers which shows comparable likelihood of generation."
  },
  {
    "objectID": "posts/ORPO/orpo.html#odds-ratio-preference-optimization-orpo",
    "href": "posts/ORPO/orpo.html#odds-ratio-preference-optimization-orpo",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "Odds Ratio Preference Optimization (ORPO)",
    "text": "Odds Ratio Preference Optimization (ORPO)\nLet’s consider an input sequence \\(x\\), the average log-likelihood of generating the output sequence \\(y\\), of length \\(m\\) tokens is: \\(\\log_\\theta P(y|x) = \\frac{1}{m} \\sum_{i=1}^{m} \\log P_\\theta(y_i|x)\\), where \\(\\theta\\) is the model parameters. Then the odds of generating the output sequence \\(y\\) given the input sequence \\(x\\) is: \\(odds_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\). Thus, the odds ratio of choosen answer \\(y_w\\) over the rejected answer \\(y_l\\) is: \\(OR_\\theta(y_w, y_l|x) = \\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)}\\), which indicates that how likely the model is to generate the choosen output answer over the rejected output answer.\nORPO incorporates the odds ratio based loss to the original negative log-likelihood loss function for differentiating the likelihood of generating the choosen answer and the rejected answer.\n\n\n\nFig b. Comparison of model alignment techniques.\n\n\nORPO aligns the language model without a reference model in a single-step by assigning a weak penalty to the log probabilities of the rejected answers and a strong adoption signal to the log probabilities of the choosen answers with a simple odds ratio based loss appended to the original negative log-likelihood loss function."
  },
  {
    "objectID": "posts/ORPO/orpo.html#objective-function-orpo-loss",
    "href": "posts/ORPO/orpo.html#objective-function-orpo-loss",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "Objective Function & ORPO Loss",
    "text": "Objective Function & ORPO Loss\nThe objective function of ORPO consists of two components:\n1. Supervised fine-tuning loss:\n\\(L_{SFT} = -\\frac{1}{M} \\sum_{k=1}^{M}\\sum_{i=1}^{|V|} y_{i}^{k} \\log p_{i}^{k}\\).\n\nOdds ratio loss:\n\\(L_{OR} = - log\\sigma(log \\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)})\\), where \\(\\sigma\\) is the sigmoid function.\n\nTherefore, \\(L_{ORPO} = L_{SFT} + \\lambda L_{OR}\\), where \\(\\lambda\\) controls the pretrained language model to adopt to the specific subset of desired output and discourages the generation of the rejected answers. Log odds ratio loss is wrapped in the sigmoid function so that it can be minimized by increasing the log odds between \\(y_w\\) and \\(y_l\\).\n\nImplementation of ORPO Loss\ndef compute_loss(model, inputs, return_outputs=False):\n    \"\"\"\n    This is the original implementation of computing the ORPO loss function \n    added by the authors of the paper. Reference: https://github.com/xfactlab/orpo/blob/d13cdc025f942bdc49ba9c793350a3b1f2df25a0/src/orpo_trainer.py#L38\n    \"\"\"\n    \n    # Generate the hidden states for 'chosen' and 'reject'.\n    neg_labels = inputs['negative_input_ids'].clone()\n    pos_labels = inputs['positive_input_ids'].clone()\n\n    # Generate the outputs for 'chosen' and 'reject'.\n    outputs_neg = model(**{'input_ids': inputs['negative_input_ids'],\n                            'attention_mask': inputs['negative_attention_mask'],\n                            'labels': neg_labels,}, output_hidden_states=True)      \n    outputs_pos = model(**{'input_ids': inputs['positive_input_ids'],\n                            'attention_mask': inputs['positive_attention_mask'],\n                            'labels': pos_labels,}, output_hidden_states=True)\n        \n    # Calculate NLL loss.\n    pos_loss = outputs_pos.loss\n    \n    # Calculate the log probabilities of the 'chosen' and 'reject' outputs.\n    pos_prob = self.compute_logps(prompt_attention_mask=inputs['attention_mask'],\n                                    chosen_inputs=inputs['positive_input_ids'], \n                                    chosen_attention_mask=inputs['positive_attention_mask'], \n                                    logits=outputs_pos.logits)\n    neg_prob = self.compute_logps(prompt_attention_mask=inputs['attention_mask'], \n                                    chosen_inputs=inputs['negative_input_ids'], \n                                    chosen_attention_mask=inputs['negative_attention_mask'], \n                                    logits=outputs_neg.logits)\n\n    # Calculate the log odds and the ratio.\n    log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n    sig_ratio = torch.nn.functional.sigmoid(log_odds)\n    ratio = torch.log(sig_ratio)\n    \n    # Calculate the ORPO loss.\n    loss = torch.mean(pos_loss - self.alpha * ratio).to(dtype=torch.bfloat16)\n    \n    return (loss, outputs_pos) if return_outputs else loss"
  },
  {
    "objectID": "posts/ORPO/orpo.html#conclusion",
    "href": "posts/ORPO/orpo.html#conclusion",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we discussed the ORPO algorithm which is a reference model free monolithic odds ratio preference optimization algorithm. ORPO incorporates the odds ratio based loss to the original negative log-likelihood loss function for differentiating the likelihood of generating the choosen answer and the rejected answer. The ORPO loss function is a combination of the supervised fine-tuning loss and the odds ratio loss."
  },
  {
    "objectID": "posts/ORPO/orpo.html#references",
    "href": "posts/ORPO/orpo.html#references",
    "title": "Odds Ratio Preference Optimization (ORPO)",
    "section": "References",
    "text": "References\n\nHong, J., Lee, N., & Thorne, J. (2024). ORPO: Monolithic Preference Optimization without Reference Model. ArXiv. /abs/2403.07691"
  },
  {
    "objectID": "posts/PGO/policy_gradient.html",
    "href": "posts/PGO/policy_gradient.html",
    "title": "Policy Gradient Optimization",
    "section": "",
    "text": "This blog is the continuation of my previous blog, Introduction to Reinforcement Learning. In this blog, we will discuss the concept of Policy Gradient algorithm in the context of Reinforcement Learning. We will also derive the expression for the gradient of the objective function w.r.t the policy parameters and discuss the intuition behind the Policy Gradient algorithm. We will also show the practical implementation of training the agent using the Policy Gradient algorithm."
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#gradient-descent",
    "href": "posts/PGO/policy_gradient.html#gradient-descent",
    "title": "Policy Gradient Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nTo understand the Gradient Descent, we first need to understand the concept of a gradient. Slope is the rate of change of a vertical variable w.r.t the horizontal variable i.e. the ratio of vertical distance to horizontal distance. Derivative is the instantaneous rate of change of the function at a point. In other words, the derivate of a function at a point is the slope of the tangent line where the point touches the curve of the function. Similarly, the gradient is the generalization of the derivative for the multi-variable functions. It represents the rate of change of the function w.r.t each variable. Mathematically, for the multi-variable function \\(f(x_1, x_2, ..., x_n)\\), the gradient is denoted by \\(\\nabla f\\) and is defined as the vector composed of partial derivatives of the function w.r.t each variable. For example, the gradient of the function \\(f(x, y) = x^2 + y^2\\) is \\(\\nabla f = [2x, 2y]\\).\nWhile training neural networks, we activate each neurons with parameterized weights along with some biases, then impose the sum of the output of each neurons by a non-linear activation functions like sigmoid or ReLU. At first, the values of the weights and biases are randomly initialized and the output of the network is also random which means the output of the model is far from the actual output. So, we need to use the cost function which basically tells how bad is the performance of our model. Mathematically, the cost function is the sum of the squared difference between the predicted output and the actual output. The sum is small when the model confidently gives the correct output but it’s large when it seems the model really doesn’t know what it’s giving. When we say the model is training, we are actually trying to minimize the cost function in such a way that the predicted output is as close as possible to the actual output. Then, we use the novel method called gradient descent to minimize the cost function as quickly as possible. In calculus, gradient of the function gives the direction of steepest ascent to increase the function quickly. The negative of the gradient gives the direction of the steepest descent to decrease the function quickly. The gradient of the cost function basically tells which weights and biases of which neuron matters the most. Therefore, gradient descent is the iterative optimization algorithm that minimizes the cost function w.r.t the parameters of the model by moving in the direction of the negative gradient of the cost function. The learning rate is the hyperparameter that controls the step size of the gradient descent algorithm.\n\n\n\nFig a. Gradient Descent"
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#loss-function",
    "href": "posts/PGO/policy_gradient.html#loss-function",
    "title": "Policy Gradient Optimization",
    "section": "Loss Function",
    "text": "Loss Function\nIn simple words, the cost function is the average of the loss function of all the available data points. The loss function tells that how bad is the performance of our model for each single data point.\nWhile training the neural networks, we try to optimize the model parameters i.e. weights and biases in such a way that we get the right weights and biases so that our model can give correct output. Let’s say we have a loss function for a single data point as \\(L(y, \\hat{y}) = (y - \\hat{y})^2\\) where \\(y\\) is the actual output and \\(\\hat{y}\\) is the predicted output, then we calculate the gradient of the loss function w.r.t the model parameters (weights and biases), we obtain the vector where each element of the vector tells how much the LF changes w.r.t the corresponding parameter. This gives us which parameter (weight or bias) matters the most to minimize the loss function. We repeat this process for all the data points with the objective to minimize the squared difference between the actual output and the predicted output. Therefore, the loss function is used to optimize the model parameters (weights and biases) in such a way that the model can give the correct output."
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#objective-of-reinforcement-learning",
    "href": "posts/PGO/policy_gradient.html#objective-of-reinforcement-learning",
    "title": "Policy Gradient Optimization",
    "section": "Objective of Reinforcement Learning",
    "text": "Objective of Reinforcement Learning\nIn my previous blog, Introduction to Reinforcement Learning, I have explained the key concepts of RL. In the context of stochastic nature, policy is the mapping from states to actions that tells us what is the probability of taking an action given a current state in such a way that the agent receives the maximum reward from the environment. Therefore, the objective or the goal of the RL is to learn the best policy or probability distribution of actions that maximizes the expected reward when the agent acts according to the policy.\nMathematically, if we consider the stochastic policy which is parameterized by \\(\\theta\\), then the objective of the RL is to find the optimal policy that maximizes the expected reward: \\(J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\) where \\(\\tau\\) is the trajectory of the agent (sequence of states, actions, and rewards), \\(\\pi_{\\theta}\\) is the policy parameterized by \\(\\theta\\) and \\(R(\\tau)\\) is the total reward of the trajectory. Therefore, \\(J(\\theta)\\) is the objective function that we want to maximize."
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#policy-gradient",
    "href": "posts/PGO/policy_gradient.html#policy-gradient",
    "title": "Policy Gradient Optimization",
    "section": "Policy Gradient",
    "text": "Policy Gradient\nAs mentioned earlier, the goal of the RL is to maximize the expected reward of all the possible trajectories of the agent by learning the optimal policy. The \\(J(\\theta)\\) is the objective function that we want to maximize using the gradient ascent algorithm. In the context of DL, we use the stochastic gradient descent to minimize the loss function or overall cost function where we calculate the gradient of the loss function w.r.t the model parameters and move in the direction of the negative gradient to minimize the loss function. Similarly, in the context of RL, we use the policy gradient ascent to maximize the objective function where we calculate the gradient of the objective function w.r.t the policy parameters and move in the direction of the gradient to maximize the objective function. Mathematically, we will optimize the policy parameters using gradient ascent algorithm: \\(\\theta_{k+1} = \\theta_k + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta} | \\theta_k)\\) where \\(\\alpha\\) is the learning rate and \\(\\nabla_{\\theta} J(\\pi_{\\theta} | \\theta_k)\\) is the gradient of the objective function w.r.t the policy parameters.\n\nIntuition behind Policy Gradient\n\n\n\nFig b. Policy Gradient Optimization in RL inspired by Andrej Karpathy’s blog.\n\n\nAs shown in the Fig a, let’s say we have a standard ML architecture where we feed the input data to the neural network or model and we get the probabilities of the actions as (UP, DOWN, LEFT, and RIGHT) in this case. Do you remember that we always optimize the probability of the correct actions in the context of the supervised learning? But in the context of RL, we don’t have the correct actions to optimize, so we use the Policy Gradient algorithm in this scenario. After we get the probabilities of the actions, we sample an action from the probability distribution: e.g. suppose we sample the action UP, we will execute the action in the environment. At this point please note that we could immediately fill in the gradient of 1.0 for the action UP as we do in the supervised learning and find the gradient of the objective function that would encourage the agent to more likely take the action UP in the future. But the problem is that we don’t yet know whether the action UP is good or bad so we can’t fill in the gradient until we get the reward from the environment. Let’s say we end up getting the positive reward after taking the action UP, then we fill in the gradient of the action UP with 1.0 and move in the direction of the gradient to maximize the objective function. Similarly, if we get the negative reward after taking the action UP, then we fill in the gradient of the action UP with -1.0 and do the backpropagation to find the gradient of the objective function that would discourage the agent to take the action UP in the future. Therefore, we have a stochastic policy that samples the actions from the probability distribution, and the action that happen to be good is encouraged and the action that happen to be bad is discouraged. This is the intuition behind the Policy Gradient algorithm in the context of Reinforcement Learning and this is how the agent learns the optimal policy even without the correct actions or references.\nDerivation of the Policy Gradient taken from the Policy Gradient Optimization blog.\n\n\n\nFig c. Policy Gradient Optimization in RL\n\n\nAs shown in the Fig b, we derived the expression for the gradient of the objective function as \\(\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) R(\\tau)]\\) where \\(\\tau\\) is the trajectory of the agent, \\(T\\) is the time horizon, \\(a_t\\) is the action at time \\(t\\), \\(s_t\\) is the state at time \\(t\\), \\(\\pi_{\\theta}(a_t | s_t)\\) is the probability of taking the action \\(a_t\\) given the state \\(s_t\\), and \\(R(\\tau)\\) is the total reward of the trajectory. This equation tells us that we should shift the distribution of the actions through the policy parameters \\(\\theta\\) if we wanted its sampled actions to achieve higher scores as judged by the reward \\(R(\\tau)\\). Particularly, it means that we should take sample action \\(a_t\\), evaluate the log probability of the action \\(\\log \\pi_{\\theta}(a_t | s_t)\\), use the gradient of the log probability \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)\\) that gives us the direction in parameter space that would increase the probability of the action \\(a_t\\) given the state \\(s_t\\).\nSo, this is an expectation which means that we can estimate it with a sample mean. If we sample the trajectory \\(\\tau\\) from the policy \\(\\pi_{\\theta}\\), then we can estimate the gradient of the objective function as \\(\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i | s_t^i) R(\\tau^i)\\) where \\(N\\) is the number of trajectories sampled from the policy \\(\\pi_{\\theta}\\), \\(a_t^i\\) is the action at time \\(t\\) in the trajectory \\(i\\), \\(s_t^i\\) is the state at time \\(t\\) in the trajectory \\(i\\), and \\(R(\\tau^i)\\) is the total reward of the trajectory \\(i\\). Therefore, we can estimate the gradient of the objective function using the sample mean of the trajectories sampled from the policy \\(\\pi_{\\theta}\\).\n\n\nPractical Implementation of Training the Agent using Policy Gradient\nREINFORCE algorithm is the simplest form of the Policy Gradient algorithm that optimizes the policy parameters directly using the gradient ascent. The update method of the REINFORCE algorithm which is a part of this policy_gradient.py script is shown below. We are using -1 in the loss function to ensure that we are moving in the direction of the gradient while calling the loss.backward() method.\ndef update(self):\n    \"\"\"Update the policy network using the REINFORCE algorithm.\"\"\"\n\n    # Compute the discounted rewards.\n    discounted_rewards = []\n    cumulative_reward = 0\n    for reward in self.rewards[::-1]:\n        cumulative_reward = reward + self.gamma * cumulative_reward\n        discounted_rewards.insert(0, cumulative_reward)\n\n    discounted_rewards = torch.tensor(discounted_rewards)\n\n    # Normalize the discounted rewards.\n    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (\n        discounted_rewards.std() + self.eps\n    )\n\n    # Compute the loss.\n    loss = 0\n    for log_prob, discounted_reward in zip(self.probs, discounted_rewards):\n        loss += (-1) * log_prob.mean() * discounted_reward\n\n    # Update the policy network.\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()\n\n    # Reset the memory.\n    self.probs = []\n    self.rewards = []\n\n    return loss.item()"
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#conclusion",
    "href": "posts/PGO/policy_gradient.html#conclusion",
    "title": "Policy Gradient Optimization",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog, we have discussed the concept of the Gradient Descent, Loss Function, and the Policy Gradient algorithm in the context of Reinforcement Learning. We have also derived the expression for the gradient of the objective function w.r.t the policy parameters and discussed the intuition behind the Policy Gradient algorithm. We have also shown the practical implementation of training the agent using the Policy Gradient algorithm. In the next blog, we will discuss the Proximal Policy Optimization (PPO) loss and other concepts in the context of Reinforcement Learning."
  },
  {
    "objectID": "posts/PGO/policy_gradient.html#references",
    "href": "posts/PGO/policy_gradient.html#references",
    "title": "Policy Gradient Optimization",
    "section": "References",
    "text": "References\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. ArXiv. /abs/1312.5602\nLi, Y. (2018). Deep Reinforcement Learning. ArXiv. /abs/1810.06339\nReinforcement Learning from Human Feedback explained with math derivations and the PyTorch code.\nIntro to Policy Optimization\nDeep Reinforcement Learning: Pong from Pixels"
  },
  {
    "objectID": "posts/RL/intro_to_rl.html",
    "href": "posts/RL/intro_to_rl.html",
    "title": "Introduction to Reinforcement Learning",
    "section": "",
    "text": "I am going to write a series of posts on Reinforcement Learning. This post is the first post in the series. In this post, I will introduce the basic concepts of Reinforcement Learning."
  },
  {
    "objectID": "posts/RL/intro_to_rl.html#what-is-reinforcement-learning",
    "href": "posts/RL/intro_to_rl.html#what-is-reinforcement-learning",
    "title": "Introduction to Reinforcement Learning",
    "section": "What is Reinforcement Learning?",
    "text": "What is Reinforcement Learning?\nReinforcement Learning is a technique in machine learning that enables an agent to learn how to behave in an environment by performing actions and observing the rewards. The agent learns to achieve a goal by interacting with the environment. The agent learns by trial and error, and the goal is to maximize the cumulative reward.\n\n\n\nFig a. An agent interactivng with environment\n\n\nIn the above figure, the agent at state \\(s\\) takes an action \\(a\\) on the given environment. The environment returns the reward \\(r\\) and the next state \\(s'\\) based on the action performed at the given state \\(s\\). The agent learns from the reward and the next state and takes the next action in such a way that it maximizes the cumulative reward.\n\nState, Action, Reward, Transition Probability, Discounted Reward & Trajectory\nAn RL agent interacts with the environment over time. At each time step \\(t\\), the agent receives a state \\(s_t\\) in a state space \\(S\\), and selects an action \\(a_t\\) from an action space \\(A\\), following a policy \\(\\pi(a_t|s_t)\\), which is the agent’s strategy for selecting actions or mapping from state \\(s_t\\) to action \\(a_t\\). The agent receives a scalar reward \\(r_t\\), and transitions to the next state \\(s_{t+1}\\) according to the environment’s dynamics. The environment’s dynamics are defined by the transition probability \\(P(s_{t+1}|s_t, a_t)\\), which is the probability of transitioning to state \\(s_{t+1}\\) given that the agent was in state \\(s_t\\) and took action \\(a_t\\). The agent’s goal is to learn a policy \\(\\pi\\) that maximizes the expected cumulative reward, which is the sum of rewards over time, typically with a discount factor \\(\\gamma\\) to give more weight to immediate rewards.\nIn Reinforcement Learning, the agent is interacting with the environment. Environment is a system or a model that defines how the agent should interact or take actions to move from one state to another state. State (\\(s \\epsilon S\\)) is the current situation or characteristics of the agent in the environment. Action (\\(a \\epsilon A\\)) is the decision taken by the agent at the given state. Reward (\\(r\\)) is the feedback from the environment based on the action taken by the agent. Transition Probability \\(P(s'|s,a)\\) is the probability of moving from state \\(s\\) to state \\(s'\\) by taking action \\(a\\). Discounted Reward is the reward that is discounted by a factor \\(\\gamma\\) and it is used to give more importance to the immediate reward than the future reward.\nA trajectory is a sequence of states, actions, and rewards that the agent encounters while interacting with the environment. The trajectory is denoted by \\(\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, ...)\\).\n\n\nPolicy\nA policy which is denoted by \\(\\pi\\) is a mapping from states to actions that tells the agent what action to take in a given state. The policy can be deterministic or stochastic. A deterministic policy is a direct mapping from states to actions, while stochastic policy is a probability distribution over actions given states.\nA policy maps a state to an action, or, a distribution over actions, and policy optimization is the process of finding the optimal mapping. Value based methods optimize the value function first, then derive the optimal policies. Policy based methods optimize the objective function directly, or usually the cumulative rewards.\nThe objective of the agent is to find the optimal policy \\(\\pi^*\\) that maximizes the cumulative discounted reward. Mathematically, the objective is given by: \\(\\displaystyle\\sum_{t&gt;0} \\gamma r_t\\)\n\n\nValue Funtion\nThe value function is a function that estimates the expected cumulative reward that the agent can achieve by following a policy \\(\\pi\\) from a given state \\(s\\). The value function is denoted by \\(V^{\\pi}(s)\\) and it is defined as the expected cumulative reward that the agent can achieve by following a policy \\(\\pi\\) from a given state \\(s\\). The value function is defined as: \\(V^{\\pi}(s) = E_{\\pi}[ \\displaystyle\\sum_{t&gt;0} \\gamma^t r_t | s_0 = s,\\pi ]\\)\n\n\nQ-Value Function\nThe Q-value function is a function that estimates the expected cumulative reward that the agent can achieve by taking action \\(a\\) at state \\(s\\) and following a policy \\(\\pi\\) thereafter. The Q-value function is denoted by \\(Q^{\\pi}(s,a)\\) and it is defined as the expected cumulative reward that the agent can achieve by taking action \\(a\\) at state \\(s\\) and following a policy \\(\\pi\\) thereafter. The Q-value function is defined as: \\(Q^{\\pi}(s,a) = E_{\\pi}[ \\displaystyle\\sum_{t&gt;0} \\gamma^t r_t | s_0 = s, a_0 = a,\\pi ]\\)\nThe optimal Q-value function is denoted by \\(Q^*(s,a)\\) and it is defined as the maximum expected cumulative reward that the agent can achieve by taking action \\(a\\) at state \\(s\\) and following the optimal policy \\(\\pi^*\\) thereafter. The optimal Q-value function is defined as: \\(Q^*(s,a) = maxE_{\\pi^*}[ \\displaystyle\\sum_{t&gt;0} \\gamma^t r_t | s_0 = s, a_0 = a,\\pi^* ]\\)"
  },
  {
    "objectID": "posts/RL/intro_to_rl.html#q-learning",
    "href": "posts/RL/intro_to_rl.html#q-learning",
    "title": "Introduction to Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nThe optimal Q-value function as known as the optimal action-value function obeys the Bellman Equation. The Bellman Equation is based on the intuition that if the optimal action-value function \\(Q^*(s', a')\\) of the sequence of states \\(s'\\) and the next time step is known for all possible actions \\(a'\\), then the optimal action-value function \\(Q^*(s,a)\\) of the current state \\(s\\) and the current action \\(a\\) can be calculated. The Bellman Equation is given by: \\(Q^*(s,a) = E_{s',r}[r + \\gamma max_{a'} Q^*(s',a') | s,a]\\).\nThe Q-Learning algorithm is based on the Bellman Equation and it is used to estimate the optimal Q-value function \\(Q^*(s,a)\\) by iteratively updating the Q-value function \\(Q(s,a)\\) using the Bellman Equation such that:\n\\(Q_{i+1}(s,a) = E[ r + \\gamma max_{a'} Q_i(s',a') | s,a]\\).\n\\(Q_{i+1}(s,a) = Q_i(s,a) + \\alpha (r + \\gamma max_{a'} Q_i(s',a') - Q_i(s,a))\\).\nIn practice, this approach is totally impractical because the action-value function is estimated separately for each state-action pair without generalizing the knowledge across states. Instead, we use a function approximator to estimate the Q-value function: \\(Q^*(s,a) \\approx Q(s,a;\\theta)\\) which introduces Deep Q-Network (DQN).\n\nImplementation of Q-Learning\nYou can see my implementation of Q-Learning in this notebook. In this notebook, I have implemented the Q-Learning algorithm to solve the Desert Navigation problem. The agent is thirsty and it needs to find the water in the 2D grid world. The agent can move in four directions: up, down, left, and right. In this notebook, the agent learns to find the water by taking actions and observing the rewards. The agent learns to find the water by maximizing the cumulative reward.\n\n\n\nFig a. Agent wants to navigate to the bottom right corner of the grid.\n\n\n# defining the state environment\nclass State:\n\n    def __init__(self, grid, agent_pos):\n        self.grid = grid\n        self.agent_pos = agent_pos\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, State)\n            and self.grid == other.grid\n            and self.agent_pos == other.agent_pos\n        )\n\n    def __hash__(self):\n        return hash(str(self.grid) + str(self.agent_pos))\n\n    def __str__(self):\n        return f\"State(grid={self.grid}, agent_pos={self.agent_pos})\"\n# defining the actions and the rewards\ndef act(state, action):\n            \n    p = new_agent_pos(state, action)\n    grid_item = state.grid[p[0]][p[1]]\n    \n    new_grid = deepcopy(state.grid)\n    \n    if grid_item == DESERT:\n        reward = -100\n        is_done = True\n        new_grid[p[0]][p[1]] += AGENT\n        \n    elif grid_item == WATER:\n        reward = 1000\n        is_done = True\n        new_grid[p[0]][p[1]] += AGENT\n        \n    elif grid_item == EMPTY:\n        reward = -1\n        is_done = False\n        old = state.agent_pos\n        new_grid[old[0]][old[1]] = EMPTY\n        new_grid[p[0]][p[1]] = AGENT        \n        \n    elif grid_item == AGENT:\n        reward = -1\n        is_done = False\n        \n    else:\n        raise ValueError(f\"Unknown grid item {grid_item}\")\n    \n    return State(grid=new_grid, agent_pos=p), reward, is_done\n# agent learns using Q-Learning\nfor e in range(N_EPISODES):\n    \n    state = start_state\n    total_reward = 0\n    alpha = alphas[e]\n    \n    for _ in range(MAX_EPISODE_STEPS):\n        action = choose_action(state)\n        next_state, reward, done = act(state, action)\n        total_reward += reward\n        \n        q(state)[action] = q(state, action) + \\\n                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n        state = next_state\n        if done:\n            break\n    print(f\"Episode {e + 1}: total reward -&gt; {total_reward}\")\n\n\nDeep Q-Network (DQN)\nDeep Q-Network (DQN) is a neural network (non-linear function approximator) that is used to estimate the action-value function \\(Q(s,a;\\theta)\\) in Q-Learning with weights given by \\(\\theta\\). The DQN is trained to minimize the loss function given by: \\(L_i(\\theta_i) = E_{s,a \\sim \\rho(.)} [(y_i - Q(s,a;\\theta_i))^2]\\), where \\(y_i = E_{s' \\sim \\epsilon} [r + \\gamma max_{a'} Q(s',a';\\theta_{i-1}) | s,a]\\) is the target value for iteration \\(i\\) and \\(\\rho(s, a)\\) is a probability distribution over states and actions which is also known as the behavior distribution. The parameters from the previous iteration \\(\\theta_{i-1}\\) are held fixed while optimizing the loss function \\(L_i(\\theta_i)\\).\nWhen differentiating the loss function \\(L_i(\\theta_i)\\) with respect to the weights \\(\\theta_i\\), the gradient of the loss function is given by: \\(\\nabla_{\\theta_i} L_i(\\theta_i) = E_{s,a \\sim \\rho(.); s' \\sim \\epsilon} [(r + \\gamma max_{a'} Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_i)) \\nabla_{\\theta_i} Q(s,a;\\theta_i)]\\). The gradient of the loss function is used to update the weights \\(\\theta_i\\) using the gradient descent algorithm.\nThe Deep Q-Network (DQN) algorithm is a model-free: it solves the reinforcement learning task directly using the samples from the environment, without explicitly estimating the transition probability. It is an off-policy algorithm: it learns about the greedy policy \\(a = argmax_{a} Q(s,a;\\theta)\\) while following the behavior distribution \\(\\rho(s,a)\\).\n\n\n\nFig b. Deep Q-Network algorithm taken from Mnih et al.\n\n\nDeep Q-Networks make a big leap in the field of Reinforcement Learning by showing that Q-Learning with a non-linear function approximation, in particular, deep convolutional neural networks, can achieve outstanding results on a wide range of Atari games.\n\nDouble DQN\nIn standard Q-Learning, and in DQN, the parameters are updated as follows:\n\\(\\theta_{i+1} = \\theta_i + \\alpha (y_i - Q(s_i,a_i;\\theta_i)) \\nabla_{\\theta_i} Q(s_i,a_i;\\theta_i)\\)\nwhere \\(y_i = r + \\gamma max_{a'} Q(s_{i+1}',a_i';\\theta_{i-1})\\).\nThe problem with this approach is that the Q-values are overestimated because the same values are used to select and evaluate an action. So, the Double DQN algorithm proposes to evaluate the greedy policy according to the online network, but to use the target network to estimate its value. This can be achieved by:\n\\(\\theta_{i+1} = \\theta_i + \\alpha (y_i - Q(s_i,a_i;\\theta_i)) \\nabla_{\\theta_i} Q(s_i,a_i;\\theta_i)\\)\nwhere \\(y_i = r_{i+1} + \\gamma Q(s_{i+1}',argmax_{a'} Q(s_{i+1}',a';\\theta_i);\\theta_{i-1})\\).\n\n\nDueling DQN Architecture\nDueling DQN architecture propse the dueling network architecture to estimate the state value function \\(V(s)\\) and the associated advantage function \\(A(s,a)\\) and then combine them to estimate the action-value function \\(Q(s,a)\\) to converge faster than Q-Learning. In DQN, a CNN Layer is followed by a fully connected layer but in Dueling DQN, a CNN Layer is followed by two streams of fully connected layers: one for the state value function \\(V(s)\\) and the other for the advantage function \\(A(s,a)\\). The output of the two streams is combined to estimate the action-value function \\(Q(s,a)\\)."
  },
  {
    "objectID": "posts/RL/intro_to_rl.html#conclusion",
    "href": "posts/RL/intro_to_rl.html#conclusion",
    "title": "Introduction to Reinforcement Learning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, I have introduced the basic concepts of Reinforcement Learning. I have discussed the concepts of State, Action, Reward, Transition Probability, Discounted Reward, Trajectory, Policy, Value Function, Q-Value Function, Q-Learning, Deep Q-Network (DQN), Double DQN, and Dueling DQN."
  },
  {
    "objectID": "posts/RL/intro_to_rl.html#references",
    "href": "posts/RL/intro_to_rl.html#references",
    "title": "Introduction to Reinforcement Learning",
    "section": "References",
    "text": "References\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. ArXiv. /abs/1312.5602\nLi, Y. (2018). Deep Reinforcement Learning. ArXiv. /abs/1810.06339\nReinforcement Learning from Human Feedback explained with math derivations and the PyTorch code."
  }
]