<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.547">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thinam Tamang">
<meta name="dcterms.date" content="2024-03-09">

<title>Welcome to my blog! - Mixture of Experts in Mistral</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Welcome to my blog!</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Thinam Tamang</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/thinam-tamang/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ThinamXx"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linktr.ee/Thinam"> <i class="bi bi-gear-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/thinam_"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Mixture of Experts in Mistral</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Mixture of Experts</div>
                <div class="quarto-category">Mistral</div>
                <div class="quarto-category">LLMs</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Thinam Tamang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 9, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="sliding-window-attention" class="level2">
<h2 class="anchored" data-anchor-id="sliding-window-attention"><strong>Sliding Window Attention</strong></h2>
<p>Attention mechanism is a key component in Transformer models. It allows the model to focus on different parts of the input sequence and derive the relationship between tokens of the input sequence. In other words, Attention refers to the mechanism of sharing the information between tokens in the input sequence. For the computation of attention in Transormer Models:</p>
<ul>
<li>We multiply the <em>query matrix</em> with the transpose of the <em>key matrix</em> to get the attention scores. The attention scores are then fed to the <em>softmax function</em> to get the attention weights. These attention weights are then multiplied with the <em>value matrix</em> to get the context vector or attention output.</li>
</ul>
<p>In vanilla Transformer models, the attention mechanism uses <em>causal mask</em> which means that each token in the input sequence can only attend to itself and all the tokens before it. This approach ensures that the model is causal and it can only use the information from the past tokens to predict the future tokens. However, the number of operations in the attention mechanism is quadratic with respect to the length of the input sequence and the memory requirement is also linear with respect to the length of the input sequence which incurs higher latency and smaller throughput from the model at inference time.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img src="VT.png" alt="" width="300">
<p style="text-align: center;">
Fig a. Vanilla Attention with causal mask taken from Mistral paper.
</p>
</div>
<p>So, to address these limitations, <strong>Sliding Window Attention</strong> mechanism was used in the Mistral model. The Sliding Window Attention is a variant of the attention mechanism which uses a fixed window size to select the tokens from the past to attend for computation of attention output. In other words, each token in the input sequence can only attend at most <strong>W</strong> tokens from the past where <strong>W</strong> is the window size. This approach increases the inference speed and reduces the memory usage of the model. This mechanism still ensures that the model is causal but it does not use the entire tokens from the past, whereas it uses a fixed number of tokens from the past to compute the attention output.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img src="SW.png" alt="" width="300">
<p style="text-align: center;">
Fig b. Sliding Window Attention with window size 3.
</p>
</div>
<p>In the <strong>Sliding Window Attention</strong>, the tokens outside the sliding window still influence the next word prediction because at each attention layer, the information can move forward by <strong>W</strong> tokens at most, but after the next layer, the information can move forward by <strong>2W</strong> tokens and so on, as the hidden state in position <strong>i</strong> of the layer <strong>k</strong>, attends to all the hidden states from position <strong>i-W</strong> to <strong>i</strong> of the layer <strong>k-1</strong> since the layers are stacked on top of each other in Transformer models.</p>
<p>Suppose, we have a input sequence with 10 tokens, a Transformer model with 4 layers and using a window size of 4,then the information flow from one layer to another using Sliding Window Attention is given below:</p>
<ul>
<li>In first layer, we use the first 4 tokens to compute the attention output for the next token.</li>
<li>In second layer, we use the information from the previous 4 tokens and the next 3 tokens to compute the attention output for the next token.</li>
<li>In third layer, we use the information from the previous tokens and the next 3 tokens compute the attention output for the next token.</li>
<li>In fourth layer, we use the information of the entire input sequence to compute the attention output for the next token. At this point, the information has propagated from the first token to the last token in the input sequence.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img src="RF.png" alt="" width="300">
<p style="text-align: center;">
Fig c.&nbsp;Information flow from one layer to another using Sliding Window Attention.
</p>
</div>
</section>
<section id="rotating-buffer-cache" class="level2">
<h2 class="anchored" data-anchor-id="rotating-buffer-cache"><strong>Rotating Buffer Cache</strong></h2>
<p><strong>Rotating Buffer Cache</strong> is a mechanism used in the Mistral model which limits the size of <em>KV</em> cache to a fixed size. In my blog post on <a href="https://thinamxx.github.io/blog/posts/KV/kv.html">Understanding KV Cache</a>, I have explained the needs and limitations of KV cache along with implementation. In the paper of Mistral, they mentioned that: when we use a sequence length of <strong>32k</strong> tokens, the Rotating Buffer Cache reduces the cache memory usage by <strong>8x</strong>, without impacting the model quality. While using KV cache, the memory usage of GPU or CPU increases linearly with respect to the length of the input sequence because we need to save KV cache for each layers of the model.</p>
<section id="rotating-buffer-cache-for-sliding-window-attention" class="level3">
<h3 class="anchored" data-anchor-id="rotating-buffer-cache-for-sliding-window-attention"><strong>Rotating Buffer Cache for Sliding Window Attention</strong></h3>
<p>When we use <strong>Sliding Window Attention</strong> mechanism, we have the fixed attention span of <strong>W</strong> tokens which means that we can limit the size of the <em>KV</em> cache using the <strong>Rotating Buffer Cache</strong> also called <strong>Rolling Buffer Cache</strong>. The cache has a fixed size of <strong>W</strong> where <strong>W</strong> is the window size, and the Keys and Values for the timestep <strong>i</strong> are stored in the positions <strong>i mod W</strong> of the cache. Similarly, when the position <strong>i</strong> is larger than <strong>W</strong>, the previous Keys and Values are overwritten by the new Keys and Values in the cache. This approach keeps the cache size fixed and reduces the linear memory usage of the model with respect to the length of the input sequence.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img src="RBC.png" alt="" width="900">
<p style="text-align: center;">
Fig d.&nbsp;Rotating Buffer Cache with cache size W=4
</p>
</div>
<p>In the above figure, the cache size is <strong>W=4</strong> and we have 3 input sequences. The Keys and Values are stored by following operations:</p>
<ul>
<li>We have 3 input sequences:
<ul>
<li>“This is an example of …”</li>
<li>“Mistral is a good …”</li>
<li>“The cat sat on the mat …”</li>
</ul></li>
<li>For the first input sequence, at timestep <strong>i</strong>, “an” is stored in the cache position <strong>2 mod 4 = 2</strong>. At timestep <strong>i+1</strong>, “example” is stored in the cache position <strong>3 mod 4 = 3</strong>. At timestep <strong>i+2</strong>, “of” is stored in the cache position <strong>4 mod 4 = 0</strong>.</li>
<li>For the second input sequence, at timestep <strong>i</strong>, “is” is stored in the cache position <strong>1 mod 4 = 1</strong>. At timestep <strong>i+1</strong>, “a” is stored in the cache position <strong>2 mod 4 = 2</strong>. At timestep <strong>i+2</strong>, “good” is stored in the cache position <strong>3 mod 4 = 3</strong>.</li>
<li>For the third input sequence, at timestep <strong>i</strong>, “on” is stored in the cache position <strong>3 mod 4 = 3</strong>. At timestep <strong>i+1</strong>, “the” is stored in the cache position <strong>4 mod 4 = 0</strong>. At timestep <strong>i+2</strong>, “mat” is stored in the cache position <strong>1 mod 4 = 1</strong>.</li>
</ul>
</section>
</section>
<section id="pre-filling-and-chunking" class="level2">
<h2 class="anchored" data-anchor-id="pre-filling-and-chunking"><strong>Pre-Filling and Chunking</strong></h2>
<p>When we generate the sequence using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. The output of the model when we use self attention mechanism without KV cache is a sequence of tokens whereas the output of the model when we use KV cache is a required final token. Therefore, KV cache makes the inference faster.</p>
<p>At inference, the prompt sequence is known in advance, and we can pre-fill the KV cache with the prompt sequence. In the Sliding Window Attention mechanism, we have the fixed size of the cache and if the prompt sequence is larger than the cache size, we can chunk the prompt sequence into smaller sequences and pre-fill the cache with these smaller sequences. In this approach, we use the chunk size of <strong>W</strong> where <strong>W</strong> is the window size.</p>
<p>Suppose, we have a prompt sequence “The cat sat on the mat and saw the dog go to” and the window size is <strong>W=4</strong>, then we can chunk the prompt sequence into smaller sequences as follows: - “The cat sat on” - “the mat and saw” - “the dog go to”</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img src="PF.png" alt="" width="800">
<p style="text-align: center;">
Fig e. Pre-fill and Chunking representation taken from Mistral
</p>
</div>
<p>In the above figure, the prompt sequence is chunked into smaller sequences and the cache is pre-filled with these sequences. For the third chunk “the dog go to”: it attends itself using the causal mask which is the rightmost block, and it attends the previous block using the sliding window attention mechanism, and it does not attend any past tokens that is outside the window size of sliding window attention.</p>
<p>Therefore, pre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences.</p>
<section id="blockdiagonalcausalmask-and-blockdiagonalmask-during-pre-filling" class="level3">
<h3 class="anchored" data-anchor-id="blockdiagonalcausalmask-and-blockdiagonalmask-during-pre-filling"><strong>BlockDiagonalCausalMask and BlockDiagonalMask during Pre-Filling</strong></h3>
<p>We use the <strong>Pre-Filling</strong> and <strong>Chunking</strong> mechanism to pre-fill the KV cache with the prompt sequence. Both the <strong>BlockDiagonalCausalMask</strong> and <strong>BlockDiagonalMask</strong> are used to pre-fill the cache and <a href="https://github.com/ThinamXx/mistral-src/blob/main/mistral/cache.py#L237C9-L260C14">generate the mask</a> of each chunk of the prompt sequence. The generated mask is then used to compute the <a href="https://github.com/ThinamXx/mistral-src/blob/main/mistral/model.py#L114C9-L117C10">attention</a> by the Attention Layer in Mistral model. The generation of the mask is done by the following operations:</p>
<ul>
<li>Let’s say we have a prompt sequence “The cat sat on” and we want to generate the 4 new tokens. The cache size will be <strong>max_tokens + prompt_length + 1 = 4 + 4 + 1 = 9</strong>.</li>
<li>Suppose, we consider the chunk size of 2, then the prompt sequence is divided into 3 chunks because while encoding the prompt sequence, we need to add the start token at index 0 during the inference, so the prompt sequence is divided into 3 chunks: “[start token] The,”cat sat”, “on”. You can find the implementation of chunking in <a href="https://github.com/ThinamXx/mistral-src/blob/main/main.py#L76C5-L80C43">here</a>.</li>
<li>So, the prefill or mask for the first chunk is generated by <strong>BlockDiagonalCausalMask</strong> and the prefill or mask for the second (or subsequent chunks if any) is generated by <strong>BlockDiagonalMask</strong>. In our example above, we have the last chunk that doesn’t contain the complete tokens as the chunk size has only one token, and in such cases, the prefill or mask is generated by <strong>BlockDiagonalCausalWithOffsetPaddedKeysMask</strong>.</li>
</ul>
<p>So, the <strong>BlockDiagonalCausalMask</strong> is used to pre-fill the cache with the first chunk of the prompt sequence and the <strong>BlockDiagonalMask</strong> is used to pre-fill the cache with the subsequent chunks of the prompt sequence. You can find the implementation of these masks in the <a href="https://github.com/ThinamXx/mistral-src/blob/main/mistral/cache.py#L237C9-L260C14">cache.py</a>.</p>
</section>
</section>
<section id="blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation" class="level2">
<h2 class="anchored" data-anchor-id="blockdiagonalcausalwithoffsetpaddedkeysmask-for-token-generation"><strong>BlockDiagonalCausalWithOffsetPaddedKeysMask for Token Generation</strong></h2>
<p>When we generate the tokens using the model at inference time, we need to generate the tokens one by one as each token is dependent on the previous tokens. In Mistral, the <strong>BlockDiagonalCausalWithOffsetPaddedKeysMask</strong> is used to generate the mask when we feed a single token to the model to generate the next token. Therefore, the <strong>BlockDiagonalCausalWithOffsetPaddedKeysMask</strong> is used to generate the mask for the token generation. You can find the implementation of generating a new token in <a href="https://github.com/ThinamXx/mistral-src/blob/main/main.py#L132C9-L135C52">here</a> which then uses the <strong>BlockDiagonalCausalWithOffsetPaddedKeysMask</strong> to generate the mask for the generation of the next token.</p>
<section id="how-multiple-prompts-are-handled-in-mistral" class="level3">
<h3 class="anchored" data-anchor-id="how-multiple-prompts-are-handled-in-mistral"><strong>How multiple prompts are handled in Mistral?</strong></h3>
<p>In Mistral model, multiple prompts are packed into a single tensor during pre-filling phase at inference time and the corresponding mask is generated using the <strong>BlockDiagonalCausalMask</strong>, <strong>BlockDiagonalMask</strong> and <strong>BlockDiagonalCausalWithOffsetPaddedKeysMask</strong>. The packed tensor is then send to the model to pre-fill the cache and generate the mask for the token generation. You can find the implementation of packing the prompts into a single tensor and generating the mask in <a href="https://github.com/ThinamXx/mistral-src/blob/main/main.py#L83C5-L91C10">here</a>.</p>
</section>
</section>
<section id="kv-cache" class="level2">
<h2 class="anchored" data-anchor-id="kv-cache"><strong>KV Cache</strong></h2>
<p>KV cache, short for Key &amp; Value cache, is a technique used to accelerate the inference process in Large Language Models (LLMs), particularly in autoregressive models i.e.&nbsp;the current token depends on the previous tokens in a sequence. In the KV cache, the output of the model from previous time step is appended to the cache of key and value matrices of the current time step but the query matrix is updated at each time step to generate the next token. This way of caching the previous keys and values ensures that the model does not repeat the computations at each time step. This significantly reduces the size of the matrices used in the computation which makes the inference process faster (matrix multiplication faster). I have explained KV cache in detail along with its implementation in my blog post on <a href="https://thinamxx.github.io/blog/posts/KV/kv.html">Understanding KV Cache</a>.</p>
<section id="kv-cache-in-pre-filling-and-chunking" class="level3">
<h3 class="anchored" data-anchor-id="kv-cache-in-pre-filling-and-chunking"><strong>KV Cache in Pre-Filling and Chunking</strong></h3>
<p>Pre-filling is the process of filling the KV cache with the prompt sequence and chunking is the process of dividing the long prompt sequence into smaller sequences and pre-filling the cache with these smaller sequences as explained above. Since, we divide the prompt sequence into smaller sequences or chunks to pre-fill the cache, the dimensions of the KV cache changes at each iteration of the pre-filling process. The complete operation of pre-filling is done by the following operations:</p>
<ul>
<li>Let’s say we have a <a href="https://github.com/ThinamXx/mistral-src/blob/main/main.py#L185C5-L194C33">prompt sequence</a> “This is another great test” and after encoding we will have encoded prompt sequence of length 6 because we need to add the start token at index 0 during the inference.</li>
<li>Suppose, we consider the <a href="https://github.com/ThinamXx/mistral-src/blob/main/main.py#L77C5-L80C43">chunk size</a> of 2, then the prompt sequence is divided into 3 chunks: “[start token] This”, “is another”, “great test”.</li>
<li>For the first chunk, the KV cache sequence length is 0, and since the chunk size is 2, the KV cache dimension is <code>[2, 8, 128]</code> where 2 is sequence length, 8 is the number of KV heads, and 128 is the head dimension.</li>
<li>For the second chunk, the KV cache sequence length is 2, and since the chunk size is 2, the KV cache dimension now becomes <code>[4, 8, 128]</code> where 4 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.</li>
<li>For the third and last chunk, the KV cache sequence length is 4, and since the chunk size is 2, the KV cache dimension now becomes <code>[6, 8, 128]</code> where 6 is the sequence length, 8 is the number of KV heads, and 128 is the head dimension.</li>
</ul>
<p>So, the KV cache is pre-filled with the prompt sequence and the dimensions of the KV cache changes at each iteration of the pre-filling process. You can check the dimensions of the KV cache at each iteration of the pre-filling process in the <a href="https://github.com/ThinamXx/mistral-src/blob/main/mistral/model.py#L96C9-L99C57">model.py</a>.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion"><strong>Conclusion</strong></h2>
<p>In this blog post, I have explained the Sliding Window Attention mechanism, Rotating Buffer Cache, Pre-Filling and Chunking, BlockDiagonalCausalMask, BlockDiagonalMask, BlockDiagonalCausalWithOffsetPaddedKeysMask, and KV Cache in Mistral model. These techniques are used to accelerate the inference process as well as fine-tuning process in Large Language Models (LLMs). We prepare this blog post based on the source code of Mistral model and the paper of Mistral. You can learn more novel techniques used in Mistral model by reading my previous blog post on <a href="https://thinamxx.github.io/blog/posts/MS/mistral.html">Model Sharding</a> and <a href="https://thinamxx.github.io/blog/posts/MOE/mistral.html">Mixture of Experts</a>.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references"><strong>References</strong></h2>
<ul>
<li><a href="https://github.com/ThinamXx/mistral-src">Mistral Source Code</a></li>
<li><a href="https://github.com/ThinamXx/Meta-llama/blob/main/llama/llama2.py">Llama2 Implementation</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>