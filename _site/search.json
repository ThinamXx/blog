[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Thinam Tamang",
    "section": "",
    "text": "I am a data science, machine Learning, and deep learning practitioner and learner. Solving a problem and gaining insights with the help of machine learning algorithms have always seemed to be superpowers for me. I am here to solve things, Learning a lot in the process. I’m currently working as a Machine Learning Scientist at Speechify.\n\nPublication:\n\nClassification of White Blood Cells: A Comprehensive Study Using Transfer Learning Based on Convolutional Neural Networks\nToward insights on antimicrobial selectivity of host defense peptides via machine learning model interpretation\n\n\n\nExperience:\n\nSpeechify | Machine Learning Engineer | Dec 2022 - Present\nRippeyAI | NLP Engineer | May 2022 - Dec 2022\nGenese Solution | Data Scientist | Feb 2022 - May 2022\nCenter of Data Mining and Biomedical Informatics | Research Assistant | Nov 2020 - Feb 2022\nKharpann Enterprises | NLP Engineer | Apr 2021 - Aug 2021"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "Git & GitHub\n\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\nversion control\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Attention & Transformer\n\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nWord Vectors\n\n\n\n\n\n\n\nmachine learning\n\n\nword vectors\n\n\nword embeddings\n\n\ntransformers\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nData Engineering Fundamentals\n\n\n\n\n\n\n\nmachine learning\n\n\ndata engineering\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nData Fundamentals\n\n\n\n\n\n\n\nmachine learning\n\n\ndata preparation\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nPattern Recognition & ML\n\n\n\n\n\n\n\nmachine learning\n\n\npattern recognition\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks Architectures\n\n\n\n\n\n\n\nconvolutional neural networks\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2022\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nFundamentals of CNNs\n\n\n\n\n\n\n\nmachine learning\n\n\nconvolutional neural network\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nFundamentals of Image Classification\n\n\n\n\n\n\n\ncomputer vision\n\n\nimage classification\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\n  \n\n\n\n\nJourney of 66DaysOfData in Natural Language Processing\n\n\n\n\n\n\n\nnatural language processing\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2021\n\n\nThinam Tamang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Attention/Transformer.html#recurrent-models",
    "href": "posts/Attention/Transformer.html#recurrent-models",
    "title": "Self-Attention & Transformer",
    "section": "Recurrent Models",
    "text": "Recurrent Models\n\nThe de facto strategy in Natural Language Processing (NLP) is to encode sentences with a bidirectional LSTM models. For example: Source sentence in a translation.\nWe should define our output (parse, sentence, summary) as a sequence, and use an LSTm model to generate it.\nUse of attention mechanism allows flexible access to the memory. We use attention mechanism to take a representation from our decoder and look back to treat the encoded representation as a memory, that we can reference and pick out what’s important to any given time.\n\n\nIssues with Recurrent Models\n\nLinear interaction distance.\n\nRecurrent Neural Networks (RNN) are unrolled left-to-right.\nRNNs encode linear locality which means the nearby words often affect each other’s meaning in the sentence.\nRNNs take O(sequence length) steps for distant words pairs to interact which means that it is hard to learn long-distance dependencies because of the gradient problems.\nLinear order of words is sort of baked into the model because we have to unroll the RNN throughout the sequence and linear order isn’t the right way to think about sentences.\n\nLack of parallelizability.\n\nForward and backward passes have O(sequence length) unparallelizable operations.\nThough GPUs can perform a bunch of independent computations at once, a future RNN hidden states can’t be computed in full before past RNN hidden states have been computed.\n\nSo, RNNs inhibits training on very large dataset."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-windows-model",
    "href": "posts/Attention/Transformer.html#word-windows-model",
    "title": "Self-Attention & Transformer",
    "section": "Word Windows Model",
    "text": "Word Windows Model\n\nWord window models aggregate local context. Number of unparallelizable operations doesn’t increase sequence length.\nStacking word window layers allows interaction between farther words.\nMaximum interaction distance = sequence length / window size. But if the sentences are too long, we will just ignore the long-distance context."
  },
  {
    "objectID": "posts/Attention/Transformer.html#attention-model",
    "href": "posts/Attention/Transformer.html#attention-model",
    "title": "Self-Attention & Transformer",
    "section": "Attention Model",
    "text": "Attention Model\n\nAttention model treats each word’s representation as a query to access and incorporate information for a set of values. Example: In a machine translation system, the set of values were all of the encoder states for the source sentences.\nNumber of unparallelizable operations doesn’t increase sequence length.\nMaximum interaction distance is O(1) since all the wrods interact at every layer.\n\n\nSelf-Attention Model\n\nAttention model operates on queries, keys, and values.\nIn self-attention models, the queries, keys, and values are drawn from the same source sentences.\nSince, self-attention mechanism doesn’t build in order information, we need to encode the order of the sentences in our keys, queries, and values. We will consider representing each sequence index as a vector and add it to our inputs in *self-attention** block.\nThe position representation vectors are represented through sinusoids. Sinusoidal position representations concatenate functions of varying periods. Learned absolute position representations are flexible to be learned to fit the data on each position.\n\n\n\nBarriers & Solutions for Self-Attention Model\n\n\n\n\n\n\n\nBarriers\nSolution\n\n\n\n\n1. Doesn’t have an inherent notion of order.\n1. Add position representations to the inputs.\n\n\n2. No nolinearities to produce the deep learning magic. But it’s all just the weighted averages.\n2. Apply the same feedforward networks to each self-attention output.\n\n\n3. Need to ensure that we don’t look at the future outputs when predicting a sequence. Like in machine translation or language modeling.\n3. Mask out the future by artificially setting the attention weights to 0.\n\n\n\nThe necessities for a self-attention model are as follows:\n\nSelf-attention:\n\nThe basis of the method or implementation process.\n\nPosition representations:\n\nSpecify the sequence order, since self-attention is an unordered function of its inputs.\n\nNonlinearities:\n\nAt the output of the self-attention block.\nFrequently implemented as a simple feedforward network.\n\nMasking:\n\nIn order to parallelize operations while not looking at the future.\nKeeps information about the future from leaking to the past."
  },
  {
    "objectID": "posts/Attention/Transformer.html#the-transformer",
    "href": "posts/Attention/Transformer.html#the-transformer",
    "title": "Self-Attention & Transformer",
    "section": "The Transformer",
    "text": "The Transformer\n\nWe take the dot product of the query-key in one matrix multiplication.\nThen we apply softmax and compute the weighted average with another matrix multiplication.\nWe define multiple attention heads through multiple query, key, and value matrices.\nResidual connections are thought to make the loss landscape considerably smoother and thus enhances easier training.\nLayer normalization is a trick to help models train faster. It cuts down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer.\nScaled Dot Product attention is necessary when dimensionality d becomes large, dot products between vectors tend to become large and because of this, inputs to the softmax function can be large, making the gradients small.\nTransformers parallelizability allows for efficient pretraining, and have made them the de facto standard."
  },
  {
    "objectID": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "href": "posts/Attention/Transformer.html#word-structure-and-subword-models",
    "title": "Self-Attention & Transformer",
    "section": "Word Structure and Subword Models",
    "text": "Word Structure and Subword Models\n\nIn a language’s vocabulary, we assume that a fixed vocabulary of tens of thousands of words are built from the training dataset. All other novel words which are seen only at test time are mapped to a single unknown token UNK.\nFinite vocabulary assumptions in not an ideal solution in many languages. Many languages exhibit complex morphology or word structure which is more word types, each occurring in fewer times."
  },
  {
    "objectID": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "href": "posts/Attention/Transformer.html#byte-pair-encoding-algorithm",
    "title": "Self-Attention & Transformer",
    "section": "Byte-Pair Encoding Algorithm",
    "text": "Byte-Pair Encoding Algorithm\nSubword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level. The dominant modern paradigm is to learn a vocabulary of parts of words also known as subword tokens. At the training and testing time, each word is split into a sequence of known subwords.\nByte-pair encoding is a simple, and effective startegy for defining a subword vocabulary: - Start with a vocabulary containing only characters and an end-of-word symbol. - Using a corpus of text, find the most common adjacent characters known as subwords. - Replace instances of the character pair with the new subword and iterate until the desired vocab size is met.\nThis technique was originally used in NLP for machine translation, and now a similar method WordPiece is used in pretrained models.\n\nPretrained Word Embeddings & Models\n\nAlmost all parameters in NLP networks are initialized via pretraining which is similar to initializing the Word2Vec parameters.\nThe pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts.\nThis has been exceptionally effective at building strong:\n\nrepresentations of language.\nparameter initializations for strong NLP models.\nprobability distributions over language that we can sample from.\n\n\n\n\nGenerative Pretrained Transformer (GPT)\n\nGPT is a decoder only Transformer model with 12 layers.\nGPT contains 768 dimensional hidden states, and 3072 dimensional feed-forward hidden layers.\nA subword vocabulary called Byte-Pair encoding with 40,000 merges.\nGPT models are trained on book corpus and contains over 7000 unique books which contains long spans of contiguous text, for learning long-distance dependencies.\n\n\n\nBidirectional Encoder Representations from Transformers (BERT)\nDevlin et al., 2018 proposed the Masked LM objective and released the weights of a pretrained Transformer and labeled BERT.\nSome of the details about Masked Language Model for BERT are: - Predict a random 15% of subword tokens. - Replace input word with [MASK] 80% of the time. - Replace input word with a random vocabulary token 10% of the time. - Leave input word unchanged 10% of the time but still predict.\nSome of the details about BERT: - Two models were released: - BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params. - BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params. - Trained on: - Books Corpus (800 million words) - English Wikipedia (2500 million words) - Pretraining is expensive and impractical on a single GPU: - BERT was pretrained with 64 TPU chips for a total of 4 days. - TPU are special tensor operations acceleration hardware. - Finetuning is practical and common on a single GPU."
  },
  {
    "objectID": "posts/ClassifyImage/Fundamentals.html",
    "href": "posts/ClassifyImage/Fundamentals.html",
    "title": "Fundamentals of Image Classification",
    "section": "",
    "text": "Image Classification\n1. Image Classification is the task of using computer vision and machine learning algorithms to extract meaning from an image. It is the task of assigning a label to an image from a predefined set of categories.\n2. Semantic Gap is the difference between how a human perceives the contents of an image versus how an image can be represented in a way a computer can understand the process.\n3. Feature Extraction is the process of taking an input image, applying an algorithm, and obtaining a feature vector that quantifies the image.\n4. The common supervised learning algorithms include Logistic Regression, Support Vector Machines, Random Forests, and Artificial Neural Networks.\n5. Unsupervised Learning which is also called self-taught learning has no labels associated with the input data and thus we cannot correct our model if it makes an incorrect prediction.\n6. K-Nearest Neighbor classifier doesn’t actually learn anything, but it directly relies on the distance between feature vectors.\n7. A learning model that summarizes data with a set of parameters of fixed size which is independent of the number of training examples is called a Parametric Model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs. – Russell and Norvig.\n8. Parameterization is the process of defining the necessary parameters of a given model. In machine learning, parameterization involves in four key components: data, a scoring function, a loss function, and weights and biases.\n9. The scoring function accepts the data as an input and maps the data to class labels. A loss function quantifies how well our predicted class labels agree with our ground-truth labels.\n10. Softmax Classifiers give probabilities for each class label while hinge loss gives the margin scores.\n11. The gradient descent method is an iterative optimization algorithm that operates over a loss landscape also called and optimization surface. Also, gradient descent refers to the process of attempting to optimize the parameters for low loss and high classification accuracy via an iterative process of taking a step in the direction that minimize loss.\n\nFigure: The naive loss visualized as a 2D plot.\nAs shown in figure, the loss landscape has many peaks and valleys. Each peak is a local maximum that represents very high regions of loss. The local maximum with largest loss across the entire loss landscape is the global maximum. Similarly, the local minimum represents many small regions of loss. The local minimum with the smallest loss across the loss landscape is the global minimum.\n12. An optimization algorithm may not be guaranteed to arrive at even a local minimum in a reasonable amount of time, but it often finds a very low value of the loss function quickly enough to be useful. – Goodfellow.\n13. Stochastic Gradient Descent (SGD) is a simple modification to the standard gradient descent algorithm that computes the gradient and updates the weight matrix on small batches of training data, rather than the entire training set.\n14. Momentum is a method used to accelerate Stochastic Gradient Descent (SGD), enabling it to learn faster by focusing on dimensions whose gradient point in the same direction. Nesterov’s Acceleration can be conceptualized as a corrective update to the momentum which lets us obtain an approximate idea of where our parameters will be after the update.\n13. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are collectively known as Regularization. – Goodfellow.\n\nFigure: An example of model fitting.\nThe figure presents an example of underfitting (orange line), overfitting (blue line), and generalizing (green line). The goal of deep learning classifiers is to obtain these types of “green functions” that fit the training data nicely, but avoid overfitting. Regularization helps to obtain the desired fit."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#convolutional-neural-networks",
    "href": "posts/CNNs/CNNs.html#convolutional-neural-networks",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nThe five architectures of CNNs that have been pre-trained on the ImageNet dataset and, are present in the Keras library are mentioned below:\n\nVGG16\nVGG19\nResNet50\nInception V3\nXception"
  },
  {
    "objectID": "posts/CNNs/CNNs.html#parameterized-learning",
    "href": "posts/CNNs/CNNs.html#parameterized-learning",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Parameterized Learning",
    "text": "Parameterized Learning\nThe point of parameterized learning is to define a machine learning model that can learn patterns from our input data during training time, but have the testing process be must faster and to obtain a model that can be defined using a small number of parameters that can easily represent the network regardless of training size."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#vgg16-vgg19",
    "href": "posts/CNNs/CNNs.html#vgg16-vgg19",
    "title": "Convolutional Neural Networks Architectures",
    "section": "VGG16 & VGG19",
    "text": "VGG16 & VGG19\n\nFig: A visualization of the VGG architecture.\nThe figure presents the visualization of the VGG architecture. Images with 224 x 224 x3 dimensions are inputted to the network. Convolutions filters of only 3 x 3 are then applied with more convolutions stacked on top of each other prior to max pooling operations deeper in the architecture.\nThe VGG network architecture was introduced by Simonyan and Zisserman in their 2014 paper, Very Deep Convolutional Networks for Large Scale Image Recognition. The VGG family of networks is characterized by using only 3 x 3 convolutional layers stacked on top of each other in increasing depth. The volume size is reduced by max pooling. Two fully-connected layers are then followed by a softmax classifier."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#resnet",
    "href": "posts/CNNs/CNNs.html#resnet",
    "title": "Convolutional Neural Networks Architectures",
    "section": "ResNet",
    "text": "ResNet\n\nFig: Left: The original residual module. Right: The updated residual module using pre-activation.\nThe ResNet module was introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition. The ResNet architecture has become a seminal work in the deep learning literature, demonstrating that extremely deep networks can be trained using standard SGD through the use of residual modules. Accuracy can be obtained by updating the residual module to use identity mappings."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#inception-v3",
    "href": "posts/CNNs/CNNs.html#inception-v3",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Inception V3",
    "text": "Inception V3\n\nFig: The original Inception module used in GoogLeNet.\nThe Inception module was introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions. The goal of the Inception module is to act as multi-level feature extractor by computing 1 x 1, 3 x 3, and 5 x 5 convolutions within the same module of the network. The output of these filters are then stacked along the channel dimension before being fed into the next layer in the network. The original incarnation of this architecture was called GoogLeNet."
  },
  {
    "objectID": "posts/CNNs/CNNs.html#xception",
    "href": "posts/CNNs/CNNs.html#xception",
    "title": "Convolutional Neural Networks Architectures",
    "section": "Xception",
    "text": "Xception\nXception module was introduced by Francois Chollet in their 2016 paper, Xception: Deep Learning with Depthwise Separable Convolutions. Xception is an extension to the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-sources",
    "href": "posts/DataEng/DataEngineering.html#data-sources",
    "title": "Data Engineering Fundamentals",
    "section": "Data Sources",
    "text": "Data Sources\nUser input data can be text, images, videos, uploaded files, etc. It requires more heavy-duty checking and processing. User input data tends to require fast processing as well.\nSystem-generated data is the data generated by different components of your systems which includes various types of logs and system output such as model predictions. The logs of data provide visibility into system performance which can be used for debugging and potentially improving the application."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-formats",
    "href": "posts/DataEng/DataEngineering.html#data-formats",
    "title": "Data Engineering Fundamentals",
    "section": "Data Formats",
    "text": "Data Formats\nThe process of converting a data structure or object state into a format that can be stored or transmitted and reconstructed later is data serialization.\nJSON\nJSON, JavaScript Object Notation was derived from JavaScript but it’s language-independent. It’s key-value pair paradigm is simple but powerful, capable of handling data of different levels of structuredness.\nRow-Major Versus Column-Major Format\nCSV (Comma-separated values) is row-major, which means consecutive elements in a row are stored next to each other in memory. Parquet is column-major, which means consecutive elements in a column are sotred next to each other. Row-major formats are better when you have to do a lot of writes, whereas column-major ones are better when you have to do a lot of column-based reads.\nText Versus Binary Format\nCSV and JSON are text files, whereas Parquet files are binary files. Text files are files that are in plain text, which means they are human-readable. Binary files are nontext files and contains onlys 0s and 1s. Binary Files are more compact.\nAWS recommends using the Parquet format because the Parquet format is up to 2x faster to unload and consumes up to 6x less storage in Amazon s3, compared to text formats."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#data-models",
    "href": "posts/DataEng/DataEngineering.html#data-models",
    "title": "Data Engineering Fundamentals",
    "section": "Data Models",
    "text": "Data Models\nData models describe how data is represented. The attributes of the entities present in the model make up a data model.\nRelational Model\nRelational model was invented by Edgar F. Codd in 1970. In this model, data is organized into relations; each relation is a set of tuples. A table is an accepted visual representation of the relation, and each row of a table makes up a tuple.\nDocument Model\nThe document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare. The document model is built from around the concepts of “document” which is often a single continuous string encoded as JSON, XML, or a binary format like BSON (Binary JSON).\nGraph Model\nThe graph model goes in the opposite direction of targeting use cases where relationships between data items are common and important. The graph model is built around the concepts of “graph” which consists of nodes and edges, where the edges represent the relationships between the nodes. A database that uses graph structures to store its data is caled a graph database.\nStructured Versus Unstructured Data\nStructured data follows a predefined data model, also known as a data schema which makes your data easier to analyze. Unstructured data doesn’t adhere to a predefined data schema. Even though unstructured data doesn’t adhere to a schema, it might still contain intrinsic patterns that help you extract structures.\nA repository for storing structured data is called a data warehouse. A repository for storing unstructured data is called a data lake. Data lakes are usually used to store raw data before processing. Data warehouses are used to store data that has been processed into formats ready to be used."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#etl-extract-transform-and-load",
    "href": "posts/DataEng/DataEngineering.html#etl-extract-transform-and-load",
    "title": "Data Engineering Fundamentals",
    "section": "ETL: Extract, Transform, and Load",
    "text": "ETL: Extract, Transform, and Load\nWhen data is extracted from different sources, it’s first transformed into the desired format before being loaded into the target destination such as a database or a data warehouse. This process is called ETL, which stands for Extract, Transform, and Load.\nExtract is extracting the data from all data sources. In extraction phase, we need to validate the data and reject the data that doesn’t meet our requirements. Transform is the meaty part of the process, where most of the data processing is done. We can apply operations such as transposing, deduplicating, sorting, aggregating, deriving new features, and more data validation. Load is deciding how and how often to load the transformed data into the target destination, which can be a file, a database, or a data warehouse."
  },
  {
    "objectID": "posts/DataEng/DataEngineering.html#batch-processing-versus-stream-processing",
    "href": "posts/DataEng/DataEngineering.html#batch-processing-versus-stream-processing",
    "title": "Data Engineering Fundamentals",
    "section": "Batch Processing Versus Stream Processing",
    "text": "Batch Processing Versus Stream Processing\nWhen data is processed in batch jobs, we refer to it as batch processing. When we have data in real-time transports like Apache Kafka and Amazon Kinesis, we say that we have streaming data and stream processing refers to doing computation on streaming data.\nStream processing, when done right, can give low latency because we can process data as soon as data is generated, without having to first write it into databases. Stream processing is used to compute features that change quickly and it is more difficult because the data amount is unbounded and the data comes in at variable rates and speeds."
  },
  {
    "objectID": "posts/Dataset/DataPreparation.html",
    "href": "posts/Dataset/DataPreparation.html",
    "title": "Data Fundamentals",
    "section": "",
    "text": "Outliers\nOutliers are examples that look dissimilar to the majority of examples from the dataset. Dissimilarity is measured by some distance metric, such as Euclidean distance. Deleting outliers from the training dataset is not considered scientifically significant, especially in small datasets. In the big data context, outliers don’t typically have a significant impact on the model.\n\n\nData Leakage\nData leakage, also known as target leakage, is a problem affecting several stages of the machine learning life cycle, from data collection to model evaluation. Data leakage in supervised learning is the unintentional introduction of information about the target that shouldn’t be made available.\n\n\nWhat is Good Data\n\nGood data is informative.\nGood data has good coverage.\nGood data reflects real inputs.\nGood data is unbiased.\nGood data is not a result of a feedback loop.\nGood data has consistent labels.\nGood data is big enough to allow generalization.\n\n\n\nData Augmentation\nThe most effective strategy applied to images to get more labeled examples without additional labeling is called data augmentation. The simple operations are flip, rotation, crop, color shift, noise addition, perspective correction, contrast change, and information loss.\nMixup is the popular technique of data augmentation which consists of training the model on a mix of the images from the training set. Instead of training the model on the raw images, we take two images and use for training their linear combination:\nmixup_image = t x image₁ + (1 - t) x image₂\nmixup_target = t x target₁ + (1 - t) x target₂\n\n\nOversampling\nOversampling is a technique to mitigate the class imbalance by making multiple copies of minority class examples. Two popular algorithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\n\n\nUndersampling\nUndersampling is a technique to mitigate the class imbalance by removing some examples from the training set of the majority class based on the property called Tomek links. A Tomek link exists between two examples xi and xj belonging to two classes if there’s no other examples xk in the dataset closer to either xi or xj than the latter two are to each other.\n\n\nData Sampling\nIn probability sampling, all examples have a chance of being selected and it involves randomness. Nonprobability sampling is not random and it follows a fixed deterministic sequence of heuristic actions which means that some examples don’t have a chance of being selected, no matter how many samples we build.\nIn stratified sampling, we first divide our dataset into groups called strata and then randomly select examples from each stratum, like in simple random sampling. It often improves the representativeness of the sample by reducing its bias.\n\n\nData Versioning\nIf data is held and updated in multiple places, we might need to keep track of versions. Versioning the data is also needed if we frequently update the model by collecting more data, especially in an automated way. Data versioning can be implemented in several levels of complexity.\nLevel 0: data is unversioned. Level 1: data is versioned as a snapshot at training time. Level 2: both data and code are versioned as one asset. Level 3: using or building a specialized data versioning solution.\n\n\nData Lake\nA data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or files. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#the-perfect-commit",
    "href": "posts/Git & GitHub/Git.html#the-perfect-commit",
    "title": "Git & GitHub",
    "section": "The Perfect Commit:",
    "text": "The Perfect Commit:\n\nAdd the right changes.\nCompose a good commit message.\nAdd the changes to a commit that is only related to the single topic rather than adding all the changes to a single commit. Git staging is vital in selecting the correct files for perfect commit.\n\ngit add -p file: Commit only a patch of file.\ngit commit: Write a subject with one space and write body of commit."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#git-branching-strategies",
    "href": "posts/Git & GitHub/Git.html#git-branching-strategies",
    "title": "Git & GitHub",
    "section": "Git Branching Strategies",
    "text": "Git Branching Strategies\n\nGit allows you to create branches - but it doesn’t tell you how to use them.\nWe need a written best practice of how work is ideally structured in our team - to avoid mistakes and collisions.\nIt highly depends on our team and team size, on our project, and on how we handle releases.\nIt helps to onboard new team members with proper documentation."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#integrating-changes-structuring-releases",
    "href": "posts/Git & GitHub/Git.html#integrating-changes-structuring-releases",
    "title": "Git & GitHub",
    "section": "Integrating Changes & Structuring Releases",
    "text": "Integrating Changes & Structuring Releases\n\nMainline Development (“Always Be Integrating”).\nState, Release, and Feature Branches.\n\nLong Running Branches:\n\nExist through the complete lifecycle of the project.\nOften, they mirror “stages” in the development life cycle.\nCommon convention: no direct commits.\n\nShort-Lived Branches:\n\nFor new features, bug fixes, refactorings, and experiments.\nWill be deleted after integration (merge/ rebase).\n\n\n\nPull Requests\n\nCommunicating About and Reviewing Code."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#cherry-picking",
    "href": "posts/Git & GitHub/Git.html#cherry-picking",
    "title": "Git & GitHub",
    "section": "Cherry Picking",
    "text": "Cherry Picking\n\nPick one specific commit and move it to another branch.\n\ngit checkout branch\ngit cherry-pick #hash"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#reflog",
    "href": "posts/Git & GitHub/Git.html#reflog",
    "title": "Git & GitHub",
    "section": "Reflog",
    "text": "Reflog\n\nA protocol or journal of every movement of the HEAD pointer.\nIt will be useful in recovering Deleted Commits & Deleted Branches.\n\ngit reflog\ngit reset/branch (branch-name) #hash"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#submodules",
    "href": "posts/Git & GitHub/Git.html#submodules",
    "title": "Git & GitHub",
    "section": "Submodules",
    "text": "Submodules\n\nmkdir lib & cd lib for standards.\ngit submodule add {url}\n\nWhen we create a submodule, the actual contents of the submodule are not stored in the parent repository. The parent repo only stores the submodule remote URL. The information of submodules is stored in .git modules & .git config files.\nWhen we clone a repo with the default git clone {url}, the submodules folders, however, are stayed empty. We can get the contents of submodules using the command:\n\ngit submodule update –init –recursive\n\nWe can achieve the above directly while cloning the repo like this:\n\ngit clone –recursive –submodules {url}"
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#search-find",
    "href": "posts/Git & GitHub/Git.html#search-find",
    "title": "Git & GitHub",
    "section": "Search & Find",
    "text": "Search & Find\n\nFiltering out commit history\n\nBy date –before / –after\nBy message –grep\nBy author –author\nBy file –<filename>\nBy branch <branch>\n\ngit log branch..main will show all the commits that are in the main but not in branch which comes in handy in merging."
  },
  {
    "objectID": "posts/Git & GitHub/Git.html#basic-git-commands",
    "href": "posts/Git & GitHub/Git.html#basic-git-commands",
    "title": "Git & GitHub",
    "section": "Basic Git Commands",
    "text": "Basic Git Commands\n\ngit init: Initialize a local Git repository.\ngit add <file>: Add files to staging area.\ngit status: Check status of working tree.\ngit commit: Commit changes in index.\ngit push: Push to remote repository.\ngit pull: Pull latest from remote repository.\ngit clone url: Clone repository into a new directory.\ngit checkout branch: Switch branches or restore working tree files.\ngit branch: List, create, or delete branches.\ngit diff: Show changes between commits, commit and working tree, etc.\ngit log: Show commit logs.\ngit stash: Stash the changes in a dirty working directory away.\ngit restore --staged <file>: Unstage a file while retaining the changes in working directory.\ngit reset hash: Reset current HEAD to the specified state.\ngit stash pop: Remove a single stashed state from the stash list and apply it on top of the current working tree state.\ngit stash list: List the stash entries that you currently have.\ngit stash drop: Remove a single stashed state from the stash list.\ngit stash clear: Remove all the stash entries that you currently have.\ngit  remote -v: List all currently configured remote repositories.\ngit remote add origin <url>: Add a remote repository.\ngit remote set-url origin <url>: Change the url of the remote repository.\ngit remote add upstream <url>: Add a upstream repository.\ngit remote set-url upstream <url>: Change the url of the upstream repository.\ngit fetch --all --prune: Fetch all the remote branches and delete the remote branches that are deleted in the remote repository.\ngit reset --hard origin/<branch>: Reset the current branch to the remote branch.\ngit merge <branch>: Merge a branch into the current branch.\ngit rebase <branch>: Rebase the current branch onto the specified branch.\ngit rebase -i <branch>: Rebase the current branch onto the specified branch and squash commits."
  },
  {
    "objectID": "posts/Machine/MachineLearning.html",
    "href": "posts/Machine/MachineLearning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine Learning\nMachine learning can be defined as the process of solving a practical problem by collecting a dataset, and algorithmically training a statistical model based on that dataset.\n\n\nSupervised Learning\nThe goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector. For instance, a model created using a dataset of patients could take as input a feature vector describing a patient and output a probability that the patient has cancer.\n\n\nUnsupervised Learning\nThe goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practice problem. Clustering is useful for finding groups of similar objects in a large collection of objects, such as images or text documents.\n\n\nReinforcement Learning\nReinforcement learning is a subfield of machine learning where the machine also called an agent lives in an environment and is capable of perceiving the state of that environment as a vector of features. A common goal of reinforcement learning is to learn a function that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average long-term reward.\n\n\nRaw and Tidy Data\nRaw data is a collection of entities in their natural form: they cannot always be directly fed to machine learning algorithms. Tidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example.\n\n\nTraining and Holdout Sets\nThe first step in a machine learning project is to shuffle the examples and split them into three distinct sets: training, validation, and test. The learning algorithm uses the training set to produce the model. The validation set is used to choose the learning algorithm, and find the best configuration values for that learning algorithm, also known as hyperparameters. The test set is used to assess the model performance before delivering it to the client or putting the model into production.\n\n\nParameters vs. Hyperparameters\nHyperparameters are inputs of machine learning algorithms or pipelines that influence the performance of the model. They don’t belong to the training data and cannot be learned from it.\nParameters are variables that define the model trained by the learning algorithm. Parameters are directly modified by the learning algorithm based on the training data to find the optimal values of the parameters.\n\n\nWhen to use Machine Learning\n\nWhen the problem is too complex for coding.\nWhen the problem is constantly changing.\nWhen it is a perceptive problem such as speech, image, and video recognition.\nWhen it is an unstudied phenomenon.\nWhen the problem has a simple objective.\n\n\n\nMachine Learning Engineering\nMachine learning engineering is the use of scientific principles, tools and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE ecompasses all stages from data collection, to model training, to making the model available for use by the customers. MLE includes any activity that lets machine learning algorithms be implemented as a part of an effective production system.\nA machine learning project life cycle consists of the following steps:\n\nGoal definition.\nData collection and preparation.\nFeature engineering.\nModel training.\nModel evaluation.\nModel deployment.\nModel serving.\nModel monitoring.\nModel maintenance.\n\n\n\nBias\nBias in data is an inconsistency with the phenomenon that data represents. Selection bias is the tendency to skew your choice of data sources to those that are easily available, convenient, and cost-effective. Omitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction. Sampling bias also known as distribution shift occurs when the distribution of examples used for training doesn’t reflect the distribution of the inputs the model will receive in production.\nIt is usually impossible to know exactly what biases are present in a dataset. Biases can be avoided by questioning everything: who created the data, what were their motivations and quality criteria, and more importantly, how and why the data was created."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#neural-networks",
    "href": "posts/NeuralNetworks/CNN.html#neural-networks",
    "title": "Fundamentals of CNNs",
    "section": "Neural Networks",
    "text": "Neural Networks\nNeural networks are the building blocks of deep learning systems. A system is called a neural network if it contains a labeled, directed graph structure where each node in the graph performs some computation.\n\nFigure: A simple neural network that takes the weighted sum of inputs x and weights w which is then passed through the activation function to determine the output."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#activation-functions",
    "href": "posts/NeuralNetworks/CNN.html#activation-functions",
    "title": "Fundamentals of CNNs",
    "section": "Activation Functions",
    "text": "Activation Functions\n\nSigmoid Function is continuous and differentiable everywhere. It is symmetric around the y-axis. The outputs of the sigmoid are not zero centered. Saturated neurons essentially kill the gradient, since the delta of the gradient will be extremely small.\nRectified Linear Unit (ReLU) is zero for negative inputs but increases linearly for positive inputs. The ReLU function is not saturable and is also extremely computationally efficient. ReLU is the most popular activation function used in deep learning and has stronger biological motivations."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#feedforward-neural-networks",
    "href": "posts/NeuralNetworks/CNN.html#feedforward-neural-networks",
    "title": "Fundamentals of CNNs",
    "section": "Feedforward Neural Networks",
    "text": "Feedforward Neural Networks\nThe neural networks architecture in which a connection between nodes is only allowed from nodes in layer i to nodes in layer i + 1, with no backward or inter-layer connections are called feedforward neural networks. When feedforward neural networks include feedback connections i.e. output connections that feed back into the inputs, are called recurrent neural networks."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#backpropagation",
    "href": "posts/NeuralNetworks/CNN.html#backpropagation",
    "title": "Fundamentals of CNNs",
    "section": "Backpropagation",
    "text": "Backpropagation\nBackpropagation is a generalization of the gradient descent algorithms that is specifically used to train multi-layer feedforward networks. The backpropagation algorithm consists of two phases:\n\nThe forward pass where we pass our inputs through the network to obtain our output classifications.\nThe backward pass i.e. weight update phase where we compute the gradient of the loss function and use the information to iteratively apply the chain rule to update the weights in neural networks.\n\nThe activation functions inside the network are differentiable, allowing the chain rule to be applied. Any layers inside the network that require updates to their weights and parameters must be compatible with backpropagation.\nOne Hot Encoding is the process of transforming integer labels into vector labels, where the index in the vector for label is set to 1 and 0 otherwise."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#convolutions",
    "href": "posts/NeuralNetworks/CNN.html#convolutions",
    "title": "Fundamentals of CNNs",
    "section": "Convolutions",
    "text": "Convolutions\nConvolutions are one of the most critical, fundamental building-blocks in computer vision and image processing. Convolution is an element-wise multiplication of two matrices followed by a sum."
  },
  {
    "objectID": "posts/NeuralNetworks/CNN.html#convolutional-layers",
    "href": "posts/NeuralNetworks/CNN.html#convolutional-layers",
    "title": "Fundamentals of CNNs",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\nIt accepts an input volume of size Winput  X Hinput X Dinput .The input sizes are normally square, so it’s common to see Winput = Hinput . The number of filters K controls the depth of the output volume. The output of the convolutional layer is Woutput X Houtput  X Doutput , where:\n\nWoutput = ((Winput - F + 2P)/S) + 1.\nHoutput = ((Hinput - F + 2P)/S) + 1.\nDoutput = K\n\nHere, K is the number of filters, which when used for convolution yields receptive field F. S is the stride and P is the amount of zero-padding."
  },
  {
    "objectID": "posts/NLP/NLP.html#books",
    "href": "posts/NLP/NLP.html#books",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Books:",
    "text": "Books:\n\nNatural Language Processing with Python\nNatural Language Processing in Action\nNatural Language Processing with PyTorch\nNatural Language Processing Specialization"
  },
  {
    "objectID": "posts/NLP/NLP.html#research-papers",
    "href": "posts/NLP/NLP.html#research-papers",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Research Papers:",
    "text": "Research Papers:\n\nTeacher Forcing in Recurrent Neural Networks\nExploring the Limits of Transfer Learning with a Unified Text to Text Transformer\nDeep Learning Based Text Classification: A Comprehensive Review\nThe Illustrated Transformer\nNLP’s ImageNet Moment\nThe Illustrated BERT, ELMO and Co.\nGeneralized Language Models\nEfficient Estimation of Word Representations in Vector Space\nDistributed Representations of Words and Phrases and their Compositionality\nGloVe : Global Vectors for Word Representation\nA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\nNeural Machine Translation By Jointly Learning To Align and Translate\nDynamic Memory Networks for Natural Language Processing\nDynamic Memory Networks for Natural Language Processing\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\nStanza: A Python Natural Language Processing Toolkit for Many Human Languages\nLearning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models\nImproved Semantic Representations From Tree Structured Long Short Term Memory Networks\nThe Stanford CoreNLP Natural Language Processing Toolkit\nZero Shot Learning Through Cross Modal Transfer\nAssessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies\nFinding Universal Grammatical Relations in Multilingual BERT"
  },
  {
    "objectID": "posts/NLP/NLP.html#notebooks",
    "href": "posts/NLP/NLP.html#notebooks",
    "title": "Journey of 66DaysOfData in Natural Language Processing",
    "section": "Notebooks:",
    "text": "Notebooks:\n\nTopic Modeling with Singular Value Decomposition and Non negative Matrix Formation\nSentiment Classification of Internet Movie Reviews Database Reviews\nSemantic Analysis with LDA, LSA and LDIA\nSentiment Analysis of Large Movie Dataset using RNN, CNN and LSTM\nText Generation using Long Short Term Memory or LSTM\nChatbot with Sequence to Sequence Networks\nYELP Reviews Sentiment Analysis\nAmazon Reviews Analysis\nSurname Classification with Demographics\nDuplicate Questions Recognition using Trax\n\nDay1 of 66DaysOfData! - Natural Language Processing: Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today I am learning Natural Language Processing from very begining. I have read and Implemented the Fundamentals of Natural Language Processing. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also presented the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited about the days ahead !! - Topics: - Fundamentals of Natural Language Processing\n\nDay2 of 66DaysOfData! - String Tokenization: In Natural Language Processing, String Tokenization is a process where the string is splitted into Individual words or Individual parts without blanks and tabs. In the same step, the words in the String is converted into lower case. The Tokenize Module from NLTK or Naural Language Toolkit makes very easy to carry out this process. In my Journey of Natural Language Processing, Today I have learned about String Tokenization, Stop word and Punctuation in Natural Language Processing. I have implemented TweetTokenizer and presented the process to remove Stopwords and Punctuation from the Tokenized Tweets here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited about the days ahead!! - Topics: - Fundamentals of Natural Language Processing\n\nDay3 of 66DaysOfData! - Stemming in Natural Language Processing: Stemming is a process of converting a word to its most General form or Stem. It’s basically the process of removing the suffix from a word and reduce it to it’s root word. It helps in reducing the size of Vocabulary. In my Journey of Natural Language Processing, Today I learned about Stemming in Natural Language Processing which is one of the most important steps while working with Text. I have presented the Implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited for coming days!! - Porter Stemmer: It is one of the most common and gentle stemmer which is very fast but not very precise. - Snowball Stemmer: It’s actual name is English Stemmer is more precise over large Dataset. - Lancaster Stemmer: It is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons. - Topics: - Fundamentals of Natural Language Processing\n\nDay4 of 66DaysOfData! - Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word’s Lemma or a Dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than Simple Stemming. Lemmatization looks at the surrounding text to determine a given words’s part of speech where it doesn’t categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it’s simple implementation using Spacy as well NLTK. I have covered the fundamentals of Natural Language Processing such as Tokenization, Stemming and Lemmatization. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics mentioned above. Excited for the days ahead!! - Topics: - Fundamentals of Natural Language Processing\n\nDay5 of 66DaysOfData! - Natural Language Processing: Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. As a part of Natural Langauge Processing Journey, I have started reading and Implementing from the Book Natural Language Processing with Python. It’s really amazing and I have encountered many basic functions which are unknown to me such as Concordance Function, Similar Function, Common Context Function and a basic Dispersion plot as well. I will be using this Book in my journey. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited for the days ahead!! - Book: - Natural Language Processing with Python\nDay6 of 66DaysOfData! - In my Journey of Natural Language Processing, Today I have explored about Gutenberg Corpus using NLTK and Python. I have also learned about various Interesting Challenges with proper explanation of each Topics under the hood of Natural Language Processing from the Book Natural Language Processing with Python. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. - Word Sense Disambiguation: In Natural Language Processing, Word Sense Disambiguation is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other Computer related writing such as discourse, improving relevance of Search Engines, Anaphora Resolution, Coherence, and Inference. - Pronounce Resolution - Generating Language Output - Machine Translation: Machine Translation is a sub field of Computational Linguistics that investigates the use of software to translate text or speech from one language to another. - Spoken Dialog System - Textual Entailment - Book: - Natural Language Processing with Python\n\nDay7 of 66DaysOfData! - On my Journey of Natural Language Processing, Today I have learned about different Text Corpora and Basic Corpus Functionality defined in NLTK from the Book Natural Language Processing with Python . I have also learned about Loading own Corpus, Plotting and Tabulating Distributions and Generating Random Text with Bigrams here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. The Topics I have covered are summarized below: - Web and Chat Text - Brown Corpus - Reuters Corpus - Inaugural Address Corpus - Annotated Text Corpora - Corpora in Other Languages - Conditional Frequency Distributions - Book: - Natural Language Processing with Python\n\nDay8 of 66DaysOfData! - On my Journey of Natural Language Processing, Today I have learned about Processing Raw Text in Natural Language Processing. Basically, I have completed Processing the Text from Electronic Books and from HTML documents from the Book Natural Language Processing with Python. Apart from that, I have learned about WordNet. The topics I have covered in WordNet are: The WordNet Hierarchy and Semantic Similarity: Semantic Similarity is a metric defined over a set of Documents or Terms where the idea of distance between items is based on the likeness of their meaning or Semantic content as opposed to Lexicographical Similarity. Example:Road and Driving. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay9 of 66DaysOfData! - On my journey of Natural Language Processing, Today I have learned about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper explanations. Within a Program we can manipulate Unicode just like normal strings. Unicode are encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8. I have also Implemented the Regular Expressions for Detecting the Word Patterns using basic meta characters such as: - Dollar sign ( $ ) matches the characters of the end of word. - Dot symbol ( . ) matches any single character. - Caret symbol ( ^ ) matches the characters of the start of word. - Question mark ( ? ) specifies the previous character is optional. - Plus sign ( + ) means one or more instances of the preceding item. - Sign ( * ) means zero or more instances of the preceding item. - Backslash (  ) means following character is deprived. - Pipe symbol ( | ) specifies the choice between left and right. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned above and below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay10 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about the various useful applications of Regular Expressions such as Finding Word Stems, Regular Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have also read about the issues of Tokenization: Tokenization turns out to be far more difficult task than one might have expected. No single solution works well accross the board. Another issue of Tokenization is the presence of contractions as well such as in didn’t. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay11 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have completed all the preliminaries process or techniques required in Natural Language Processing included in the Book Natural Language Processing with Python. I have completed the topics such as Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions from this Book. Apart from that, I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters Corpus and so on. And I have completed first 110 pages of the Book Natural Language Processing with Python. I have plotted a simple bar plot using various categories of Brown Corpus presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n \nDay12 of 66DaysOfData! - Part of Speech Tagging: The process of classifying words into their Parts of Speech and Labeling them accordingly is known as Part of Speech Tagging which is also known as POS tagging or simply Tagging. The collections of tags used for a particular task is known as Tagset. In my Journey of Natural Language Processing, Today I have learned about Automatic Tagging such as Default Tagger, Regular Expression Tagger and Lookup Tagger along with N-Gram Tagging such as Unigram Tagger, Bigram Tagger and so on. I have also learned about Combining Taggers using backoff parameter. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead !! - Book: - Natural Language Processing with Python\n\nDay13 of 66DaysOfData! - Supervised Classification: Classification is the process of choosing the correct class label for a given input. The Classification is said to be Supervised Classification if it is built based on training corpora containing the correct label for each input. One common example of Classification is deciding whether an Email is spam or not. In my Journey of Natural Language Processing, Today I have learned about Supervised Classification. I have covered the topics such as Choosing Right Features, Document Classification, Gender Identification, Part of Speech Tagging using Naive Bayes Classifier and Decision Trees Classifier under the hood of Supervised Classification of Natural Language Processing. I have presented the basic Implementation of Naive Bayes Classifier in Document Classification. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead!! - Book: - Natural Language Processing with Python\n\nDay14 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Sequence Classification, Sentence Segmentation and various Evaluation methods under the hood of Natural Language Processing. I have covered the Fundamental Topics such as Test Data, Accuracy measure, Precision and Recall, Confusion Matrices, Cross validation, Decision Trees and Naive Bayes with proper implementations which are so helpful for my understanding. I have been in this Journey for 2 weeks and I have covered all the fundamentals which are so relevant in Natural Language Processing. I have been following the Natural Language Processing with Python Book and it really helps me a lot. Now, I will be focusing more on Implementations so that I will be following the course of Fastai on Natural Language Processing. I have implemented the Naive Bayes Classifier in Text Corpus and I hope you can gain insight about the Implementation of Naive Bayes. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Book mentioned below. Excited about the days ahead!! - Book: - Natural Language Processing with Python\n\nDay15 of 66DaysOfData! - Singular Value Decomposition or SVD: The words that appear most frequently in one topic would appear less frequently in the other, otherwise that word wouldn’t make a good choice to separate out the two topics. Therefore, the Topics are Orthogonal. The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows along with diagonal matrix which contains the relative importance of each factor. - NonNegative Matrix Factorization or NMF: Non Negative Matrix Factorization (NMF) is a factorization or constrain of non negative dataset. NMF is non exact factorization that factors into one short positive matrix. - Topic Frequency Inverse Document Frequency or TFIDF: TFIDF is a way to normalize the term counts by taking into account how often they appear in a document and how long the document is and how common or rare the document is. - In my journey of Natural Language Processing, Today I have learned and Implemented about SVD, NMF and TFIDF in Topic Modeling Project. I have captured just the overview of the implementations here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai\n\nTopic Modeling with SVD and NMF\n\n \nDay16 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Implementations of Natural Language Processing from Fastai course which has been published recently. As the Fastai course primarily focuses on the Coding part and follows the top down aspect of Learning and Teaching. It’s bit complicated to learn than other courses. Fastai’s API is really amazing and powerful as well. I learned the basic steps of Natural Language Processing with Fastai such as Word Tokenization, Subword Tokenization, Numericalization, and Preparing TextBlock and DataBlock. I am currently working on Sentiment Analysis of IMDB reviews using Fastai. I have shared the Implementations of the Word Tokenization, Subword Tokenization, Numericalization and process to prepare the TextBlock and DataBlock with Fastai here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai - Sentiment Classification of Internet Movie Database reviews\n\nDay17 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about the Implementation of Fastai in preparing a Sentiment Classifier Model. I have prepared a Model using Fastai API which can classify Sentiment of Internet Movie Database reviews i.e. classifying the Positive or Negative Sentiment. Fastai’s API is really powerful and effective so that the Model can classify the Sentiment of Internet Movie Database reviews with above 90% accuracy in just few lines of code. I have learned about Word Tokenization, Subword Tokenization, Numericalization, TextBlock and DataBlock API and Training the Classifier Model using Fastai.I have presented the snapshot of the Implementation of Fastai API in preparing the Language Model and Training the Model. I have also presented the Implementation of Fastai API in preparing the Classifier Model using the Language Model and also the concept of unfreezing the Model. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Course mentioned below. Excited about the days ahead!! - Course: Fastai - Sentiment Classification of Internet Movie Database reviews\n\nDay18 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Topic of Natural Language Processing in the Book Dive into Deep Learning by Aston Zhang. Here, I have covered the Topics such as Text Processing, Machine Translation, Natural Language Processing Pretraining and Word Embedding. The information and explanations were great and the code implementation is in MXNET. I am not quite familiar with MXNET framework so I have just presented a small Snapshot here. Apart from that, Today I have read a very small part of the book Natural Language Processing in Action by Hobson Lane. I am so much thankful to Anthony Camarillo for sharing this book with me. And I will give continuation with this Book Natural Language Processing in Action. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Dive into Deep Learning - Natural Language Processing in Action\n\nDay19 of 66DaysOfCode! - In my journey of Natural Language Processing, Today I have read and implemented the First chapter of the Book Natural Language Processing in Action. In this chapter, I have covered the Topics such as Natural Language Processing and the Magic, Regular Expressions, Finite State Machine or FSM concept, Word order and Grammar, simple NLP Chatbot Pipeline and Natural Language IQ. I have presented the simple Chatbot using Regular Expressions and Finite State Machine or FSM concept. Basically, I will be working on much advanced Chatbots using Neural Networks in coming days. I hope you will also google out the FSM concept in NLP and also the Implementation of Regular Expressions in FSM from here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Natural Language Processing in Action\n\nDay20 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a couple of topics under the hood of Natural Language Processing such as Tokenized Phrase, Dot Product in Natural Language Processing, Bag of Words and Overlapping, Token improvement with Regex which is apart from Regular Expressions, Contractions, Extending Vocabulary with NGrams. I have also read and implemented the Regex Tokenizer, Tree Bank Tokenizer and Casual Tokenizer. Actually I am continuing my learning Journey with a Book Natural Language Processing in Action and lots of the preprocessing concepts which I have already read are coming along my way. I prefer to go through the concepts again because I don’t want to skip any topics from this book. Although the concepts might match along the way I won’t repeat the same implementation in any of my Snapshots. I have presented the Implementation of Regex Tokenizer, Tree Bank Tokenizer, Casual Tokenizer and NGrams here in the Snapshots. These Tokenization steps are more better than Traditional Tokenization steps using Regular Expressions. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead!! - Books: - Natural Language Processing in Action\n\nDay21 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and implemented about Stopwords, Stemming, Lemmatization and Sentiment Analysis using VADER Approach which is the Algorithm composed by human and also the Machine Learning Approach with the help of Naive Bayes Classifier. I have completed the first 2 chapters of the book, Natural Language Processing in Action and it is helping me a lot along my Journey. I have presented the Implementation of VADER Approach in Sentiment Analysis along with Naive Bayes Classifier and I have also included the Implementation of Casual Tokenizer in the Movies Dataset. I hope you will gain insights about the Implementation of VADER Approach and Naive Bayes in Sentiment Analysis. Actually, VADER Approach is not as efficient as Machine Learning Approach such as Naive Bayes Classifier. I hope you will spend some time in learning about Naive Bayes in Sentiment Analysis along with VADER Approach. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay22 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Bag of Words, Vectorizing concept, Vector Spaces, Cosine Similarity, Zipf’s Law and Inverse Frequency concept, Text Modeling, TFIDF, Relevance Ranking and Okapi BM25 concept. I have completed the first three chapters of the book Natural Language Processing in Action and this chapter primarily focuses on the concept of Vectorizing the Tokens which are obtained after Tokenization using TFIDF Vectorizer. Text Vectorization is the process of converting Text into Numerical representation. I have also read and Implemented the concept of Cosine Similarity under the hood of Natural Language Processing. I have presented the Implementation of TFIDF Vectorizer and also the process of Tokenizing the Text Documents and removing the Stopwords. I have also Implemented the Cosine Similarity using Numpy and pure Python as well in this Snapshot. I hope you will gain insights about Text Vectorization and Tokenization from here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay23 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have started learning about Semantic Analysis. I have read and implemented about Topic Vectors, Algorithms for Scoring Topics and Semantic Analysis such as Latent Semantic Analysis LSA, Linear Discriminant Analysis LDA and Latent Dirichlet Allocation LDIA. Today, I primarily focused on reading and Implementing about Linear Discriminant Analysis LDA. LDA is one of the most straight forward and fast Dimension Reduction and Classification Models. In Natural Language Processing, Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of LDA’s working Principal which states that Computing the centroid of TFIDF vectors for each side of the binary Class here in the Snapshot. I hope you will gain insights about Implementation of LDA Classifier and creating NLP Pipeline of Tokenizer and Vectorizer from here. And I hope you will spend some time in learning about Semantic Analysis as well. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days ahead !! - Books: - Natural Language Processing in Action\n\nDay24 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented Latent Semantic Analysis “LSA”, Singular Value Decomposition “SVD”, Principal Component Analysis “PCA”, Truncated SVD and Latent Dirichlet Allocation “LDIA”. I have primarily focused on reading and Implementing LSA and LDIA for Semantic Analysis. LSA works well with Normalized TFIDF Vectors whereas LDIA works well with raw Bag of Words “BOW” Count Vectors. Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of Linear Discriminant Analysis “LDA” while working with TFIDF Vectors and BOW Count Vectors here in the Snapshot. I hope you will gain insights about the Implementation of LDA Classifier along with LDIA Topic Vectors and BOW count Vectors. Incase you want to see my Notebook, I have presented the overall Implementation of Semantic Analysis with LSA, LDA and LDIA with proper Documentation here: Excited about the days to come!! - Books: - Natural Language Processing in Action - Semantic Analysis\n\nDay25 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have started learning and Implementing Neural Networks and Deep Learning for Natural Language Processing. I have completed the Implementation of LSA and LDIA in Semantic Analysis along with LDA. I have read the topics such as Neural Networks and Perceptrons, Gradients, Local and Global Minimum and Backpropagation under the hood of Neural Networks and Deep Learning. Actually, I have primarily focused on reading the topics needed to understand the Neural Networks and Deep Learning rather than Implementing the concepts. I have presented the Simple workflow of Neural Networks using Keras API. I will be Implementing the Keras API in Natural Language Processing from today. I hope you will also spend some time to learn the basic topics which I have mentioned above to understand the Neural Networks and Deep Learning. Excited to learn and Implement Neural Networks for NLP in coming days!! - Book: - Natural Language Processing in Action\n\nDay26 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Word Vectors, Softmax Function, Negative Sampling, Document Similarity with Doc2Vec and Google’s Word2vec, GloVe and Facebook’s FastText Models which were pretrained on Billions of Text Data. I have primarily focused on learning and Implementing the Word2vec pretrained Model today. I am continuing my learning journey along with the book, Natural Language Processing in Action. Word2vec is a Model for Natural Language Processing. The Word2vec Algorithm uses a Neural Network Model to learn Word associations from a large corpus of text. Once Word2vec Model is trained, It can detect Synonymous words or suggest additional words for a partial sentence. Word2vec is a group of related Models that are used to produce Word Embeddings. I have presented the Implementation to access the Google’s Word2vec pretrained Model and it’s basic Functions and the process to create own Domain Specific Word2vec Model. I have also presented the Implementation of Doc2vec Model here in the Snapshot. I hope you will also spend so time to learn about Word2vec pretrained Model. Excited about the days to come!! - Book: - Natural Language Processing in Action\n\nDay27 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as CNN building blocks, Step size or Stride, Filter Composition, Padding, Convolutional Pipeline, Learning, Narrow Windows and Implementing the Keras API under the hood of Convolutional Neural Network for NLP. I have started working on the Sentiment Analysis of Large Movie Review Dataset which was compiled for the 2011 paper “Learning Word Vectors for Sentiment Analysis”. Since, It is a very large Dataset, I have used just the subset of the Dataset. I will be Implementing CNN for this Project. I have presented the basic Implementations of approaching the Dataset such as Importing the Dependencies, Processing the Dataset with Tokenization and Google News pretrained Model Vectorization and Splitting the Dataset into Training set and Test set. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come !! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n \nDay28 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as Convolutional Neural Network Architecture, Pooling, Dropout, CNN parameters, Optimization and Training CNN Model under the hood of Convolutional Neural Network for Natural Language Processing. I have presented the overall Implementation of Convolutional Neural Network here in the Snapshot. I have also presented the short info of all the parameters mentioned in the CNN Model and the process of Compiling and Training the Model as well. I hope you will gain insights about the Implementation of CNN Model in Sentiment Analysis. Actually, It is the continuation of yesterday’s Snapshots. I hope you will also spend some time working on it. I have completed working on the Sentiment Analysis of Large Movie Review Dataset. I have prepared a Model using Convolutional Neural Network which can classify the Sentiment of Text Data. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay29 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Simple Recurrent Neural Network as well as Bidirectional Recurrent Neural Network. Here, I have covered the topics such as Backpropagation through Time, Hyperparameters, Statefulness, Bidirectional Networks and various other topics similar to Convolutional Neural Network mentioned in the previous Snapshot. I have Implemented the Recurrent Neural Network in the same Large Movie Review Dataset to predict the Sentiment of Text Data. I have presented the overall Implementation of RNN Model here in the Snapshot. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of RNN Model in Sentiment Analysis. Although the Implementation of RNN Model is not so appreciable, I hope you will spend some time understanding the working principle of RNN and working on the same. I have completed working on the Sentiment Analysis of Large Movie Review Dataset using Simple RNN as well as Bidirectional RNN. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay30 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Long Short Term Memory or LSTM. I have covered the topics such as LSTM working principle, Backpropagation through time, Keras API and various other topics similar to CNN and RNN as mentioned in the previous Snapshots under the hood of NLP. Actually, I have primarily focused on Implementing the LSTM Model in the same Large Movie Review Dataset to compare the effectiveness of CNN, RNN and LSTM on Sentiment Analysis of Text Data. Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. I have presented the Simple Implementation of Long Short Term Memory or LSTM Model in the same Large Movie Review Dataset. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of LSTM Model in Sentiment Analysis. I have completed working on the same with LSTM Model. Excited about the days ahead!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n\nDay31 of 66DaysOfData! - Long Short Term Memory or LSTM: Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. LSTM is applicable for Text Generation as well. Today I have read and Implemented about Generating the Text using Long Short Term Memory or LSTM. I have prepared a Model using LSTM which generates the Text similar to William Shakespeare’s writing. I have used the Gutenberg Corpus which contains the 3 plays of Shakespeare to train the Neural Network. I have presented the Implementation of LSTM Model as well as the Implementation of Keras API in Text Generation here in the Snapshot. I have also presented the Snapshot of Generated Text with the help of LSTM Model. I have completed working on Generating Text with the help of LSTM Model. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Sentiment Analysis of Large Movie Dataset\n \nDay32 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented the topics such as Encoder and Decoder Architecture in Sequence to sequence Models, Thought Vector, Decoding Thought using LSTM, Sequence Encoder and Keras Functional API in assembling Sequence to Sequence Pipeline under the hood of Natural Language Processing. I have started working on building a Chatbot using Sequence to sequence Neural Networks and Keras Functional API. I have presented the simple Implementation of processing the Text Corpus and few steps to make the Text Data ready to train the Sequence to sequence Chatbot here in the Snapshot. I will be using the Cornell Dialog Dataset for training the Chatbot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Chatbot with Sequence to sequence Networks\n\nDay33 of 66DaysOfData! - Sequence to Sequence Model: Sequence to Sequence Neural Networks can be built with a modular and reusable Encoder and Decoder Architecture. The Encoder Model generates a Thought Vector which is a Dense and fixed Dimension Vector representation of the Data. The Decoder Model use Thought Vectors to generate Output Sequences. In my journey of Natural Language Processing, Today I have read and Implemented various topics under Sequence to Sequence Networks. I have continued working on the Chatbot using Sequence to Sequence Learning. I have used the Keras Functional API and Cornell Dialog Dataset for Training the Model. I have presented the Implementation of Thought Encoder and Thought Decoder using Keras Functional API here in the Snapshot. I have also presented the techniques for Training the Model and Generating the Response Sequences here. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! - Book: - Natural Language Processing in Action - Chatbot with Sequence to sequence Networks\n\nDay34 of 66DaysOfData! - In my Journey of Natural Language Processing, Today I have read about Sentence Segmentation, Named Entity Recognition, Understanding Chatbot Approaches and few NLP Pipelines under the hood of Natural Language Processing. I have also started reading the book Natural Language Processing with PyTorch. Actually, I had never worked with PyTorch and never found any particular reason to start with PyTorch. But, Today I got motivated to start Natural Language Processing with PyTorch. I will be reading and Implementing Natural Language Processing with PyTorch from today. I have presented the simple Implementation of AIML Bot and the Vectorization concept here in the Snapshot. I am fond of revisiting the small concepts again and again so that I won’t get stuck while Implementing in real problems. I am so excited to start Natural Language Processing with PyTorch. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch\n \nDay35 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented The Supervised Learning Paradigm, Computational Graphs, PyTorch Basics and various NLP fundamentals such as Tokenization, Lemmatization, Tagging and Semantics. I have also read and Implemented about Activation Functions such as Sigmoid, Tanh, ReLU, Softmax and Loss Functions such as Mean Squared Error and Cross Entropy under the hood of Natural Language Processing with PyTorch. I have presented the Implementation of PyTorch in various Activation Functions and Loss Functions along with few preliminaries for Understanding and working with PyTorch here in the Snapshots. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch\n \nDay36 of 66DaysOfData! - Regularization: In Machine Learning, Regularization is the process of adding Information in order to solve a well posed problems or to prevent Overfitting. In my journey of Natural Language Processing, Today I have read and Implemented about Model Selection approaches, Choosing Loss Functions, Choosing Optimizers, Gradient Based Supervised Learning, Evaluation Metrics, Regularization and Early Stopping under the hood of Natural Language Processing with PyTorch. I have started working on the YELP Reviews Dataset and the Neural Networks will be Implemented using PyTorch. I have presented some Data Preprocessing Techniques which I have Implemented while working with YELP Reviews Dataset here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis\n\nDay37 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about PyTorch’s Data Representation, The Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of Natural Language Processing with PyTorch. I am continuing my journey along with the book Natural Language Processing with PyTorch. - The Vocabulary Class: The Vocabulary Class not only manages the Bijection i.e Allowing user to add new Tokens and have the Index auto increment but also handles the special token called UNK which stands for Unknown. By using the UNK Token, It will be easy to handle Tokens at Test time that were never seen in Training Instance. - The Vectorizer Class: The second stage of going from a Text Dataset to a vectorized minibatch is to iterate through the Tokens of an Input Data Point and convert each Token to its Integer form. The result of this iteration should be a Vector. Because this Vector will be combined with Vectors from other Data points, there is Constraint that the Vectors produced by the Vectorizer should always have the same length. - The DataLoader Class: The Final step of Text to Vectorized minibatch pipeline is to actually group the Vectorized Datapoints. Because grouping into mini batches is a viatal part of Training the Neural Networks, PyTorch provides a built in class called DataLoader for coordinating the Process. - I have presented the Implementation of Dataset Class using PyTorch here in the Snapshot. I have been working on the Implementation of Dataset Class, Vectorizer Class and DataLoader Class and I feel quite overwhelmed with the complexity of PyTorch because I am not familiar with PyTorch Framework. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis.\n \nDay38 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Perceptron Classifier, The Training Routine Class using PyTorch’s Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using PyTorch under the hood of “Natural Language Processing with PyTorch”. I am continuing my journey along with the book, Natural Language Processing with PyTorch. Today, I have continued working with YELP Review Dataset for Sentiment Analysis using PyTorch. I have presented the simple Implementation of PyTorch in Training the Classifier Model along with the process of Instantiating the Dataset, Model, Loss, Optimizer and Training State here in the Snapshots. Actually, It is the continuation of yesterday’s Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some time learning the Topics from the Books mentioned below. Excited about the days to come!! Book: - Natural Language Processing with PyTorch - YELP Reviews Sentiment Analysis.\n \nDay39 of 66DaysOfData! - **Long Short Term Memory or LSTM Model: Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. In my journey of Natural Language Processing, Today I have started working on new Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. I will be working on the same until I finish it completely. Apart from that, Today I have watched some videos on YouTube and read some Notebooks in Kaggle under the hood of Natural Language Processing. I have presented the overall Implementation of TensorFlow and TensorBoard in Processing the Data such as Tokenization and Encoding and the techniques for preparing LSTM Model here in the Snapshots. I hope you will gain some insights and work on the same. Excited about the days to come!! - Amazon Reviews Analysis\n \nDay40 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have continued working on yesterday’s Notebook which was of Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. Actually, I spent most of my time in Training the Model and I tried to optimize the Model to increase the speed of Training but still the Training procedure took most part of my time using GPU as well. Apart from that, Today I have spent some time working on Greedy Programming Algorithms. Actually I got a chance to interview from one of the Tech Giants as well. I had prepared a Model using LSTM which was trained on Amazon Reviews Dataset. This Snapshot is the continuation of yesterday’s Snapshots. So, I have presented some basic workflow of Model Evaluation and deploying the Trained Model on unseen Text Data for Analysis. I have also presented the simple technique of Data Visualization for evaluating the Model here in the Snapshot. I hope you will gain some insights and work on the same. Excited about the days to come!! - Amazon Reviews Analysis\n\nDay41 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about Feed Forward Networks for Natural Language Processing using PyTorch. I have covered the topics such as The Multi Layer Perceptron or MLP, Simple XOR Functions with MLP using PyTorch and Softmax Activation Function under the hood of Natural Language Processing with PyTorch. I have started working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. I will be using PyTorch for building the Classifier Model. I have presented the Implementation of PyTorch in building the simple MLP Class here in the Snapshots. I have also presented the techniques for processing the raw Dataset for Surname Classification Project using PyTorch. I hope you will gain some insights and you will also spend some time learning about MLP and working on the same and get ready for building the Model. Excited about the days ahead !! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n \nDay42 of 66DaysOfData! - The Vectorizer Class: The Vocabulary converts individual tokens into Integers and The Surname Vectorizer is responsible for applying the Vocabulary and converting surname into Vectors. Surnames are sequence of characters and each character is an individual token in the Vocabulary. In my journey of Natural Language Processing, Today I have read and Implemented about PyTorch’s Data Representation, The Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of Natural Language Processing with PyTorch. I have continued working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. Today, I have worked out the Implementation of Dataset Class, Vectorizer Class, Vocabulary Class and Multi Layer Perceptron or MLP Classifier Model. I have presented the Implementation of Vectorizer Class using PyTorch here in the Snapshot. I hope you will gain some insights from here and you will also spend some time working on the same. Excited about the days to come!! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n\nDay43 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read and Implemented about MLP Classifier, The Training Routine Class using PyTorch’s Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using PyTorch under the hood of Natural Language Processing with PyTorch. I have continued working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. Today, I have worked out the Implementation of MLP Classifier, The Training Routine Class, Inference and Inspection of the Model prepared using PyTorch. I have presented the simple Implementation of MLP Classifier Model along with the Snapshot of the Inference and Inspection of the Model Evaluation using PyTorch. Actually, It is the continuation of yesterday’s Snapshot. I hope you will gain some insights and you will also spend some time working on the same. Excited about the days to come !! - Book: - Natural Language Processing with PyTorch - Surname Classification with Demographics: PyTorch\n \nDay44 of 66DaysOfData! - Logistic Regression: Logistic Regression is a Statistical Model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In Regression Analysis, Logistic Regression is estimating the parameters of a Logistic Model in a form of Binary Regression. In my journey of Natural Language Processing, Today I have started learning from Natural Language Processing Specialization on Coursera. I have just started learning the first course and I will spend couple of weeks in this Specialization. I have covered the topics such as Logistic Regression and Naive Bayes along with various Fundamental Preprocessing steps under the hood of Natural Language Processing. This Specialization is managed and organized by the team of Andrew Ng i.e deeplearning.ai so that I will gain more exposure to Mathematics behind every topics which has very high Importance. I have presented the Implementation of Naive Bayes Classifier along with Testing procedure here in the Snapshot. I hope you will also spend some time going through this Specialization and I am so much excited about the days to come !! - Course: - Natural Language Processing Specialization: Coursera\n\nDay45 of 66DaysOfData! - KNearest Neighbors: The KNN Algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. KNN works by finding the distances between a Query and all the examples in the Data, selecting the specified number of examples or K closest to the Query, then votes for the most Frequent Label or Averages the Labels. In my journey of Natural Language Processing, Today I have completed the first course i.e Natural Language Processing with Classification and Vector Spaces from Natural Language Processing Specialization on Coursera. I have covered the topics such as Vector Space Models, Euclidean Distance, Cosine Similarity, PCA, KNN, Logistic Regression, Naive Bayes and many more in this course. I have presented the simple Implementation of KNN along with techniques for creating Hash table here in the Snapshots. Actually, I have presented this Implementation on the basis of this Course for self understanding so I have not included all the dependencies here in the Snapshot. I hope you will also spend some time going through this Specialization. Excited about the days ahead !! - Course: - Natural Language Processing Specialization: Coursera - Natural Language Processing with Classification and Vector Spaces\n\nDay46 of 66DaysOfData! - Minimum Edit Distance: The Minimum Edit Distance between two strings is defined as the minimum number of Editing Operations like Insertion, Deletion and Substitution needed to transform one string into another. It’s working principle is applicable in building the Auto Correction Model. In my journey of Natural Language Processing, Today I have started learning and Implementing NLP from the second course of Natural Language Processing Specialization on Coursera i.e Natural Language Processing with Probabilistic Models. I have covered the topics such as Autocorrect Models using Minimum Edit Distance Algorithm, POS Tagging, Markov’s Models and The Viterbi Algorithm under the hood of Natural Language Processing. I will spend couple of weeks in this Specialization. I have presented the simple Implementation of Minimum Edit Distance Algorithm whose working principle is applicable in Auto Correction Model here in the Snapshot. I hope you will also spend some time learning about the topics mentioned above. I hope you will also spend some time in this Specialization. I am excited about the days to come!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay47 of 66DaysOfData! - Continuous Bag of Words: In the Continuous Bag of Words Model, The distributed representations of context or surrounding words are combined to predict the word in the middle. The Model predicts the current word from a window of surrounding context words. In my journey of Natural Language Processing, Today I have learned about Ngrams and Probabilities, Sequence Probabilities, Smoothing, Word Embeddings, CBOW, Cleaning and Tokenization from the course Natural Language Processing with Probabilistic Models which is the second course in Natural Language Processing Specialization on Coursera. I have completed the two Courses in this Specialization. I have presented the simple Implementation of Initializing the Model, Softmax Activation Function, Forward Propagation, Cost Function and Back Propagation for training the CBOW Model here in the Snapshot. It presents the Mathematics behind simple Neural Network which is crucial for understanding the Implementation of Neural Networks and Deep Learning. I hope you will gain some insights and you will also spend some time learning about the topics mentioned above. Excited about days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay48 of 66DaysOfData! - Gated Recurrent Unit: A Gated Recurrent Unit or GRU is a gating mechanism in RNN similar to a Long Short Term Memory or LSTM unit but without an output gate. GRU solves the vanishing Gradient problem that can come with standard RNN by using an Update Gate and a Reset Gate. In my journey of Natural Language Processing, Today I have started learning and Implementing NLP from the third course of Natural Language Processing Specialization on Coursera i.e Natural Language Processing with Sequence Models. I have covered the topics such as Trax and Neural Networks, Dense and ReLU Layers, Serial Layers, RNN and GRU using Trax, Deep and Bidirectional RNN under the hood of Natural Language Processing. Neural Networks and Trax: Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks. I have presented the simple Implementation of Training GRU Model using Trax here in the Snapshot. I hope you will gain some insights from here and you will also spend some time learning about Trax Frameworks in Natural Language Processing and Deep Learning. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay49 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about LSTMs and Named Entity Recognition, RNNs and Vanishing Gradients, Siamese Neural Networks, Triplets and One Shot Learning from the course Natural Language Processing with Sequence Models which is the part of Natural Language Processing Specialization on Coursera. I have completed three Courses in this Specialization. I have started working with Quora Questions Answers Dataset to build a LSTM Model that can Identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I will build this Model using Trax Framework which is maintained by Google Brain Team and good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. I have presented the simple Implementation of Data Preparation for training the LSTM Model using Quora Dataset here in the Snapshot. I hope you will gain some insights and you will also start working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n\nDay50 of 66DaysOfData! - Siamese Neural Network: Siamese Neural Network is an Artificial Neural Network which uses the same weight while working in tandem on two different input vectors to compute comparable output vectors. In my journey of Natural Language Processing, Today I have learned and Implemented about Data Generators or Iterators, Siamese Neural Networks, LSTMs and Triplet Loss under the hood of Natural Language Processing. I have continued working with Quora Questions Answers Dataset to build a Model using LSTM that can identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I have presented the Implementation of Siamese Neural Network using LSTM Model along with the Implementation of Triplet Loss here in the Snapshots. I have also presented the Implementation of Training the Model using Data Generators and other dependencies. I have presented all the Implementations using Trax Framework here. I hope you will gain some insights from here and you will also continue working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n \nDay51 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned and Implemented about Siamese Neural Networks and LSTMs, Implementation of Trax in Siamese Neural Networks under the hood of Natural Language Processing. I have completed working with Quora Questions Answers Dataset to build a LSTM Model that can identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I have build a Model using Trax which can identify the Duplicate Questions. Neural Networks and Trax: Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks. I have presented the Implementation of Siamese Neural Network using LSTM Model that can identify the Similar or Duplicate Questions here in the Snapshots. I have also presented the output of the Model here which fascinates me a lot. I hope you will gain some insights from here and you will also spend some time working on the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Duplicate Questions Recognition: Trax\n \nDay52 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have learned about Sequence to Sequence Models, Alignment, Attention Models, BLEU or Bilingual Evaluation Understudy, ROUGE or Recall Oriented Understudy, Greedy Decoding, Random Sampling, Temperature Parameter, Beam Search Decoding, Minimum Bayes Risk or MBR and Teacher Forcing Algorithm in LSTM from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization. Teacher Forcing: Teacher Forcing is the technique where the target word is passed as the next input to the Decoder. Training with Teacher Forcing converges faster. I have presented the simple Implementation of Neural Machine Translation Model which uses Attention along with the Implementation of Preparing Attention Input using Encoder and Decoder Activations here in the Snapshots. I have presented this Implementation on the basis of this course for self understanding so I have not included all the dependencies here in the Snapshot. I hope you will also spend some time going through the topics mentioned above. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Teacher Forcing in RNNs\n\nDay53 of 66DaysOfData! - Transformer and Natural Language Processing: The Transformer is a Deep Learning Model which are designed to handle Sequential Data such as Translation and Summarization and allows much more parallelization than RNNs such as LSTM. In my journey of Natural Language Processing, Today I have learned about Transformers and it’s Applications, Dot Product, Multi Head and Causal Attentions, The Transformer Decoder and Summarizer, State of Art Transformers such as GPT2 or Generative Pretraining for Transformer, BERT or Bidirectional Encoder Representations from Transformer, T5 or Text to Text Transfer Transformer and Multitask Training Strategy and GLUE Benchmark from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization on Coursera. I have presented the simple Implementation of Transformer Language Model using Trax here in the Snapshot. I have also presented the Implementation of PositionalEncoder, FeedForward and DecoderBlock which returns the list of Layers required for the Transformer Model here. I hope you will gain some insights and you will also spend some time learning about the Transformer Model. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera\n\nDay54 of 66DaysOfData! - Reformer: The Efficient Transformer or The Reformer is a Transformer Model which combines two crucial techniques to solve the problems of Attention and Memory allocation that limit Transformer Application to Long context windows. Reformer uses LSH to reduce the complexity of attending over Long sequences and Reversible residual layers to more efficiently use the memory available. In my journey of Natural Language Processing, Today I have learned about Long Sequential Data, Transformer Complexity, Locality Sensitive Hashing or LSH Attention, Reversible Residual Layers and Reformer from the course Natural Language Processing with Attention Models which is the last course of Natural Language Processing Specialization. I have presented the simple Implementation of Reformer Language Model along with the Implementation of Training the Reformer Model using Trax here in the Snapshots. I have also presented the Implementation of Forward Reversible Layer and Reverse Reversible Layer here. I hope you will gain some insights and you will also spend some time learning about the same. Excited about the days ahead!! - Course: - Natural Language Processing Specialization: Coursera - Natural Language Processing Specialization Completion\n\nDay55 of 66DaysOfData! - Transfer Learning: Transfer Learning is a process where a Model is first pretrained on a Data rich task before being fine tuned on a Downstream Task and Transfer Learning has emerged as a powerful technique in Natural Language Processing or NLP. The effectiveness of Transfer Learning has given rise to a Diversity of approaches, methodology, and practice. In my journey of Natural Language Processing, Today I have started reading the Papers of Transformers and Natural Language Processing which is named as Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer. I have presented the Implementation of Encoder and Decoder Class which are the key components in any Dominant Sequence Models. I hope you will also spend some time reading about the Transformer from the Paper mentioned above. Excited about the days ahead!! - Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer\n\nDay56 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Paper about Deep Learning and Text Classification which is named as Deep Learning Based Text Classification: A Comprehensive Review. I have read about various Text Classification Tasks such as Sentiment Analysis, News Categorization, Topic Analysis and Question Answering. I have also read about various Deep Learning Models for Text Classification such as Feed Forward Neural Networks, RNN Based Models, CNN Based Models, Capsule Neural Networks, Models with Attention, Transformers, Graph Neural Networks, Siamese Neural Networks and Hybrid Models from this Paper. I have presented the simple Implementation of Layer Normalization, Sublayer Connection, Encoder Layer and Feed Forward Layer using PyTorch here in the Snapshots. I hope you will also spend time reading about Deep Learning and Text Classification from the Paper mentioned above. I am excited about the days ahead!! - Deep Learning Based Text Classification: A Comprehensive Review\n\nDay57 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read a Paper about Deep Learning and Text Classification which is named as Deep Learning Based Text Classification: A Comprehensive Review. I have completed reading this Paper. I have read about Unsupervised Learning using Autoencoders, Adversarial Training, Reinforcement Learning and few Popular Datasets such as YELP and IMDB. I have also read about Popular Metrics for Text Classification such as Accuracy and Error Rate, Precision, Recall and F1 Score from this Paper. I have also read one of the most popular Articles named The Illustrated Transformer by Jay Alammar. It teaches about the Transformer from Scratch. I have read the topics such as Encoder, Decoder, Self Attention, Feed Forward and Multi Head Attention in this Article. I have presented the Implementation of Decoder Class, DecoderLayer with FeedForward and Subsequent Mask using PyTorch here in the Snapshots. I hope you will spend some time reading the Paper and Article mentioned above. Excited about the days ahead!! - Deep Learning Based Text Classification: A Comprehensive Review - The Illustrated Transformer\n\nDay58 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read some Articles about Deep Learning and Natural Language Processing. I have read an Article named NLP’s ImageNet Moment which is written by Sebastian Ruder. Here, I have read about the recent advances in NLP such as ULMFiT, ELMO, OpenAI Transformer and the ImageNet for Language. Similarly, I have read another Article named as The Illustrated BERT, ELMO and Co. which is written by Jay Alammar. Here, I have read about BERT Model Architecture, Word Embeddings, Transfer Learning and the topics mentioned above. I have also read another Article named Generalized Language Models which is written by Lilian Weng. Here, I am reading about Language Models and I am still reading this Article. I have presented the Implementation of Attention and Multi Head Attention Class using PyTorch here in the Snapshot. Actually, It is the continuation of yesterday’s Snapshot. I hope you will also spend some time reading the Articles mentioned above. Excited about the days ahead!! - NLP’s ImageNet Moment - The Illustrated BERT, ELMO and Co. - Generalized Language Models\n\nDay59 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read Papers about Skip Gram and NLP. I have read a Paper which is named as Efficient Estimation of Word Representations in Vector Space. Here, I have read about Model Architectures, Feed Forward Neural Net Language Model, Recurrent Neural Net Language Model, Log Linear Models, CBOW Models, Continuous Skip Gram Model and various other related topics. I have also read the Paper which is named as Distributed Representations of Words and Phrases and their Compositionality. Here, I have read about Skip Gram Model, Hierarchical Softmax, Negative Sampling, Subsampling and Additive Compositionality. I have presented the Implementation of Positionwise Feed Forward Class, Embedding, Positional Encoding and Feed Forward using PyTorch here in the Snapshot. Actually, It is the continuation of yesterday’s Snapshot. I hope you will also spend some time reading the Topics and Papers mentioned above about Skip Gram Model. Excited about the days ahead!! - Efficient Estimation of Word Representations in Vector Space - Distributed Representations of Words and Phrases and their Compositionality\n\nDay60 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read Papers about GloVe and Recurrent Neural Network Language Model. I have read a Paper which is named as GloVe : Global Vectors for Word Representation. Here, I have read about Matrix Factorization Methods, The GloVe Model and Complexity of the Model, Evaluation Methods, Word Analogies, Model Analysis and various other related topics. Similarly, I have read another Paper which is named as A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. Here, I have read about Language Modeling and Initialization, Speech Recognition and various other related topics. I have presented the Implementation of Transformer Model, Batches and Masking using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. I am excited about the days ahead!! - GloVe : Global Vectors for Word Representation - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\n\nDay61 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Paper about Neural Machine Translation with Attention and Dynamic Memory Network. I have read the Paper which is named as Neural Machine Translation By Jointly Learning To Align and Translate. Here, I have read the Topics related to Neural Machine Translation such as RNN Encoder and Decoder, Translation and to Align and Qualitative and Quantitative Analysis and more related to the same. I have also started reading another Paper which is named as Dynamic Memory Networks for Natural Language Processing. I have just started reading this Paper and I will complete it soon. I have presented the Implementation of Training the Loop of Transformer Model along with the Implementation of Batching using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. I am excited about the days ahead!! - Neural Machine Translation By Jointly Learning To Align and Translate - Dynamic Memory Networks for Natural Language Processing\n\nDay62 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Dynamic Memory Networks and Recursive Neural Networks. I have read the Paper which is named as Dynamic Memory Networks for Natural Language Processing. Here, I have read about the Dynamic Memory Networks and Episodic Memory Module, Attention and Memory Update Mechanism and various Topics related to the same. I have also read the Paper which is named as Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. Here, I have read about Semantic Vector Space and Compositionality in Vector Space, Recursive Neural Tensor Networks, Backpropagation and various Topics related to the same. I have presented the Implementation of Optimizer along with Learning Rates using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Dynamic Memory Networks for Natural Language Processing - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n\nDay63 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Stanza: NLP Toolkit and Transfer Learning to study Linguistic Structure. I have read the Paper which is named as Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Here, I have read about Neural Multilingual NLP Pipeline, Dependency Parsing and Lemmatization, NER and various Topics related to the same. I have also read the Paper which is named as Learning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models. Here, I have read about LSTM Architecture and Training, Random Baselines, Recursive Structure, Non Linguistic Structure and various Topics related to the same. I have presented the Implementation of Label Smoothing and Feed Forward using PyTorch here in the Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Stanza: A Python Natural Language Processing Toolkit for Many Human Languages - Learning Music Helps you Read: Using Transfer to Study Linguistic Structure in Language Models\n\nDay64 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Long Short Term Memory and Stanford CoreNLP Toolkit. I have read the Paper which is named as Improved Semantic Representations From Tree Structured Long Short Term Memory Networks. Here, I have read about Bidirectional LSTM and Multilayer LSTM, Tree Structured LSTM, Semantic Relatedness and Sentiment Classification and various Topics related to the same. I have read another Paper which is named as The Stanford CoreNLP Natural Language Processing Toolkit. Here, I have read about Model Design and Development, Elementary Usage, Annotators, Tokenization and Lemmatization and various Topics related to the same. I have presented the Implementation of Data Generator, Loss Computation and Greedy Decoding in Transformer Model using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Improved Semantic Representations From Tree Structured Long Short Term Memory Networks - The Stanford CoreNLP Natural Language Processing Toolkit\n\nDay65 of 66DaysOfData! - In my journey of Natural Language Processing, Today I have read the Papers about Zero Shot Learning and Syntax Sensitive Dependencies in LSTM. I have read the Paper which is named as Zero Shot Learning Through Cross Modal Transfer. Here, I have read about Zero Shot and One Shot Learning, Word and Image Representations, Zero Shot Learning Models, Novelty Detection and various Topics related to the same. I have also read another Paper which is named as Assessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies. Here, I have read about LSTM Model and Baselines, Number Prediction, Relative Clauses, Language Modeling and various Topics related to the same. I have presented the Implementation of Greedy Decoding and Iterators using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above. Excited about the days ahead!! - Zero Shot Learning Through Cross Modal Transfer - Assessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies\n\nDay66 of 66DaysOfData! - Today I have read the Paper which is named as Finding Universal Grammatical Relations in Multilingual BERT. Here, I have read about BERT, Cross Lingual Probing, Universal Grammatical Relations, Finer Grained Analysis, Determiners and various Topics related to the same. I have also spent some time going through the Documentation of my Journey in GitHub. Today, I have completed this Journey and I am so grateful and thankful to meet lots of amazing persons along the way. Actually, Ken Jee is one of the amazing persons who started this Journey. He has always been supportive and a great example to the Data Science Aspirants and I am so thankful to him. I have been Learning and Implementing Natural Language Processing for couple of months now. I will continue my Learning with new Journey and new Topics. I hope you have also learned something interesting in Natural Language Processing until this day. Personally, I am so thankful to you, the one who has come along this way and supported me since the start of the Journey. We will start to learn new Topics with new hope and energy. Thank you!! - Finding Universal Grammatical Relations in Multilingual BERT"
  },
  {
    "objectID": "posts/Pattern/Pattern.html#introduction",
    "href": "posts/Pattern/Pattern.html#introduction",
    "title": "Pattern Recognition & ML",
    "section": "Introduction",
    "text": "Introduction\nThe field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Generalization, the ability to categorize correctly new examples that differ from those used for training is a central goal in pattern recognition.\nApplications in which the training data comprises examples of the input vectors with their corresponding target vectors are known as supervised learning problems. The cases such as the digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification. If the desired output consists of one or more continuous variables, then the task is called regression.\nThe pattern recognition problems in which the training data consists of a set of input vectors x without any corresponding target values are called unsupervised learning problems. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.\nThe technique of reinforcement learning is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#probability-theory",
    "href": "posts/Pattern/Pattern.html#probability-theory",
    "title": "Pattern Recognition & ML",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory provides a consistent framework for the quantification and manipulation of uncertainty. When combined with decision theory, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#the-rules-of-probability",
    "href": "posts/Pattern/Pattern.html#the-rules-of-probability",
    "title": "Pattern Recognition & ML",
    "section": "The Rules of Probability",
    "text": "The Rules of Probability\n\nSum Rule: p(X) = \\(\\\\sum\\_{Y}^{}{p(X,\\\\ Y)}\\)\nProduct Rule: p(X, Y) = p(Y / X)p(X)\n\nHere, p(X, Y) is a joint probability and is verbalized as “the probability of X and Y”. Similarly, the quantity p(Y/X) is a conditional probability and is verbalized as “the probability of Y given X”, where the quantity p(X) is a marginal probability and is verbalized as “the probability of X”."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#bayes-theorem",
    "href": "posts/Pattern/Pattern.html#bayes-theorem",
    "title": "Pattern Recognition & ML",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nFrom the product rule, together with the symmetry property p(X, Y) = p(Y, X), we obtain the following relationship between conditional probabilities which is called Bayes’ theorem which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator as being the normalization constant required ensuring that the sum of the conditional probability on the left-hand side over all values of Y equals one.\n\np(Y / X) = \\(\\\\frac{p\\\\left( \\\\frac{X}{Y} \\\\right)p(X)}{p(X)}\\)\np(X) = \\(\\\\sum\\_{Y}^{}{p\\\\left( \\\\frac{X}{Y} \\\\right)p(Y)}\\)"
  },
  {
    "objectID": "posts/Pattern/Pattern.html#model-selection",
    "href": "posts/Pattern/Pattern.html#model-selection",
    "title": "Pattern Recognition & ML",
    "section": "Model Selection",
    "text": "Model Selection\nIn the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, the one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. However, in many applications, the data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. If the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this problem is to use cross-validation. This allows a proportion (S – 1) / S of the available data to be used for training while making use of all of the data to assess performance."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#decision-theory",
    "href": "posts/Pattern/Pattern.html#decision-theory",
    "title": "Pattern Recognition & ML",
    "section": "Decision Theory",
    "text": "Decision Theory\nIf we have an input vector x together with a corresponding vector t of target variables, our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classification problems, t will represent class labels."
  },
  {
    "objectID": "posts/Pattern/Pattern.html#minimizing-loss-function",
    "href": "posts/Pattern/Pattern.html#minimizing-loss-function",
    "title": "Pattern Recognition & ML",
    "section": "Minimizing Loss Function",
    "text": "Minimizing Loss Function\nLoss function is also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#word-vectors",
    "href": "posts/WordVectors/WordVectors.html#word-vectors",
    "title": "Word Vectors",
    "section": "Word Vectors",
    "text": "Word Vectors\nWord vectors are also called word embeddings or neural word representations because these whole bunch of words are represented in a high dimensional vector space and they are embedded into that space. They are also called as a distibuted representation.\nWord vectors means having a vector for each word type i.e both for context and outside, which are initialized randomly and those vectors are progressively updated by using iterative algorithms so that they can do better job at predicting which words appear in the context of other words.\n\nDistributional Semantics\nIt states that a word’s meaning is given by the words that frequently appear close-by. When a word w appears in a text, it’s context is the set of words that appear nearby within a fixed-size window."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#word2vec",
    "href": "posts/WordVectors/WordVectors.html#word2vec",
    "title": "Word Vectors",
    "section": "Word2Vec",
    "text": "Word2Vec\nWord2Vec is a framework for learning word vectors developed by Mikolov et al. 2013. The idea behind Word2Vec is that we have a large corpus or body of text and every word in a fixed-vocabulary is represented by a vector. We have to go through each position t in the text, which has a center word c and context words o and we have to use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa. We have to keep adjusting the word vectors to maximize this probability. Word2Vec model maximizes the objective function by putting similar words nearby in high dimensional vector space.\nTwo model variants: - Skip Grams: Predict context words given center word. - Continuous Bag of Words: Predict center word from context words.\nMain Idea of Word2Vec - Start with random word vectors. - Iterate through each word in the whole corpus. - Try to predict the surrounding words using word vectors. Try and predict what words surrounds the center word by using the probability distribution that is defined in terms of the dot product between the word vectors for the center word and the context words. - Updating the vectors so that they can predict the actual surrounding words better and better.\nKey Points: - Word2Vec model actually ignores the position of words. - Taking a log likelihood turns all of the products into sums which decreases the computational complexity. - A dot product is a natural measure for similarity between words. If two words have a larger dot product, that means they are more similar. - The simple way to avoid negative probabilities is to apply exponential function.\nTraining Methods: - To train a model, we gradually adjust parameters to minimize a loss. - Theta represents all the model parameters, in one long vector. We optimize these parameters by walking down the gradient.\nBag of Words Model: - Bag of words models are the models that don’t pay attention to words order or position, it doesn’t matter if the word is near to the center word or a bit further away on the left or right. The probability estimation will be the same at each position.\nIn bag of words, we have outside word vector and center word vector, which undergoes dot product followed by softmax activation function.\n\nOptimization: Gradient Descent\n\nTo learn good word vectors, we have a cost function J(\\(\\theta\\)).\nGradient descent is an algorithm to minimize the J(\\(\\theta\\)) by changing \\(\\theta\\).\nGradient Descent is an iterative learning algorithm that learns to maximize the J(\\(\\theta\\)) by changing the \\(\\theta\\).\nFrom the current value of \\(\\theta\\), calculate the gradient of J(\\(\\theta\\)), then take small step in the direction of negative gradient to gradually move down towards the minimum.\n\nProblems of Gradient Descent - J(\\(\\theta\\)) is a function of all windows in the corpus which is often billions. So, actually working out J(\\(\\theta\\)) or the gradient of J(\\(\\theta\\)) would be extremely expensive because we have to iterate over the entire corpus. - We would wait a very long before making a single update.\n\n\nStochastic Gradient Descent\nStochastic gradient descent is a very simple modification of the gradient descent algorithm. Rather than working out an estimate of the gradient based on the entire corpus, we simply take one center word or a small batch like 32 center words, and we work out the estimate of the gradient based on them.\nStochastic gradient descent is kind of noisy and bounces around as it makes progress, it actually means that in complex networks it learns better solution. So, it can do much more quickly and much better.\n\n\nSoftmax Function\nThe softmax function will take any R in vector and turns it into the range 0 and 1. The name of the softmax function comes from the fact that it’s sort of like a max because the exponential function gives more emphasis to the big contents in different dimensions of calculating the similarity. The softmax function takes some numbers and returns the whole probability distribution.\n\n\nCo-occurence Vector\n\nVectors increase in size with the vocabulary.\nVery high dimensional and requires a lot of storage though sparse.\nSubsequent classification models have sparsity issues which makes models less robust."
  },
  {
    "objectID": "posts/WordVectors/WordVectors.html#glove",
    "href": "posts/WordVectors/WordVectors.html#glove",
    "title": "Word Vectors",
    "section": "GloVe",
    "text": "GloVe\n\nFast training.\nScalable to huge corpus.\nGood performance even with small corpus and small vectors.\n\nGloVe model unify the thinking between the co-occurence matrix models and the neural models by being someway similar to the neural models but actually calculated on top of a co-occurence matrix count."
  }
]